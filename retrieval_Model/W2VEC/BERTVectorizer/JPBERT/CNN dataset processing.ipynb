{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Kabul , Afghanistan -LRB- CNN -RRB- -- Military divers have found the body of a U.S. paratrooper who went missing last week in a river in western Afghanistan .\\n\\nThe NATO-led International Security Assistance Force said the soldier was found close to where he disappeared November 4 . Officials are continuing their search for a second paratrooper lost at the same time .\\n\\nBoth men -- from the 4th Brigade Combat Team , 82nd Airborne Division -- disappeared in the Morghab River near Afghanistan 's border with Turkmenistan . The men were on a routine resupply mission , the NATO group said .\\n\\nFamily members identified the recovered body as that of Benjamin Sherman , 21 , of Plymouth , Massachusetts .\\n\\nThey said he jumped into the river when he saw a fellow soldier struggling in the water .\\n\\n`` I know that day he jumped into the river to try to save his comrade was because he did n't just see another soldier in the water ; he saw his brother , '' said Sherman 's sister , Meredith , in a statement to CNN affiliate WCVB in Boston , Massachusetts .\\n\\n`` He did n't jump in because he was trained to but because that 's what his heart told him to do . ''\\n\\nSherman 's mother , Denise , said the family called him `` the unstoppable one . ''\\n\\n`` I raised him with the understanding that when you choose to do something , you do it to the best of your ability , '' she said in a statement to WCVB . `` He was powerful , ingenuous and determined . ''\\n\\nCNN 's Thomas Evans contributed to this report .\\n\\n@highlight\\n\\nTwo U.S. soldiers went missing in Afghan river on November 4\\n\\n@highlight\\n\\nBoth lost in Morghab River near border with Turkmenistan\\n\\n@highlight\\n\\nFamily identifies recovered body as that of Benjamin Sherman of Plymouth , Massachusetts\\n\\n@highlight\\n\\nSearch continues for the other paratrooper\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os import listdir\n",
    "import string\n",
    " \n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, encoding='utf-8')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "doc = load_doc(\"/home/ifte/amiebot_project/Large_files/cnn_part_dataset/fffcaffda91f80b841efaefe04704e0357e4276c.story\")\n",
    "\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       a\n",
       "0  10000\n",
       "1   5000\n",
       "2  15000\n",
       "3   2000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[10000, 5000, 15000, 2000]\n",
    "   \n",
    "\n",
    "import pandas as pd\n",
    "d = pd.DataFrame()\n",
    "d['a'] = a\n",
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Kabul , Afghanistan -LRB- CNN -RRB- -- Military divers have found the body of a U.S. paratrooper who went missing last week in a river in western Afghanistan .\\n\\nThe NATO-led International Security Assistance Force said the soldier was found close to where he disappeared November 4 . Officials are continuing their search for a second paratrooper lost at the same time .\\n\\nBoth men -- from the 4th Brigade Combat Team , 82nd Airborne Division -- disappeared in the Morghab River near Afghanistan 's border with Turkmenistan . The men were on a routine resupply mission , the NATO group said .\\n\\nFamily members identified the recovered body as that of Benjamin Sherman , 21 , of Plymouth , Massachusetts .\\n\\nThey said he jumped into the river when he saw a fellow soldier struggling in the water .\\n\\n`` I know that day he jumped into the river to try to save his comrade was because he did n't just see another soldier in the water ; he saw his brother , '' said Sherman 's sister , Meredith , in a statement to CNN affiliate WCVB in Boston , Massachusetts .\\n\\n`` He did n't jump in because he was trained to but because that 's what his heart told him to do . ''\\n\\nSherman 's mother , Denise , said the family called him `` the unstoppable one . ''\\n\\n`` I raised him with the understanding that when you choose to do something , you do it to the best of your ability , '' she said in a statement to WCVB . `` He was powerful , ingenuous and determined . ''\\n\\nCNN 's Thomas Evans contributed to this report .\\n\\n\",\n",
       " ['Two U.S. soldiers went missing in Afghan river on November 4',\n",
       "  'Both lost in Morghab River near border with Turkmenistan',\n",
       "  'Family identifies recovered body as that of Benjamin Sherman of Plymouth , Massachusetts',\n",
       "  'Search continues for the other paratrooper'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split a document into news story and highlights\n",
    "def split_story(doc):\n",
    "\t# find first highlight\n",
    "\tindex = doc.find('@highlight')\n",
    "\t# split into story and highlights\n",
    "\tstory, highlights = doc[:index], doc[index:].split('@highlight')\n",
    "\t# strip extra white space around each highlight\n",
    "\thighlights = [h.strip() for h in highlights if len(h) > 0]\n",
    "\treturn story, highlights\n",
    "\n",
    "story, highlights = split_story(doc)\n",
    "story, highlights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Stories 30\n"
     ]
    }
   ],
   "source": [
    "# load all stories in a directory\n",
    "def load_stories(directory):\n",
    "\tstories = list()\n",
    "\tfor name in listdir(directory):\n",
    "\t\tfilename = directory + '/' + name\n",
    "\t\t# load document\n",
    "\t\tdoc = load_doc(filename)\n",
    "\t\t# split into story and highlights\n",
    "\t\tstory, highlights = split_story(doc)\n",
    "\t\t# store\n",
    "\t\tstories.append({'story':story, 'highlights':highlights})\n",
    "\treturn stories\n",
    "\n",
    "# load stories\n",
    "directory = '/home/ifte/amiebot_project/Large_files/cnn_part_dataset/'\n",
    "stories = load_stories(directory)\n",
    "print('Loaded Stories %d' % len(stories))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean a list of lines\n",
    "def clean_lines(lines):\n",
    "\tcleaned = list()\n",
    "\t# prepare a translation table to remove punctuation\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\tfor line in lines:\n",
    "\t\t# strip source cnn office if it exists\n",
    "\t\t#index = line.find('(CNN) -- ')\n",
    "\t\t#if index > -1:\n",
    "\t\t#\tline = line[index+len('(CNN)'):]\n",
    "\t\t# tokenize on white space\n",
    "\t\tline = line.split()\n",
    "\t\t# convert to lower case\n",
    "\t\tline = [word.lower() for word in line]\n",
    "\t\t# remove punctuation from each token\n",
    "\t\tline = [w.translate(table) for w in line]\n",
    "\t\t# remove tokens with numbers in them\n",
    "\t\tline = [word for word in line if word.isalpha()]\n",
    "\t\t# store as string\n",
    "\t\tcleaned.append(' '.join(line))\n",
    "\t# remove empty strings\n",
    "\tcleaned = [c for c in cleaned if len(c) > 0]\n",
    "\treturn cleaned\n",
    " \n",
    "\n",
    "for example in stories:\n",
    "\texample['story'] = clean_lines(example['story'].split('\\n'))\n",
    "\texample['highlights'] = clean_lines(example['highlights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "example['story']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the tony nominations have been announced',\n",
       " 'kinky boots gained the most with noms',\n",
       " 'tony awards will be broadcast on cbs on june']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example['highlights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_juman import JumanTokenizer\n",
    "juman_tokenizer = JumanTokenizer()\n",
    "\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "mod_txt = \"/home/ifte/Downloads/L12_H768_A12_E30_BPE_WWM_Transformers-Model/vocab.txt\"\n",
    "bert_tokenizer = BertTokenizer(mod_txt, do_lower_case=False, do_basic_tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the tony nominations have been announcedkinky boots gained the most with nomstony awards will be broadcast on cbs on june'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(example['highlights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokens = juman_tokenizer.tokenize(''.join(example['highlights']))\n",
    "#bert_tokens = bert_tokenizer.tokenize(\" \".join(tokens))\n",
    "bert_tokens = bert_tokenizer.tokenize(''.join(example['highlights']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]']\n",
      "['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]']\n",
      "['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]']\n"
     ]
    }
   ],
   "source": [
    "for line in example['highlights']:\n",
    "    print(bert_tokenizer.tokenize(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_for_summarization(story_lines, summary_lines, tokenizer, juman_tokenizer):\n",
    "    \"\"\" Encode the story and summary lines, and join them\n",
    "    as specified in [1] by using `[SEP] [CLS]` tokens to separate\n",
    "    sentences.\n",
    "    \"\"\"\n",
    "    \n",
    "    tokens = juman_tokenizer.tokenize(story_lines)\n",
    "    bert_tokens = bert_tokenizer.tokenize(\" \".join(tokens))\n",
    "    \n",
    "    \n",
    "    story_lines_token_ids = [\n",
    "        bert_tokenizer.convert_tokens_to_ids([\"[CLS]\"] + bert_tokens[:126] + [\"[SEP]\"])\n",
    "        for line in story_lines\n",
    "    ]\n",
    "    \n",
    "    tokens = juman_tokenizer.tokenize(summary_lines)\n",
    "    bert_tokens = bert_tokenizer.tokenize(\" \".join(tokens))\n",
    "    \n",
    "    summary_lines_token_ids = [\n",
    "        #tokenizer.add_special_tokens_single_sequence(tokenizer.encode(line))\n",
    "        bert_tokenizer.convert_tokens_to_ids([\"[CLS]\"] + bert_tokens[:126] + [\"[SEP]\"])\n",
    "        for line in summary_lines\n",
    "    ]\n",
    "\n",
    "    story_token_ids = [\n",
    "        token for sentence in story_lines_token_ids for token in sentence\n",
    "    ]\n",
    "    summary_token_ids = [\n",
    "        token for sentence in summary_lines_token_ids for token in sentence\n",
    "    ]\n",
    "\n",
    "    return story_token_ids, summary_token_ids\n",
    "\n",
    "from bert_juman import JumanTokenizer\n",
    "juman_tokenizer = JumanTokenizer()\n",
    "\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "mod_txt = \"/home/ifte/Downloads/L12_H768_A12_E30_BPE_WWM_Transformers-Model/vocab.txt\"\n",
    "bert_tokenizer = BertTokenizer(mod_txt, do_lower_case=False, do_basic_tokenize=False)\n",
    "\n",
    "encode_for_summarization(example['story'], example['highlights'], bert_tokenizer, juman_tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
