{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Japanese Pretrained model \n",
    "#### The multilingual pretrained model includes Japanese, so it is possible to use the multilingual pretrained model for Japanese tasks, but it is not appropriate that the basic unit is almost a letter. Therefore, morphological analysis was performed on the input text, and the basic unit was obtained by dividing the morpheme into subwords, and pretraining was performed using only Japanese text (using Wikipedia)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The details of the Japanese pretrained model are shown below.\n",
    "#### Input text: All Japanese Wikipedia (about 18 million sentences, normalized to half-width)\n",
    "Perform morphological analysis on the input text with Juman ++ (v2.0.0-rc2), and further apply Unsupervised Word Segmentation for Neural Machine Translation and Text Generation (BPE) https://github.com/rsennrich/subword-nmt to divide into subwords\n",
    "Same setting as BERT_ {BASE} (12-layer, 768-hidden, 12-heads)\n",
    "30 epoch (1GPU (using GeForce GTX 1080 Ti) takes about 1 day for 1epoch, so about 30 days for pretraining)\n",
    "Use the latest GPU or use a program that can use Multi-GPU.\n",
    "Number of vocabularies: 32,000 (including morphemes and subwords)\n",
    "max_seq_length: 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_juman import BertWithJumanModel\n",
    "bert = BertWithJumanModel (\"/home/ifte/Downloads/L12_H768_A12_E30_BPE_WWM_Transformers-Model/\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "445"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/home/ifte/amiebot_project/MyAI/AI-system/retrieval_Model/processed_perPage_perLine.csv\")\n",
    "df.head()\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    var = str(row['Data']).split('。')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data      メールMobiControl v14 ManualWindows Embedded: Exc...\n",
       "PageID                                                  117\n",
       "Name: 117, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(var)\n",
    "df.iloc[117]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "docvec = []\n",
    "\n",
    "for item in var:\n",
    "    emb = bert.get_sentence_embedding (item)\n",
    "    docvec.append(emb.tolist()) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('docvec.pkl', 'wb') as f:\n",
    "    pickle.dump(docvec ,f)\n",
    "    \n",
    "with open('docvec.pkl', 'rb') as f:\n",
    "    vector = pickle.load(f)\n",
    "    \n",
    "len(vector)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "n_clusters = int(np.ceil(len(var)**0.3))\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "kmeans = kmeans.fit(vector)\n",
    "clusters = kmeans.labels_.tolist()\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = var\n",
    "sentenceDictionary = {}\n",
    "for idx, sentence in enumerate(sentences):\n",
    "\tsentenceDictionary[idx] = {}\n",
    "\tsentenceDictionary[idx]['text'] = sentence\n",
    "\tsentenceDictionary[idx]['cluster'] = clusters[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterDictionary = {}\n",
    "for key, sentence in sentenceDictionary.items():\n",
    "\tif sentence['cluster'] not in clusterDictionary:\n",
    "\t\tclusterDictionary[sentence['cluster']] = []\n",
    "\tclusterDictionary[sentence['cluster']].append(sentence['text'])\n",
    "\tsentence['idx'] = len(clusterDictionary[sentence['cluster']]) - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "####################################\n",
    "# Calculate Cosine Similarity Scores\n",
    "####################################\t\t\n",
    "\n",
    "# For each cluster of sentences,\n",
    "# Find the sentence with highet cosine similarity over all sentences in cluster\n",
    "maxCosineScores = {}\n",
    "for key, clusterSentences in clusterDictionary.items():\n",
    "\tmaxCosineScores[key] = {}\n",
    "\tmaxCosineScores[key]['score'] = 0\n",
    "\ttfidf_matrix = vectorizer.fit_transform(clusterSentences)\n",
    "\tcos_sim_matrix = cosine_similarity(tfidf_matrix)\n",
    "    \n",
    "\tfor idx, row in enumerate(cos_sim_matrix):\n",
    "\t\tsum = 0\n",
    "\t\tfor col in row:\n",
    "\t\t\tsum += col\n",
    "\t\tif sum > maxCosineScores[key]['score']:\n",
    "\t\t\tmaxCosineScores[key]['score'] = sum\n",
    "\t\t\tmaxCosineScores[key]['idx'] = idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1台の会社支給端末(iOS Android)を、複数の従業員で共用する場合は、必須 また、 端末の登録後に、5項のどれかを追加したり、変更することも可能 詳しくは、を参照ください  続けて、「登録ID」、「登録用URL」または「Setup.INI」ファイルに一意的に対応する端末登録ルールに、自らの端末シリアル番号などの属性情報を申告します 文書、写真、動画などのコンテンツが主体 \n"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "# Construct Document Summary\n",
    "####################################\t\n",
    "\n",
    "# for every cluster's max cosine score,\n",
    "# find the corresponding original sentence\n",
    "resultIndices = []\n",
    "i = 0\n",
    "for key, value in maxCosineScores.items():\n",
    "\tcluster = key\n",
    "\tidx = value['idx']\n",
    "# \tstemmedSentence = clusterDictionary[cluster][idx]\n",
    "\t# key corresponds to the sentences index of the original document\n",
    "\t# we will use this key to sort our results in order of original document\n",
    "\tfor key, value in sentenceDictionary.items():\n",
    "\t\tif value['cluster'] == cluster and value['idx'] == idx:\n",
    "\t\t\tresultIndices.append(key)\n",
    "\n",
    "resultIndices.sort()\n",
    "\n",
    "# Iterate over sentences and construct summary output\n",
    "result = ''\n",
    "for idx in resultIndices:\n",
    "\tresult += sentences[idx] + ' '\n",
    "\t\t\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "avg = []\n",
    "closest = []\n",
    "for j in range(n_clusters):\n",
    "    idx = np.where(kmeans.labels_ == j)[0]\n",
    "    #print(\"IDX is: \", idx)\n",
    "    avg.append(np.mean(idx))\n",
    "closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_,vector)\n",
    "ordering = sorted(range(n_clusters), key=lambda k: avg[k])\n",
    "#Summ = ' '.join([review[closest[idx]] for idx in ordering])\n",
    "#print(\"Done for review # = \", rv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, list found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-1cef0529e909>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclosest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mordering\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, list found"
     ]
    }
   ],
   "source": [
    "summary = ' '.join([vector[closest[idx]] for idx in ordering])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_juman import JumanTokenizer\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
    "\n",
    "mod = \"/home/ifte/Downloads/L12_H768_A12_E30_BPE_WWM_Transformers-Model/\" \n",
    "mod_txt = \"/home/ifte/Downloads/L12_H768_A12_E30_BPE_WWM_Transformers-Model/vocab.txt\"\n",
    "\n",
    "model = BertModel.from_pretrained(mod)\n",
    "bert_tokenizer = BertTokenizer(mod_txt, do_lower_case=False, do_basic_tokenize=False)\n",
    "\n",
    "juman_tokenizer = JumanTokenizer()\n",
    "text = \"吾輩は猫である。\"\n",
    "tokens = juman_tokenizer.tokenize(text)\n",
    "bert_tokens = bert_tokenizer.tokenize(\" \".join(tokens))\n",
    "ids = bert_tokenizer.convert_tokens_to_ids([\"[CLS]\"] + bert_tokens + [\"[SEP]\"])\n",
    "tokens_tensor = torch.tensor(ids).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['吾輩', 'は', '猫', 'である', '。']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]', 'は', '猫', 'である', '。']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 9, 4816, 32, 7, 3]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2,    1,    9, 4816,   32,    7,    3]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
