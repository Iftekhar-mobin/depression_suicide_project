{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/iftekhar/Downloads/BERT-base_mecab-ipadic-bpe-32k_whole-word-mask/new were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertJapaneseTokenizer, BertForMaskedLM\n",
    "\n",
    "# Load pre-trained tokenizer\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained('/home/iftekhar/Downloads/BERT-base_mecab-ipadic-bpe-32k_whole-word-mask/new')\n",
    "# Load pre-trained model\n",
    "model = BertForMaskedLM.from_pretrained('/home/iftekhar/Downloads/BERT-base_mecab-ipadic-bpe-32k_whole-word-mask/new')\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '宿泊代金の支払方法について教えてください。'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = ['宿泊代金', '支払方法', '教える']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['宿泊', '代金', 'の', '支払', '方法', 'について', '教え', 'て', 'ください', '。']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text = tokenizer.tokenize(query)\n",
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['テレビ', 'で', 'サッカー', 'の', '試合', 'を', '見る', '。']\n",
    "\n",
    "# Mask a token that we will try to predict back with `BertForMaskedLM`\n",
    "# masked_index = question.index(word)\n",
    "masked_index = 1\n",
    "# tokenized_text[masked_index] = '[MASK]'\n",
    "# ['テレビ', 'で', '[MASK]', 'の', '試合', 'を', '見る', '。']\n",
    "\n",
    "# Convert token to vocabulary indices\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "# [571, 12, 4, 5, 608, 11, 2867, 8]\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "# tensor([[ 571,   12,    4,    5,  608,   11, 2867,    8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(tokens_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -5.4410,   3.8274,  -3.9452,  ...,  -8.0093,  -5.5738,  -7.6920],\n",
       "        [ -3.9263,   7.1168,  -5.4834,  ...,  -8.1102,  -5.3262,  -3.4483],\n",
       "        [ -5.8970,   8.7722,  -2.9035,  ...,  -6.5889,  -3.9831,  -3.8655],\n",
       "        ...,\n",
       "        [ -8.7810,   3.7610,  -6.2852,  ..., -12.3828,  -6.7486, -11.1676],\n",
       "        [ -8.9119,   3.8230,  -3.5011,  ..., -11.6964,  -3.7670, -12.4521],\n",
       "        [ -5.8612,   3.1078,  -2.5902,  ..., -10.4407,  -5.2364, -11.5876]],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 教え\n",
      "1 教える\n",
      "2 学び\n",
      "3 を\n",
      "4 学ぶ\n",
      "5 生きる\n",
      "6 芸能\n",
      "7 教育\n",
      "8 学習\n",
      "9 も\n"
     ]
    }
   ],
   "source": [
    "# # Predict\n",
    "# with torch.no_grad():\n",
    "outputs = model(tokens_tensor)\n",
    "predictions = outputs[0][0, masked_index].topk(10)  # 予測結果の上位5件を抽出\n",
    "\n",
    "# Show results\n",
    "for i, index_t in enumerate(predictions.indices):\n",
    "    index = index_t.item()\n",
    "    token = tokenizer.convert_ids_to_tokens([index])[0]\n",
    "    print(i, token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'outputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-81ca70570b8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'outputs' is not defined"
     ]
    }
   ],
   "source": [
    "x = outputs[0][0, masked_index].topk(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-498270fda576>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-498270fda576>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    print(tokenizer.convert_ids_to_tokens([index]))index\u001b[0m\n\u001b[0m                                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "for i, index_t in enumerate(x.indices):\n",
    "    index = index_t.item()\n",
    "    print(tokenizer.convert_ids_to_tokens([index]))index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying Next seq approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertJapaneseTokenizer\n",
    "from transformers import BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/iftekhar/Downloads/BERT-base_mecab-ipadic-bpe-32k_whole-word-mask/new were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained tokenizer\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained('/home/iftekhar/Downloads/BERT-base_mecab-ipadic-bpe-32k_whole-word-mask/new')\n",
    "# Load pre-trained model\n",
    "model = BertForMaskedLM.from_pretrained('/home/iftekhar/Downloads/BERT-base_mecab-ipadic-bpe-32k_whole-word-mask/new')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = '朝食に[MASK]を焼いて食べました。'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = tokenizer.encode(test, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 25965, 7, 4, 11, 16878, 16, 2949, 3913, 10, 8, 3]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', '朝食', 'に', '[MASK]', 'を', '焼い', 'て', '食べ', 'まし', 'た', '。', '[SEP]']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = torch.tensor([token_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2, 25965,     7,     4,    11, 16878,    16,  2949,  3913,    10,\n",
       "             8,     3]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "masked_index = torch.where(token_ids == tokenizer.mask_token_id)[1].tolist()[0]\n",
    "print(masked_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 朝食 に パン を 焼い て 食べ まし た 。 [SEP]\n",
      "[CLS] 朝食 に 肉 を 焼い て 食べ まし た 。 [SEP]\n",
      "[CLS] 朝食 に ご飯 を 焼い て 食べ まし た 。 [SEP]\n",
      "[CLS] 朝食 に 豚肉 を 焼い て 食べ まし た 。 [SEP]\n",
      "[CLS] 朝食 に ハム を 焼い て 食べ まし た 。 [SEP]\n"
     ]
    }
   ],
   "source": [
    "result = model(token_ids)\n",
    "pred_ids = result[0][:, masked_index].topk(5).indices.tolist()[0]\n",
    "for pred_id in pred_ids:\n",
    "    output_ids = token_ids.tolist()[0]\n",
    "    output_ids[masked_index] = pred_id\n",
    "    print(tokenizer.decode(output_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 クリケット\n",
      "1 タイガース\n",
      "2 サッカー\n",
      "3 メッツ\n",
      "4 カブス\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertJapaneseTokenizer, BertForMaskedLM\n",
    "\n",
    "# Load pre-trained tokenizer\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
    "\n",
    "# Tokenize input\n",
    "text = 'テレビでサッカーの試合を見る。'\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "# ['テレビ', 'で', 'サッカー', 'の', '試合', 'を', '見る', '。']\n",
    "\n",
    "# Mask a token that we will try to predict back with `BertForMaskedLM`\n",
    "masked_index = 2\n",
    "tokenized_text[masked_index] = '[MASK]'\n",
    "# ['テレビ', 'で', '[MASK]', 'の', '試合', 'を', '見る', '。']\n",
    "\n",
    "# Convert token to vocabulary indices\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "# [571, 12, 4, 5, 608, 11, 2867, 8]\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "# tensor([[ 571,   12,    4,    5,  608,   11, 2867,    8]])\n",
    "\n",
    "# Load pre-trained model\n",
    "model = BertForMaskedLM.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
    "model.eval()\n",
    "\n",
    "# Predict\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor)\n",
    "    predictions = outputs[0][0, masked_index].topk(5) # 予測結果の上位5件を抽出\n",
    "\n",
    "# Show results\n",
    "for i, index_t in enumerate(predictions.indices):\n",
    "    index = index_t.item()\n",
    "    token = tokenizer.convert_ids_to_tokens([index])[0]\n",
    "    print(i, token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('/home/iftekhar/kuhuto_cl_bert/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/iftekhar/kuhuto_cl_bert/tokenizer_config.json',\n",
       " '/home/iftekhar/kuhuto_cl_bert/special_tokens_map.json',\n",
       " '/home/iftekhar/kuhuto_cl_bert/vocab.txt',\n",
       " '/home/iftekhar/kuhuto_cl_bert/added_tokens.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('/home/iftekhar/kuhuto_cl_bert/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
