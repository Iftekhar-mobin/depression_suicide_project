{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-566a00b5fd4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMeCab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim import models\n",
    "import numpy as np\n",
    "import MeCab\n",
    "import pandas as pd\n",
    "#from gensim.models import word2vec, KeyedVectors\n",
    "mt = MeCab.Tagger('')\n",
    "mt.parse('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Just for e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "inp = '/home/iftekhar/myworkplace/AI-system/2VECs_models/word2vec/mobicontrol_corpus.txt'\n",
    "var = LineSentence(inp)\n",
    "\n",
    "for sentence in var:\n",
    "    print(sentence)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use the commented following for *.model files otherwise use keyedvectors\n",
    "gensim_model = Word2Vec.load('word2vec/combined.model')\n",
    "# import gensim.models.keyedvectors as word2vec\n",
    "# model = word2vec.KeyedVectors.load_word2vec_format('word2vec/combined.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('さようなら', 0.6923430562019348),\n",
       " ('こんにちは', 0.6762766242027283),\n",
       " ('やぁ', 0.6219089031219482),\n",
       " ('おやすみなさい', 0.6146820187568665),\n",
       " ('母さん', 0.6144062280654907),\n",
       " ('はじめまして', 0.6134737730026245),\n",
       " ('おかあさん', 0.6094452738761902),\n",
       " ('ありがとう', 0.6041801571846008),\n",
       " ('お父さん', 0.5938694477081299),\n",
       " ('おいで', 0.5913469791412354)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_model.wv.most_similar (\"こんにちわ\") \n",
    "# for element in model.most_similar(\"ありがとう\"):\n",
    "#      print(element [0] , element [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('soti', 0.7755337953567505),\n",
       " ('MCDisp', 0.7417123317718506),\n",
       " ('MCLink', 0.7239100933074951),\n",
       " ('HKEYLOCALMACHINE', 0.7149759531021118),\n",
       " ('Name', 0.7014230489730835),\n",
       " ('href', 0.7005501985549927),\n",
       " ('Launch', 0.6990233659744263),\n",
       " ('Local', 0.6928770542144775),\n",
       " ('System', 0.691757082939148),\n",
       " ('Camera', 0.6907753944396973)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_model.wv.most_similar (\"mobicontrol\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59298605"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1=\"こんにちわ\"\n",
    "s2=\"ありがとう\"\n",
    "distance = gensim_model.wv.n_similarity(s1.lower().split(), s2.lower().split())\n",
    "distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6332798224169414\n",
      "0.18655876518845044\n"
     ]
    }
   ],
   "source": [
    "def get_vector(text):\n",
    "    sum_vec = np.zeros(200)\n",
    "    word_count = 0\n",
    "    node = mt.parseToNode(text)\n",
    "    while node:\n",
    "        fields = node.feature.split(\",\")\n",
    "        # 名詞、動詞、形容詞に限定\n",
    "        if fields[0] == '名詞' or fields[0] == '動詞' or fields[0] == '形容詞':\n",
    "            try: \n",
    "                temp = gensim_model.wv[node.surface]\n",
    "            except KeyError:\n",
    "                temp = 0\n",
    "            sum_vec += temp\n",
    "            word_count += 1\n",
    "        node = node.next\n",
    "    return sum_vec / word_count\n",
    "\n",
    "# cos類似度を計算\n",
    "def cos_sim(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "v1 = get_vector('昨日、お笑い番組を見た。')\n",
    "v2 = get_vector('昨夜、テレビで漫才をやっていた。')\n",
    "v3 = get_vector('昨日、公園に行った。')\n",
    "\n",
    "print(cos_sim(v1, v2))\n",
    "print(cos_sim(v1, v3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6057180479829244\n"
     ]
    }
   ],
   "source": [
    "print(cos_sim(get_vector(\"mobicontrol settings\"), get_vector(\"wifi settings\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('テザリング', 0.7307436466217041),\n",
       " ('umts', 0.6860231161117554),\n",
       " ('dtcp', 0.6787638664245605),\n",
       " ('felica', 0.6723511815071106),\n",
       " ('mobile', 0.6712178587913513),\n",
       " ('mhl', 0.6692694425582886),\n",
       " ('bluetooth', 0.6688016653060913),\n",
       " ('airh', 0.6648038029670715),\n",
       " ('モデム', 0.6639145612716675),\n",
       " ('hw', 0.6635802388191223)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_model.wv.most_similar(\"wifi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/iftekhar/myworkplace/AI-system/retrieval_Model/Page_Ranking_Experiment/varieties_corpus/processed_word5perline_corpus.csv\")\n",
    "data = df.Text.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/iftekhar/myworkplace/AI-system/retrieval_Model/Page_Ranking_Experiment/varieties_corpus/processed_perPage_perLine.csv\")\n",
    "df.Data = df.Data.apply(lambda x: clean_text(str(x)))\n",
    "data = df.Data.values.tolist()\n",
    "cleaned_data = deep_clean(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    " def deep_clean(data):\n",
    "    clean_data = []\n",
    "    for items in ''.join(data).split():\n",
    "        if len(items) < 2:\n",
    "            items = re.sub(r'[ぁ-ん]', '', items)\n",
    "            items = re.sub(r'[ア-ン]', '', items)\n",
    "            items = re.sub(r'[A-Za-z]', '', items)\n",
    "            # print(items)\n",
    "            clean_data.append(items)\n",
    "        else:\n",
    "            clean_data.append(items)\n",
    "    return [cleaned for cleaned in clean_data if cleaned not in ['']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"corpus.txt\", \"w\") as outfile:\n",
    "     outfile.write(\"\\n\".join(str(item) for item in clean_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    replaced = text.replace(\"\\\\\",\"\")\n",
    "    replaced = replaced.replace(\"+\",\"\")\n",
    "    replaced = re.sub('\\W+',' ', replaced)\n",
    "    replaced = re.sub(r'￥', '', replaced)       # 【】の除去\n",
    "    replaced = re.sub(r'．', '', replaced)       # ・ の除去\n",
    "    replaced = re.sub(r'｣', '', replaced)     # （）の除去\n",
    "    replaced = re.sub(r'｢', '', replaced)   # ［］の除去\n",
    "    replaced = re.sub(r'～', '', replaced)  # メンションの除去\n",
    "    replaced = re.sub(r'｜', '', replaced)  # URLの除去\n",
    "    replaced = re.sub(r'＠', '', replaced)  # 全角空白の除去\n",
    "    replaced = re.sub(r'？', '', replaced) # 数字の除去\n",
    "    replaced = re.sub(r'％', '', replaced)\n",
    "    replaced = re.sub(r'＝', '', replaced)\n",
    "    replaced = re.sub(r'！', '', replaced)\n",
    "    replaced = re.sub(r'｝', '', replaced)\n",
    "    replaced = re.sub(r'：', '', replaced)\n",
    "    replaced = re.sub(r'－', '', replaced)\n",
    "    replaced = re.sub(r'･', '', replaced)\n",
    "    replaced = re.sub(r'ｔ', '', replaced)\n",
    "    replaced = re.sub(r'ｋ', '', replaced)\n",
    "    replaced = re.sub(r'ｄ', '', replaced)\n",
    "    return replaced\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old work below, will work later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"processed.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = \"ja.text.seg\" \n",
    "outp1 = \"ja.text.model\" \n",
    "outp2 = \"ja.text.vector\"\n",
    "out3 = \"ja.text.bin\"\n",
    "\n",
    "model = Word2Vec(data, min_count=1,size=200, window=5, sg=1, workers=multiprocessing.cpu_count(), iter=10)\n",
    "#model.save(outp1)\n",
    "#model.wv.save_word2vec_format(outp2, binary=False)\n",
    "#model.wv.save_word2vec_format(out3, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Query String: database setting\n"
     ]
    }
   ],
   "source": [
    "myinput = input(\"Enter Query String: \")\n",
    "s = myinput.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_saver=[]\n",
    "index_saver=[]\n",
    "distance_saver=[]\n",
    "d=0model = Word2Vec(data, min_count=1,size=200, window=5, sg=1, workers=multiprocessing.cpu_count(), iter=10)\n",
    "\n",
    "for i in range(len(df)):\n",
    "    g = only_data.values[i]\n",
    "    distance = model.wv.n_similarity(s,g)\n",
    "    #print(distance)\n",
    "    if d<distance:\n",
    "        d=distance\n",
    "        page_index = df.iloc[i,-1]\n",
    "        distance_saver.append(distance)\n",
    "        row_saver.append(i)\n",
    "        index_saver.append(page_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydf=pd.DataFrame(list(zip(index_saver,distance_saver,row_saver)),\n",
    "                  columns=['page_index','distance','Row_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "allT = []\n",
    "for i in mydf.Row_index.values:\n",
    "    #print(df.iloc[[i]].values)\n",
    "    allT.append(list(df.iloc[i,1:-1].values.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllText = pd.DataFrame()\n",
    "AllText['Text'] = allT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydf['Text'] = AllText['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_index</th>\n",
       "      <th>distance</th>\n",
       "      <th>Row_index</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.370014</td>\n",
       "      <td>0</td>\n",
       "      <td>[MobiControl, の, 設定, 順序, 端末, グループ, の, 作成, コンソー...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.406018</td>\n",
       "      <td>1</td>\n",
       "      <td>[の, 接続, プロファイル, を, 作成, し, て, おく, 全て, の, 端末, OS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.448790</td>\n",
       "      <td>17</td>\n",
       "      <td>[Inc, Canada, Translated, by, Penetrate, of, L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.464922</td>\n",
       "      <td>71</td>\n",
       "      <td>[の, 移動, ルール, データ, 収集, ルール, アラートルール, 通信, 費, 管理,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.482667</td>\n",
       "      <td>93</td>\n",
       "      <td>[の, インストール, と, サーバ, へ, の, 登録, 方法, について, は, 下記,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_index  distance  Row_index  \\\n",
       "0           0  0.370014          0   \n",
       "1           0  0.406018          1   \n",
       "2           0  0.448790         17   \n",
       "3           2  0.464922         71   \n",
       "4           2  0.482667         93   \n",
       "\n",
       "                                                Text  \n",
       "0  [MobiControl, の, 設定, 順序, 端末, グループ, の, 作成, コンソー...  \n",
       "1  [の, 接続, プロファイル, を, 作成, し, て, おく, 全て, の, 端末, OS...  \n",
       "2  [Inc, Canada, Translated, by, Penetrate, of, L...  \n",
       "3  [の, 移動, ルール, データ, 収集, ルール, アラートルール, 通信, 費, 管理,...  \n",
       "4  [の, インストール, と, サーバ, へ, の, 登録, 方法, について, は, 下記,...  "
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydf.Text=mydf.Text.apply(lambda x: ''.join(str(x).strip(\"[]\")))\n",
    "mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85121775"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance = 0.924\n"
     ]
    }
   ],
   "source": [
    "#load word2vec model, here GoogleNews is used\n",
    "Binmodel = gensim.models.KeyedVectors.load_word2vec_format('ja.text.bin', binary=True)\n",
    "#two sample sentences \n",
    "s1 = 'database setting'\n",
    "\n",
    "row_saver=[]\n",
    "index_saver=[]\n",
    "distance_saver=[]\n",
    "d=0\n",
    "for i in range(len(df)):\n",
    "    g = only_data.values[i]\n",
    "    #calculate distance between two sentences using WMD algorithm\n",
    "    distance = Binmodel.wmdistance(s1, g)\n",
    "    #print(distance)\n",
    "    if d<distance:\n",
    "        d=distance\n",
    "        page_index = df.iloc[i,-1]\n",
    "        distance_saver.append(distance)\n",
    "        row_saver.append(i)\n",
    "        index_saver.append(page_index)\n",
    "        \n",
    "print ('distance = %.3f' % distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99999994\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.utils import simple_preprocess    \n",
    "\n",
    "model = gensim.models.Word2Vec.load(\"ja.text.model\")\n",
    "\n",
    "def tidy_sentence(sentence, vocabulary):\n",
    "    return [word for word in simple_preprocess(sentence) if word in vocabulary]    \n",
    "\n",
    "def compute_sentence_similarity(sentence_1, sentence_2, model_wv):\n",
    "    vocabulary = set(model_wv.index2word)    \n",
    "    tokens_1 = tidy_sentence(sentence_1, vocabulary)    \n",
    "    tokens_2 = tidy_sentence(sentence_2, vocabulary)    \n",
    "    return model_wv.n_similarity(tokens_1, tokens_2)\n",
    "\n",
    "wv = KeyedVectors.load('ja.text.model', mmap='r')\n",
    "sim = compute_sentence_similarity('this is a sentence', 'this is also a sentence', model.wv)\n",
    "print(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ifte/amiebot_project/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.746703"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('database','google')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=5940, size=50, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
