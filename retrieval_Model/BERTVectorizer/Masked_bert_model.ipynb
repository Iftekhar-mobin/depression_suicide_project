{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for '/home/iftekhar/Downloads/BERT-base_mecab-ipadic-bpe-32k_whole-word-mask/new'. Make sure that:\n\n- '/home/iftekhar/Downloads/BERT-base_mecab-ipadic-bpe-32k_whole-word-mask/new' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or '/home/iftekhar/Downloads/BERT-base_mecab-ipadic-bpe-32k_whole-word-mask/new' is the correct path to a directory containing relevant tokenizer files\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-367fcbbad74b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load pre-trained tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertJapaneseTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/iftekhar/Downloads/BERT-base_mecab-ipadic-bpe-32k_whole-word-mask/new'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load pre-trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForMaskedLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/iftekhar/Downloads/BERT-base_mecab-ipadic-bpe-32k_whole-word-mask/new'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/exp_env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1696\u001b[0m                 \u001b[0;34mf\"- or '{pretrained_model_name_or_path}' is the correct path to a directory containing relevant tokenizer files\\n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m             )\n\u001b[0;32m-> 1698\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1700\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfile_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocab_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for '/home/iftekhar/Downloads/BERT-base_mecab-ipadic-bpe-32k_whole-word-mask/new'. Make sure that:\n\n- '/home/iftekhar/Downloads/BERT-base_mecab-ipadic-bpe-32k_whole-word-mask/new' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or '/home/iftekhar/Downloads/BERT-base_mecab-ipadic-bpe-32k_whole-word-mask/new' is the correct path to a directory containing relevant tokenizer files\n\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertJapaneseTokenizer, BertForMaskedLM\n",
    "\n",
    "# Load pre-trained tokenizer\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained('/home/iftekhar/Downloads/BERT-base_mecab-ipadic-bpe-32k_whole-word-mask/new')\n",
    "# Load pre-trained model\n",
    "model = BertForMaskedLM.from_pretrained('/home/iftekhar/Downloads/BERT-base_mecab-ipadic-bpe-32k_whole-word-mask/new')\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '宿泊代金の支払方法について教えてください。'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = ['宿泊代金', '支払方法', '教える']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['宿泊', '代金', 'の', '支払', '方法', 'について', '教え', 'て', 'ください', '。']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text = tokenizer.tokenize(query)\n",
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['テレビ', 'で', 'サッカー', 'の', '試合', 'を', '見る', '。']\n",
    "\n",
    "# Mask a token that we will try to predict back with `BertForMaskedLM`\n",
    "# masked_index = question.index(word)\n",
    "masked_index = 1\n",
    "tokenized_text[masked_index] = '[MASK]'\n",
    "# ['テレビ', 'で', '[MASK]', 'の', '試合', 'を', '見る', '。']\n",
    "\n",
    "# Convert token to vocabulary indices\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "# [571, 12, 4, 5, 608, 11, 2867, 8]\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "# tensor([[ 571,   12,    4,    5,  608,   11, 2867,    8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = outputs[0][0, masked_index].topk(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['教え']\n",
      "['教える']\n",
      "['学び']\n",
      "['を']\n",
      "['学ぶ']\n",
      "['生きる']\n",
      "['芸能']\n",
      "['教育']\n",
      "['学習']\n",
      "['も']\n"
     ]
    }
   ],
   "source": [
    "for i, index_t in enumerate(x.indices):\n",
    "    index = index_t.item()\n",
    "    print(tokenizer.convert_ids_to_tokens([index]))index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "1 野球\n",
      "2 3\n",
      "3 2\n",
      "4 試合\n",
      "5 スポーツ\n",
      "6 10\n",
      "7 18\n",
      "8 基本\n",
      "9 全\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor)\n",
    "    predictions = outputs[0][0, masked_index].topk(10)  # 予測結果の上位5件を抽出\n",
    "\n",
    "# Show results\n",
    "for i, index_t in enumerate(predictions.indices):\n",
    "    index = index_t.item()\n",
    "    token = tokenizer.convert_ids_to_tokens([index])[0]\n",
    "    print(i, token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying Next seq approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import BertJapaneseTokenizer\n",
    "from transformers import BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/iftekhar/Downloads/BERT-base_mecab-ipadic-bpe-32k_whole-word-mask/new were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained tokenizer\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained('/home/iftekhar/Downloads/BERT-base_mecab-ipadic-bpe-32k_whole-word-mask/new')\n",
    "# Load pre-trained model\n",
    "model = BertForMaskedLM.from_pretrained('/home/iftekhar/Downloads/BERT-base_mecab-ipadic-bpe-32k_whole-word-mask/new')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = '朝食に[MASK]を焼いて食べました。'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = tokenizer.encode(test, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2821, 6521, 893, 4, 932, 24183, 888, 12409, 13276, 881, 829, 3]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " '朝',\n",
       " '##食',\n",
       " 'に',\n",
       " '[MASK]',\n",
       " 'を',\n",
       " '焼い',\n",
       " 'て',\n",
       " '食べ',\n",
       " 'まし',\n",
       " 'た',\n",
       " '。',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = torch.tensor([token_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,  2821,  6521,   893,     4,   932, 24183,   888, 12409, 13276,\n",
       "           881,   829,     3]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "masked_index = torch.where(token_ids == tokenizer.mask_token_id)[1].tolist()[0]\n",
    "print(masked_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 朝食 に パン を 焼い て 食べ まし た 。 [SEP]\n",
      "[CLS] 朝食 に うどん を 焼い て 食べ まし た 。 [SEP]\n",
      "[CLS] 朝食 に 肉 を 焼い て 食べ まし た 。 [SEP]\n",
      "[CLS] 朝食 に 魚 を 焼い て 食べ まし た 。 [SEP]\n",
      "[CLS] 朝食 に ジャガイモ を 焼い て 食べ まし た 。 [SEP]\n"
     ]
    }
   ],
   "source": [
    "result = model(token_ids)\n",
    "pred_ids = result[0][:, masked_index].topk(5).indices.tolist()[0]\n",
    "for pred_id in pred_ids:\n",
    "    output_ids = token_ids.tolist()[0]\n",
    "    output_ids[masked_index] = pred_id\n",
    "    print(tokenizer.decode(output_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following code is for BERT download for masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertJapaneseTokenizer, BertForMaskedLM\n",
    "# from sentence_transformers import SentenceTransformer, models\n",
    "\n",
    "\n",
    "DEFAULT_PRIORITY = 1\n",
    "\n",
    "\n",
    "def load_bert(model_path):\n",
    "    tok = BertJapaneseTokenizer.from_pretrained(model_path)\n",
    "    masked_model = BertForMaskedLM.from_pretrained(model_path)\n",
    "    return {\"tokenizer\": tok, 'model': masked_model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'cl-tohoku/bert-large-japanese'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66288ceec6924844a834b5f9e4ed75bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/518 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27afc807bc8c4de0b0b79d68f5ebf2bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.35G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-large-japanese were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = load_bert(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('/home/iftekhar/BERTS/BERT_masking/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/iftekhar/BERTS/BERT_masking/tokenizer_config.json',\n",
       " '/home/iftekhar/BERTS/BERT_masking/special_tokens_map.json',\n",
       " '/home/iftekhar/BERTS/BERT_masking/vocab.txt',\n",
       " '/home/iftekhar/BERTS/BERT_masking/added_tokens.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('/home/iftekhar/BERTS/BERT_masking/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = model['tokenizer']\n",
    "model = model['model']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
