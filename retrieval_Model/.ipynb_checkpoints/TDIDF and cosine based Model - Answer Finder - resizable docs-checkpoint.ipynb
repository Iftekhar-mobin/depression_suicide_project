{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "import random\n",
    "import io\n",
    "import string\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import MeCab \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab = MeCab.Tagger('-Owakati')\n",
    "\n",
    "#p = Path(\"/home/ifte/amiebot_project/amie-HelpBot/mobicontrol_data/corpus_mobicontrol.csv\")\n",
    "df = pd.read_csv(\"/home/ifte/amiebot_project/amie-HelpBot/mobicontrol_data/Train_Test_Data/japanese/corpus_ver14.csv\", \n",
    "                 header=0, usecols=[\"page\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keyword Matching\n",
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
    "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
    "\n",
    "def greeting(sentence):\n",
    "    \"\"\"If user's input is a greeting, return a greeting response\"\"\"\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    replaced_text = re.sub(r'[【】]', ' ', text)       # 【】の除去\n",
    "    replaced_text = re.sub(r'[・_]', '', replaced_text)       # ・ の除去\n",
    "    replaced_text = re.sub(r'[（）()]', ' ', replaced_text)     # （）の除去\n",
    "    replaced_text = re.sub(r'[［］\\[\\]]', ' ', replaced_text)   # ［］の除去\n",
    "    replaced_text = re.sub(r'[@＠]\\w+', '', replaced_text)  # メンションの除去\n",
    "    replaced_text = re.sub(r'https?:\\/\\/.*?[\\r\\n ]', '', replaced_text)  # URLの除去\n",
    "    replaced_text = re.sub(r'　', ' ', replaced_text)  # 全角空白の除去\n",
    "    replaced_text = re.sub(r'\\d+', '', replaced_text) # 数字の除去\n",
    "    replaced_text = re.sub(r'[-/。,、.=]', ' ', replaced_text)\n",
    "    return replaced_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_ja = []\n",
    "STOPWORD_FILE = Path(\"/home/ifte/amiebot_project/amie-HelpBot/amie_helpbot/\" + '/assets/learning/stopword_ja.txt')\n",
    "with open(STOPWORD_FILE, encoding='utf-8') as fr:\n",
    "    stop_words_ja = fr.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[[\"page\"]]\n",
    "X = df[\"text\"].apply(lambda x: mecab.parse(x).strip(\"\\n\"))\n",
    "df['parsed'] = X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = df.page.unique()\n",
    "c_size = 100\n",
    "\n",
    "result = pd.DataFrame()\n",
    "\n",
    "for i in list(classes):\n",
    "    arr_index = df[df.page == i].parsed.values\n",
    "    data = ''.join(list(arr_index))\n",
    "    \n",
    "    all_data = clean_text(data)\n",
    "    \n",
    "    arr_words = np.array(all_data.split())\n",
    "    #print(arr_words) \n",
    "    \n",
    "    num = arr_words.shape[0]//c_size\n",
    "    full = c_size * num\n",
    "    rest = arr_words.shape[0] - full\n",
    "    \n",
    "    pad = np.zeros([c_size-rest], dtype=int)\n",
    "    sc = np.concatenate((arr_words,pad))\n",
    "    features = sc.reshape(num+1, c_size)\n",
    "    \n",
    "    df_v = pd.DataFrame(features)\n",
    "    df_v[\"cls\"] = i\n",
    "    \n",
    "    result = pd.concat([result, df_v]) \n",
    "\n",
    "    \n",
    "#print(result)\n",
    "#result.to_csv('processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ifte/amiebot_project/lib/python3.6/site-packages/ipykernel_launcher.py:3: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "Data_cols = result.iloc[:,:-1]\n",
    "tog = Data_cols.apply(lambda x: ' '.join(x), axis=1)\n",
    "tog.to_csv(\"tog.csv\")\n",
    "sent = list(tog.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7923"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True,\n",
       "                stop_words=['あそこ', 'あたり', 'あちら', 'あっち', 'あと', 'あな', 'あなた', 'あれ',\n",
       "                            'いくつ', 'いつ', 'いま', 'いや', 'いろいろ', 'うち', 'おおまか',\n",
       "                            'おまえ', 'おれ', 'がい', 'かく', 'かたち', 'かやの', 'から', 'がら',\n",
       "                            'きた', 'くせ', 'ここ', 'こっち', 'こと', 'ごと', 'こちら', ...],\n",
       "                strip_accents=None, sublinear_tf=False,\n",
       "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "                vocabulary=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 1), stop_words=stop_words_ja)\n",
    "vectorizer.fit(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(sent)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "df = pd.DataFrame(denselist, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5825"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5750"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stop_words_ja)\n",
    "vectors = vectorizer.fit_transform(sent)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter a question: \n",
      "vpn\n"
     ]
    }
   ],
   "source": [
    "# Read a question from the user\n",
    "question = [input('Please enter a question: \\n')]\n",
    "question = vectorizer.transform(question)\n",
    "\n",
    "# Rank all the questions using cosine similarity to the input question\n",
    "rank = cosine_similarity(question, vectorizer.transform(sent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PageID:  283 idx 5330 = し た 場合 パスワード を 入力 端末 識別 証明 書 右端 を クリック 認証 方式 に 「 証明 書 」 を 選択 し た 場合 に 入力 オンデマンド VPN を 有効 に する オンデマンド VPN を 可能 に する 場合 に チェック を 入れる 統合 さ れ た パラメータ 名 右端 を クリック 「 常に 接続 」 「 一 度 も 接続 し ない 」 「 必要 に 応じ て 接続 」 から 選択 項目 名 説 明 VPN 名 VPN の 名前 VPN サーバ IP アドレス VPN サーバ の ホスト 名 または IP アドレス ドメイン VPN サーバ の ドメイン\n",
      "\n",
      " ########## \n",
      "\n",
      "PageID:  194 idx 3615 = 参照 ください 図 の 中 の 「 VPN 」 を プルダウン する と 図 の よう に つ の VPN ベンダー の 名前 が 表示 さ れ ます 対応 し て いる VPN ベンダー を 選択 し ます 図 図 Pulse Secure の 設定 画面 VPN 名 VPN へ の アクセス 権 に対する アカウント 名 を 任意 に 命名 スペース と 特殊 文字 は 使用 不可 VPN サーバ の ホスト 名 IP アドレス VPN サーバ の ホスト 名 もしくは IP アドレス を 入力 ユーザ 名 VPN サーバ に ログイン する ため の ユーザ 名 AD など の LDAP の 下記\n",
      "\n",
      " ########## \n",
      "\n",
      "PageID:  282 idx 5242 = 右端 を クリック 認証 方式 に 「 証明 書 」 を 選択 し た 場合 に 入力 オンデマンド VPN を 有効 に する オンデマンド VPN を 可能 に する 場合 に チェック を 入れる 統合 さ れ た パラメータ 名 上 欄 に チェック を 入れ た 場合 この 欄 の 右端 を クリック 「 常に 接続 」 「 一 度 も 接続 し ない 」 「 必要 に 応じ て 接続 」 から 選択 項目 名 説 明 VPN 名 VPN の 名前 VPN サーバ IP アドレス VPN サーバ の ホスト 名 または IP アドレス ドメイン VPN サーバ の\n",
      "\n",
      " ########## \n",
      "\n",
      "PageID:  282 idx 5236 = 名 説 明 VPN 名 VPN の 名前 VPN サーバ IP アドレス VPN サーバ の ホスト 名 または IP アドレス ドメイン VPN サーバ の ドメイン ユーザ 名 ユーザ 名 を 入力 端末 の 登録 の 際 の 端末 認証 に ディレクトリ サービス を 使う と 次 の マクロ も 使用 可能 です : % EnrolledUser Upn % % EnrolledUser Domain % % EnrolledUser Username % % PromptUser % ユーザ 名 指定 を マクロ で する と 個別 端末 毎 に ユーザ 名 を 入力 し なく て 済み ます VPN グループ 名 VPN グループ 名 を 入力 VPN の 自動\n",
      "\n",
      " ########## \n",
      "\n",
      "PageID:  67 idx 1477 = VPN 」 > 「 VPN 接続 を 追加 する 」 を 選択 する と 図 が 現われ ます 図 の 「 Windows ビルトイン 」 の VPN クライアント を リモート から 設定 し ます 図 図 で 「 VPN ネイティブ プロファイル 」 を 選択 する と 図 が ポップアップ し ます 図 VPN ネーティブ 設定 画面 全般 接続 タイプ VPN プロトコル 右端 を クリック する と 次 の プロトコル を 選択 でき ます 自動 PPTP L TP IKEv VPN 名 VPN アカウント を 特定 する ため の 名前 を 入力 英数字 半角 スペース は 入れ ない VPN サーバ の ホスト 名\n",
      "\n",
      " ########## \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Grab the top 5\n",
    "top = np.argsort(rank, axis=-1).T[-5:]\n",
    "#print(top)\n",
    "\n",
    "# Print top 5\n",
    "for item in range(len(top)):\n",
    "    idx = top[item,0]\n",
    "    \n",
    "    # if the dataset file structure \"Quesition | Answer\" then use following line  \n",
    "    #print(data['Answer'].iloc[item].values[0])\n",
    "    print(\"PageID: \",result.iloc[idx,-1],\"idx\",idx,\"=\",tog.values[idx])\n",
    "    print(\"\\n ########## \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'TfidfVectorizer' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-52e2403e638f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Get TF-IDF weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'TfidfVectorizer' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Get TF-IDF weights\n",
    "weights = vectors[sent[0]]\n",
    "weights = [(dictionary[pair[0]], pair[1]) for pair in weights]\n",
    "\n",
    "# Initialize the word cloud\n",
    "wc = WordCloud(\n",
    "    background_color=\"white\",\n",
    "    max_words=2000,\n",
    "    width = 1024,\n",
    "    height = 720,\n",
    "    stopwords=stop_words_ja\n",
    ")\n",
    "\n",
    "# Generate the cloud\n",
    "wc.generate_from_frequencies(weights)\n",
    "\n",
    "# Save the could to a file\n",
    "wc.to_file(\"word_cloud.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
