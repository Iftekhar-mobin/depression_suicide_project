{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20535f8c",
   "metadata": {},
   "source": [
    "## https://blog.devgenius.io/transformers-for-text-summarization-a-step-by-step-tutorial-in-python-9d8e2c74233e\n",
    "### pip install transformers\n",
    "### pip install torch \n",
    "### pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17d663c",
   "metadata": {},
   "source": [
    "## Hugging Face provides a wide range of pre-trained models, including BERT, GPT-2, and T5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd381a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd0ff174ffa34c349449c9cef6c14f6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f5ce0c54af449c9b121c49338464f8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "059b8fb3336c41799b5af027250930a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c7643ae4e3f4fd3a85599c526bf7bee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bc5055f354c48a2b06fcd66e6ae5a28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b2ff63ba3a141d5b48ab078c2c30b5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0112ab75",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"\n",
    "Machine learning is a branch of artificial intelligence that allows computers to learn and improve from experience without being explicitly programmed. It is the process of using algorithms and statistical models to analyze and draw insights from large amounts of data, and then use those insights to make predictions or decisions. Machine learning has become increasingly popular in recent years, as the amount of available data has grown and computing power has increased. There are three main types of machine learning: supervised learning, unsupervised learning, and reinforcement learning. In supervised learning, the algorithm is given a labeled dataset and learns to make predictions based on that data. In unsupervised learning, the algorithm is given an unlabeled dataset and must find patterns and relationships within the data on its own. In reinforcement learning, the algorithm learns by trial and error, receiving feedback in the form of rewards or punishments for certain actions. Machine learning is used in a wide range of applications, including image recognition, natural language processing, autonomous vehicles, fraud detection, and recommendation systems. As the technology continues to improve, it is likely that machine learning will become even more prevalent in our daily lives.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79358aa5",
   "metadata": {},
   "source": [
    "### Tokenization \n",
    "##### We will be using the T5 tokenizer that we loaded earlier. encode() method:\n",
    "\n",
    "##### return_tensors='pt': This tells the method to return a PyTorch tensor instead of a list of integers.\n",
    "#####  max_length=512: This sets the maximum length of the input text to 512 tokens.\n",
    "#####  truncation=True: This tells the tokenizer to truncate the input text if it exceeds the maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2edbb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(input_text, return_tensors='pt', max_length=512, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1916246b",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_ids = model.generate(inputs,\n",
    "                              max_length=150,\n",
    "                              min_length=40,\n",
    "                              length_penalty=2.0,\n",
    "                              num_beams=4,\n",
    "                              early_stopping=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56d4ec7",
   "metadata": {},
   "source": [
    "#### The generate() method returns a tensor representation of the generated summary, which we can convert back to text using the decode() method of the tokenizer object.\n",
    "#### skip_special_tokens=True to remove any special tokens from the generated summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2fd3b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84168fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "', and reinforcement learning. In reinforcement learning, the algorithm learns by trial and error, receiving feedback in form of rewards or punishments for certain actions. Machine learning is used in a wide range of applications, including image recognition, natural language processing, autonomous vehicles, fraud detection, and recommendation systems.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
