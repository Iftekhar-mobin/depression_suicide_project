{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information Retrieval\n",
    "# Florida Atlantic University, Fall 2017\n",
    "# Justin Johnson jjohn273\n",
    "#\n",
    "# Assignment 3: Document Summarization\n",
    "# 1. Cluster docs4sum.txt into 10 clusters\n",
    "# 2. Calculate the centroid of each cluster\n",
    "# 3. Use centroid of each cluster to create a 10 sentence summary\n",
    "# 4. Explain choice of clustering and similarity\n",
    "\n",
    "# Notes\n",
    "# The data set contains several sentences that contain just 1 punctuation character, '?'\n",
    "# We will ignore these sentences to improve clustering algorithm\n",
    "\n",
    "import re\n",
    "import numpy\n",
    "from os import listdir\n",
    "from os.path import join, abspath\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from modules.TextPreProcessor import removeShortDocs\n",
    "from modules.TextPreProcessor import removeStopWords\n",
    "from modules.TextPreProcessor import stemSentences\n",
    "\n",
    "\n",
    "print \"\"\"\n",
    "Assignment 3: Document Summarization\n",
    "Information Retrieval: Florida Atlantic University, Fall 2017\n",
    "Justin J, jjohn273\n",
    "\"\"\"\n",
    "\n",
    "# define data set and parameters\n",
    "data_path = 'hw4data-docs4sum.txt'\n",
    "raw_data = open(data_path, 'r').read()\n",
    "ps = PorterStemmer()\n",
    "nltk_stop_words = set(stopwords.words('english'))\n",
    "cluster_count = 10\n",
    "min_sentence_length = 35\n",
    "\n",
    "\n",
    "####################################\n",
    "# PRE-PROCESSING\n",
    "####################################\n",
    "\n",
    "# split document into sentences and strip whitespace (delimeted by line)\n",
    "sentences = raw_data.split('\\n')\n",
    "sentences = map(lambda sentence: sentence.strip(), sentences)\n",
    "\n",
    "# remove sentences that do not contribute meaning by assuming short sentences have less meaning\n",
    "sentences = removeShortDocs(sentences, min_sentence_length)\n",
    "\n",
    "# remove stop words from all sentences\n",
    "processedSentences = removeStopWords(sentences, nltk_stop_words)\n",
    "\n",
    "# stem all tokens of all sentences\n",
    "processedSentences = stemSentences(sentences, ps)\n",
    "\n",
    "\n",
    "####################################\n",
    "# Apply K Means Clustering\n",
    "####################################\n",
    "\t\n",
    "# create tfidf matrix from the processed sentences\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(processedSentences)\n",
    "\n",
    "# cluster our tokenized sentences into 10 groups\n",
    "kMeansCluster = KMeans(n_clusters=cluster_count)\n",
    "kMeansCluster.fit(tfidf_matrix)\n",
    "clusters = kMeansCluster.labels_.tolist()\n",
    "\n",
    "\n",
    "####################################\n",
    "# Organize Cluster Results\n",
    "####################################\n",
    "\n",
    "# Create new dictionary that tracks which cluster each sentence belongs to\n",
    "# keeps copy of original sentences and stemmed sentences\n",
    "# sentenceDictionary { idx: { text: String, stemmed: String, cluster: Number } }\n",
    "sentenceDictionary = {}\n",
    "for idx, sentence in enumerate(sentences):\n",
    "\tsentenceDictionary[idx] = {}\n",
    "\tsentenceDictionary[idx]['text'] = sentence\n",
    "\tsentenceDictionary[idx]['cluster'] = clusters[idx]\n",
    "\tsentenceDictionary[idx]['stemmed'] = processedSentences[idx]\n",
    "\n",
    "# Create new dictionary that contains 1 entry for each cluster\n",
    "# each key in dictionary will point to array of sentences, all of which belong to that cluster\n",
    "# we attach the index to the sentenceDictionary object so we can recall the original sentence\n",
    "clusterDictionary = {}\n",
    "for key, sentence in sentenceDictionary.items():\n",
    "\tif sentence['cluster'] not in clusterDictionary:\n",
    "\t\tclusterDictionary[sentence['cluster']] = []\n",
    "\tclusterDictionary[sentence['cluster']].append(sentence['stemmed'])\n",
    "\tsentence['idx'] = len(clusterDictionary[sentence['cluster']]) - 1\n",
    "\t\t\n",
    "\t\t\n",
    "####################################\n",
    "# Calculate Cosine Similarity Scores\n",
    "####################################\t\t\n",
    "\n",
    "# For each cluster of sentences,\n",
    "# Find the sentence with highet cosine similarity over all sentences in cluster\n",
    "maxCosineScores = {}\n",
    "for key, clusterSentences in clusterDictionary.items():\n",
    "\tmaxCosineScores[key] = {}\n",
    "\tmaxCosineScores[key]['score'] = 0\n",
    "\ttfidf_matrix = vectorizer.fit_transform(clusterSentences)\n",
    "\tcos_sim_matrix = cosine_similarity(tfidf_matrix)\n",
    "\tfor idx, row in enumerate(cos_sim_matrix):\n",
    "\t\tsum = 0\n",
    "\t\tfor col in row:\n",
    "\t\t\tsum += col\n",
    "\t\tif sum > maxCosineScores[key]['score']:\n",
    "\t\t\tmaxCosineScores[key]['score'] = sum\n",
    "\t\t\tmaxCosineScores[key]['idx'] = idx\n",
    "\n",
    "\n",
    "\n",
    "####################################\n",
    "# Construct Document Summary\n",
    "####################################\t\n",
    "\n",
    "# for every cluster's max cosine score,\n",
    "# find the corresponding original sentence\n",
    "resultIndices = []\n",
    "i = 0\n",
    "for key, value in maxCosineScores.items():\n",
    "\tcluster = key\n",
    "\tidx = value['idx']\n",
    "\tstemmedSentence = clusterDictionary[cluster][idx]\n",
    "\t# key corresponds to the sentences index of the original document\n",
    "\t# we will use this key to sort our results in order of original document\n",
    "\tfor key, value in sentenceDictionary.items():\n",
    "\t\tif value['cluster'] == cluster and value['idx'] == idx:\n",
    "\t\t\tresultIndices.append(key)\n",
    "\n",
    "resultIndices.sort()\n",
    "\n",
    "# Iterate over sentences and construct summary output\n",
    "result = ''\n",
    "for idx in resultIndices:\n",
    "\tresult += sentences[idx] + ' '\n",
    "\t\t\n",
    "\n",
    "print result\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
