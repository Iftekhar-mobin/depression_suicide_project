{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea48e808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6980c1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e928f8d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "527c63a81ea34b1bbdc8f5465e61f96b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ve/main/spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3c202737f9e4e48bd8c127babc31355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0535aa37cb374670a2a1f9b1bd706455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2571c4f6f0ce40f9b7b991a8bfa53f7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/47.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AlbertTokenizer, AlbertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "model = AlbertModel.from_pretrained('albert-base-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a7b45ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Machine learning is a branch of artificial intelligence that allows computers to learn and improve from experience without being explicitly programmed. It is the process of using algorithms and statistical models to analyze and draw insights from large amounts of data, and then use those insights to make predictions or decisions. Machine learning has become increasingly popular in recent years, as the amount of available data has grown and computing power has increased. There are three main types of machine learning: supervised learning, unsupervised learning, and reinforcement learning. In supervised learning, the algorithm is given a labeled dataset and learns to make predictions based on that data. In unsupervised learning, the algorithm is given an unlabeled dataset and must find patterns and relationships within the data on its own. In reinforcement learning, the algorithm learns by trial and error, receiving feedback in the form of rewards or punishments for certain actions. Machine learning is used in a wide range of applications, including image recognition, natural language processing, autonomous vehicles, fraud detection, and recommendation systems. As the technology continues to improve, it is likely that machine learning will become even more prevalent in our daily lives.\n",
    "\"\"\"\n",
    "\n",
    "sentences = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e2002c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = [tokenizer.encode(sent, add_special_tokens=True) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "934d17c9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2,\n",
       "  1940,\n",
       "  2477,\n",
       "  25,\n",
       "  21,\n",
       "  1686,\n",
       "  16,\n",
       "  6809,\n",
       "  2872,\n",
       "  30,\n",
       "  2965,\n",
       "  7774,\n",
       "  20,\n",
       "  2484,\n",
       "  17,\n",
       "  3545,\n",
       "  37,\n",
       "  1496,\n",
       "  366,\n",
       "  142,\n",
       "  13108,\n",
       "  2866,\n",
       "  43,\n",
       "  9,\n",
       "  3],\n",
       " [2,\n",
       "  32,\n",
       "  25,\n",
       "  14,\n",
       "  953,\n",
       "  16,\n",
       "  568,\n",
       "  15935,\n",
       "  17,\n",
       "  6762,\n",
       "  2761,\n",
       "  20,\n",
       "  16051,\n",
       "  17,\n",
       "  2003,\n",
       "  9239,\n",
       "  18,\n",
       "  37,\n",
       "  370,\n",
       "  8545,\n",
       "  16,\n",
       "  1054,\n",
       "  15,\n",
       "  17,\n",
       "  94,\n",
       "  275,\n",
       "  273,\n",
       "  9239,\n",
       "  18,\n",
       "  20,\n",
       "  233,\n",
       "  13823,\n",
       "  18,\n",
       "  54,\n",
       "  6003,\n",
       "  9,\n",
       "  3],\n",
       " [2,\n",
       "  1940,\n",
       "  2477,\n",
       "  63,\n",
       "  533,\n",
       "  5054,\n",
       "  844,\n",
       "  19,\n",
       "  1764,\n",
       "  122,\n",
       "  15,\n",
       "  28,\n",
       "  14,\n",
       "  2006,\n",
       "  16,\n",
       "  904,\n",
       "  1054,\n",
       "  63,\n",
       "  3651,\n",
       "  17,\n",
       "  10626,\n",
       "  414,\n",
       "  63,\n",
       "  1644,\n",
       "  9,\n",
       "  3],\n",
       " [2,\n",
       "  80,\n",
       "  50,\n",
       "  132,\n",
       "  407,\n",
       "  2551,\n",
       "  16,\n",
       "  1940,\n",
       "  2477,\n",
       "  45,\n",
       "  15581,\n",
       "  2477,\n",
       "  15,\n",
       "  367,\n",
       "  8542,\n",
       "  3762,\n",
       "  69,\n",
       "  2477,\n",
       "  15,\n",
       "  17,\n",
       "  26374,\n",
       "  2477,\n",
       "  9,\n",
       "  3],\n",
       " [2,\n",
       "  19,\n",
       "  15581,\n",
       "  2477,\n",
       "  15,\n",
       "  14,\n",
       "  9083,\n",
       "  25,\n",
       "  504,\n",
       "  21,\n",
       "  14348,\n",
       "  1054,\n",
       "  3554,\n",
       "  17,\n",
       "  11346,\n",
       "  20,\n",
       "  233,\n",
       "  13823,\n",
       "  18,\n",
       "  432,\n",
       "  27,\n",
       "  30,\n",
       "  1054,\n",
       "  9,\n",
       "  3],\n",
       " [2,\n",
       "  19,\n",
       "  367,\n",
       "  8542,\n",
       "  3762,\n",
       "  69,\n",
       "  2477,\n",
       "  15,\n",
       "  14,\n",
       "  9083,\n",
       "  25,\n",
       "  504,\n",
       "  40,\n",
       "  367,\n",
       "  21018,\n",
       "  69,\n",
       "  1054,\n",
       "  3554,\n",
       "  17,\n",
       "  491,\n",
       "  477,\n",
       "  6282,\n",
       "  17,\n",
       "  5833,\n",
       "  363,\n",
       "  14,\n",
       "  1054,\n",
       "  27,\n",
       "  82,\n",
       "  258,\n",
       "  9,\n",
       "  3],\n",
       " [2,\n",
       "  19,\n",
       "  26374,\n",
       "  2477,\n",
       "  15,\n",
       "  14,\n",
       "  9083,\n",
       "  11346,\n",
       "  34,\n",
       "  2178,\n",
       "  17,\n",
       "  7019,\n",
       "  15,\n",
       "  3396,\n",
       "  13111,\n",
       "  19,\n",
       "  14,\n",
       "  505,\n",
       "  16,\n",
       "  7879,\n",
       "  18,\n",
       "  54,\n",
       "  6468,\n",
       "  18,\n",
       "  26,\n",
       "  1200,\n",
       "  3078,\n",
       "  9,\n",
       "  3],\n",
       " [2,\n",
       "  1940,\n",
       "  2477,\n",
       "  25,\n",
       "  147,\n",
       "  19,\n",
       "  21,\n",
       "  1051,\n",
       "  978,\n",
       "  16,\n",
       "  3767,\n",
       "  15,\n",
       "  215,\n",
       "  1961,\n",
       "  3514,\n",
       "  15,\n",
       "  1112,\n",
       "  816,\n",
       "  5511,\n",
       "  15,\n",
       "  8087,\n",
       "  3128,\n",
       "  15,\n",
       "  9531,\n",
       "  11643,\n",
       "  15,\n",
       "  17,\n",
       "  14066,\n",
       "  1242,\n",
       "  9,\n",
       "  3],\n",
       " [2,\n",
       "  28,\n",
       "  14,\n",
       "  1099,\n",
       "  2622,\n",
       "  20,\n",
       "  3545,\n",
       "  15,\n",
       "  32,\n",
       "  25,\n",
       "  1720,\n",
       "  30,\n",
       "  1940,\n",
       "  2477,\n",
       "  129,\n",
       "  533,\n",
       "  166,\n",
       "  91,\n",
       "  17584,\n",
       "  19,\n",
       "  318,\n",
       "  1954,\n",
       "  1551,\n",
       "  9,\n",
       "  3]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f26d223",
   "metadata": {},
   "source": [
    "### Encoding the input\n",
    "\n",
    "#### To feed the tokenized sentences into the BERT model, we need to encode them using the BERT tokenizer. We also need to add special tokens such as [CLS] (for the start of the sentence) and [SEP] (for the end of the sentence) to each input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "882100ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for i in tokenized_sentences:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded_sentences = []\n",
    "for i in tokenized_sentences:\n",
    "    while len(i) < max_len:\n",
    "        i.append(0)\n",
    "    padded_sentences.append(i)\n",
    "\n",
    "input_ids = torch.tensor(padded_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dded25",
   "metadata": {},
   "source": [
    "### Generating the sentence embeddings\n",
    "#### Once we have encoded the input, we can feed it into the BERT model to generate sentence embeddings. We will use the last hidden state of the BERT model as the sentence embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a1ab576",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids)[0]\n",
    "\n",
    "sentence_embeddings = []\n",
    "for i in range(len(sentences)):\n",
    "    sentence_embeddings.append(torch.mean(last_hidden_states[i], dim=0).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4f5992",
   "metadata": {},
   "source": [
    "### Summarizing the text\n",
    "##### Finally, we can use sentence embeddings to summarize the text. One way to do this is to compute the similarity between each sentence and the other sentences and select the sentences with the highest similarity scores. We will use the cosine similarity measure for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc6f241e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compute the similarity matrix\n",
    "similarity_matrix = cosine_similarity(sentence_embeddings)\n",
    "\n",
    "# Generate the summary\n",
    "num_sentences = 2\n",
    "summary_sentences = []\n",
    "for i in range(num_sentences):\n",
    "    sentence_scores = list(enumerate(similarity_matrix[i]))\n",
    "    \n",
    "sentence_scores = sorted(sentence_scores, key=lambda x: x[1], reverse=True)\n",
    "summary_sentences.append(sentences[sentence_scores[1][0]])\n",
    "\n",
    "summary = ' '.join(summary_sentences)\n",
    "# print(summary)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0990b9b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In unsupervised learning, the algorithm is given an unlabeled dataset and must find patterns and relationships within the data on its own.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4488ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
