{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import operator\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "Stopwords = set(stopwords.words('english'))\n",
    "wordlemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    regex = r'[^a-zA-Z0-9\\s]'\n",
    "    text = re.sub(regex,'',text)\n",
    "    return text\n",
    "\n",
    "def freq(words):\n",
    "    words = [word.lower() for word in words]\n",
    "    dict_freq = {}\n",
    "    words_unique = []\n",
    "    for word in words:\n",
    "        if word not in words_unique:\n",
    "            words_unique.append(word)\n",
    "    for word in words_unique:\n",
    "          dict_freq[word] = words.count(word)\n",
    "    return dict_freq\n",
    "\n",
    "def lemmatize_words(words):\n",
    "    lemmatized_words = []\n",
    "    for word in words:\n",
    "        lemmatized_words.append(wordlemmatizer.lemmatize(word))\n",
    "    return lemmatized_words\n",
    "\n",
    "def stem_words(words):\n",
    "    stemmed_words = []\n",
    "    for word in words:\n",
    "        stemmed_words.append(stemmer.stem(word))\n",
    "    return stemmed_words\n",
    "\n",
    "def pos_tagging(text):\n",
    "    pos_tag = nltk.pos_tag(text.split())\n",
    "    pos_tagged_noun_verb = []\n",
    "    for word,tag in pos_tag:\n",
    "        if tag == \"NN\" or tag == \"NNP\" or tag == \"NNS\" or tag == \"VB\" or tag == \"VBD\" or tag == \"VBG\" or tag == \"VBN\" or tag == \"VBP\" or tag == \"VBZ\":\n",
    "             pos_tagged_noun_verb.append(word)\n",
    "    return pos_tagged_noun_verb\n",
    "\n",
    "def tf_score(word,sentence):\n",
    "    freq_sum = 0\n",
    "    word_frequency_in_sentence = 0\n",
    "    len_sentence = len(sentence)\n",
    "    for word_in_sentence in sentence.split():\n",
    "        if word == word_in_sentence:\n",
    "            word_frequency_in_sentence = word_frequency_in_sentence + 1\n",
    "    tf =  word_frequency_in_sentence/ len_sentence\n",
    "    return tf\n",
    "\n",
    "def idf_score(no_of_sentences,word,sentences):\n",
    "    no_of_sentence_containing_word = 0\n",
    "    for sentence in sentences:\n",
    "        sentence = remove_special_characters(str(sentence))\n",
    "        sentence = re.sub(r'\\d+', '', sentence)\n",
    "        sentence = sentence.split()\n",
    "        sentence = [word for word in sentence if word.lower() not in Stopwords and len(word)>1]\n",
    "        sentence = [word.lower() for word in sentence]\n",
    "        sentence = [wordlemmatizer.lemmatize(word) for word in sentence]\n",
    "        if word in sentence:\n",
    "            no_of_sentence_containing_word = no_of_sentence_containing_word + 1\n",
    "    idf = math.log10(no_of_sentences/no_of_sentence_containing_word)\n",
    "    return idf\n",
    "\n",
    "def word_tfidf(dict_freq,word,sentences,sentence):\n",
    "    word_tfidf = []\n",
    "    tf = tf_score(word,sentence)\n",
    "    idf = idf_score(len(sentences),word,sentences)\n",
    "    tf_idf = tf*idf\n",
    "    return tf_idf\n",
    "\n",
    "def sentence_importance(sentence,dict_freq,sentences):\n",
    "    sentence_score = 0\n",
    "    sentence = remove_special_characters(str(sentence))\n",
    "    sentence = re.sub(r'\\d+', '', sentence)\n",
    "    pos_tagged_sentence = []\n",
    "    no_of_sentences = len(sentences)\n",
    "    pos_tagged_sentence = pos_tagging(sentence)\n",
    "    for word in pos_tagged_sentence:\n",
    "         if word.lower() not in Stopwords and word not in Stopwords    and len(word)>1:\n",
    "            word = word.lower() \n",
    "            word = wordlemmatizer.lemmatize(word)\n",
    "            sentence_score = sentence_score + word_tfidf(dict_freq,word,sentences,sentence)\n",
    "    return sentence_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of information to retain(in percent):10\n",
      "11\n",
      "\n",
      "\n",
      "Summary:\n",
      "Setting order MobiControl of setting order MobiControl of MobiControl is done in the following order:. This time, will also be produced at the same time Setup.INI file. Data and to the sending mobile device and the PC of content, you need to send the data and content. In the case of iOS, data for which to generate the data or terminal application terminal application is used. Can be specified folder under the umbrella of apps that add-on the iOS as a download destination target AndroidWindows system any folder. When you open the app catalog at the terminal, you will see the name and icon of some of the app. MobiControl package, using, to package the app. In addition to the app installer, you can insert a script command. As reference information, also, please refer. In the case of iOS, data for which to generate the data or terminal application terminal application is used. Can be specified folder under the umbrella of apps that add-on the iOS as a download destination target AndroidWindows system any folder.\n"
     ]
    }
   ],
   "source": [
    "file = 'Translated_text_mobicontrol_Page01_JP_to_En.txt'\n",
    "file = open(file , 'r')\n",
    "text = file.read()\n",
    "\n",
    "#text = 'Generating Reports SOTI MobiControl provides an extensive range of device and system reports to analyze your SOTI MobiControl deployment. The default reports cover many aspects of SOTI MobiControl, including but not limited to, device status and information, deployment server status and information, and administrative topics. Furthermore, you can create custom reports on other statistics that SOTI MobiControl tracks. The deployment server database is a standard SQL database that can be queried using a custom query. For more information on creating custom reports, contact SOTI MobiControl support. Reports can be generated in the following formats: Adobe PDF (.pdf) Comma separated values (.csv) Excel (.xls) Plain text (.txt) You can set SOTI MobiControl to automatically generate and disseminate reports. If you experience difficulties generating reports, make sure you have correctly activated SOTI MobiControl and that you have enabled TCP/IP in your SQL Server network configuration. Additionally, authentication of SQL Server must be performed using SQL Server Authentication; Windows Authentication is no longer supported. Tip: For a more flexible alternative, consider executing an Advanced Search and exporting the search results to a .csv file.'\n",
    "\n",
    "tokenized_sentence = sent_tokenize(text)\n",
    "#print(tokenized_sentence)\n",
    "\n",
    "text = remove_special_characters(str(text))\n",
    "text = re.sub(r'\\d+', '', text)\n",
    "tokenized_words_with_stopwords = word_tokenize(text)\n",
    "\n",
    "tokenized_words = [word for word in tokenized_words_with_stopwords if word not in Stopwords]\n",
    "tokenized_words = [word for word in tokenized_words if len(word) > 1]\n",
    "tokenized_words = [word.lower() for word in tokenized_words]\n",
    "tokenized_words = lemmatize_words(tokenized_words)\n",
    "word_freq = freq(tokenized_words)\n",
    "input_user = int(input('Percentage of information to retain(in percent):'))\n",
    "no_of_sentences = int((input_user * len(tokenized_sentence))/100)\n",
    "\n",
    "print(no_of_sentences)\n",
    "c = 1\n",
    "sentence_with_importance = {}\n",
    "for sent in tokenized_sentence:\n",
    "    sentenceimp = sentence_importance(sent,word_freq,tokenized_sentence)    \n",
    "    sentence_with_importance[c] = sentenceimp    \n",
    "    c = c+1\n",
    "\n",
    "sentence_with_importance = sorted(sentence_with_importance.items(), key=operator.itemgetter(1),reverse=True)\n",
    "cnt = 0\n",
    "summary = []\n",
    "sentence_no = []\n",
    "\n",
    "for word_prob in sentence_with_importance:\n",
    "    if cnt < no_of_sentences:\n",
    "        sentence_no.append(word_prob[0])\n",
    "        cnt = cnt+1\n",
    "    else:\n",
    "        break\n",
    "    \n",
    "sentence_no.sort()\n",
    "cnt = 1\n",
    "for sentence in tokenized_sentence:\n",
    "    if cnt in sentence_no:\n",
    "        summary.append(sentence)\n",
    "    cnt = cnt+1\n",
    "    \n",
    "summary = \" \".join(summary)\n",
    "print(\"\\n\")\n",
    "print(\"Summary:\")\n",
    "print(summary)\n",
    "# outF = open('summary.txt',\"w\")\n",
    "# outF.write(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
