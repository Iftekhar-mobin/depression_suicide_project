{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d79b8d3c",
   "metadata": {},
   "source": [
    "# https://tsmatz.wordpress.com/2022/11/25/huggingface-japanese-summarization/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4efbfbb",
   "metadata": {},
   "source": [
    "## XL-Sum Japanese dataset in Hugging Face, which is the annotated article-summary pairs in BBC news corpus.\n",
    "## This dataset has around 7000 samples for training in Japanese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9096eec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'url', 'title', 'summary', 'text'],\n",
       "        num_rows: 8102\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'url', 'title', 'summary', 'text'],\n",
       "        num_rows: 1012\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'url', 'title', 'summary', 'text'],\n",
       "        num_rows: 1012\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"csebuetnlp/xlsum\", name=\"bengali\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d665f459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>news-54690291</td>\n",
       "      <td>https://www.bbc.com/bengali/news-54690291</td>\n",
       "      <td>দুর্গাপূজার সময়ে যেভাবে শোক পালন করেন 'মহিষাস...</td>\n",
       "      <td>হিন্দু বাঙালীরা যে সময়ে তাদের সবথেকে বড় উৎসব...</td>\n",
       "      <td>দুর্গাপুজায় মহিষাসুর বধ্যে মধ্য দিয়ে অশুভর ও...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>news-50797621</td>\n",
       "      <td>https://www.bbc.com/bengali/news-50797621</td>\n",
       "      <td>রাশিয়ায় ক্ষমতার ২০ বছর যেভাবে কেটেছে ভ্লাদিম...</td>\n",
       "      <td>ভ্লাদিমির পুতিন তাঁর ক্ষমতায় থাকার ২০ বছর পূর...</td>\n",
       "      <td>গত ২০ বছরে তিনি রাশিয়ার প্রেসিডেন্ট এবং প্রধা...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>news-46678346</td>\n",
       "      <td>https://www.bbc.com/bengali/news-46678346</td>\n",
       "      <td>সংসদ নির্বাচন: বরিশালে ছয়টি আসন কিন্তু সবার দ...</td>\n",
       "      <td>বাংলাদেশের দক্ষিণাঞ্চলীয় জেলা বরিশাল এখন তুমু...</td>\n",
       "      <td>বরিশাল সদরে চলছে নির্বাচনী প্রচার প্রচারণা। যদ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>news-51860832</td>\n",
       "      <td>https://www.bbc.com/bengali/news-51860832</td>\n",
       "      <td>সর্বকালের সর্বশ্রেষ্ঠ বাঙালি: বিবিসি বাংলার জর...</td>\n",
       "      <td>দু'হাজার চার সালে বিবিসি বাংলা একটি 'শ্রোতা জর...</td>\n",
       "      <td>রবীন্দ্রনাথ ঠাকুর রবীন্দ্রনাথ ঠাকুর বাঙালির কা...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56310420</td>\n",
       "      <td>https://www.bbc.com/bengali/56310420</td>\n",
       "      <td>৭ই মার্চের ভাষণ: ৫০ বছর আগে রেসকোর্স ময়দানে উ...</td>\n",
       "      <td>'ভাষণ শুরু আগে মাথার উপর দিয়ে বিমান আর হেলিকপ...</td>\n",
       "      <td>আর কুমিল্লা থেকে বাস ভাড়া করে অনেকের সাথে নিজ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>news-52552857</td>\n",
       "      <td>https://www.bbc.com/bengali/news-52552857</td>\n",
       "      <td>করোনাভাইরাস: বাংলাদেশে মধ্যবিত্তের কি সঞ্চয়ে ...</td>\n",
       "      <td>করোনাভাইরাসের সংক্রমণ ঠেকাতে বাংলাদেশে গত ২৬শে...</td>\n",
       "      <td>শুধু নিম্ন নয়, মধ্য আয়ের মানুষের রোজগারেও কর...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>news-55301521</td>\n",
       "      <td>https://www.bbc.com/bengali/news-55301521</td>\n",
       "      <td>সন্তান বিক্রির ব্যবসা: কেনিয়ার কালোবাজারে ব্য...</td>\n",
       "      <td>বিবিসির আফ্রিকা আই অনুসন্ধান বিভাগ গত মাসে কেন...</td>\n",
       "      <td>আডামা তার গ্রামে ফিরে গেছেন। তিনি বলছেন, ''জীব...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>news-56325380</td>\n",
       "      <td>https://www.bbc.com/bengali/news-56325380</td>\n",
       "      <td>আমেরিকার কোন প্রেসিডেন্ট সবচেয়ে বেশি অসৎ মিথ্...</td>\n",
       "      <td>ডোনাল্ড ট্রাম্পের বিরুদ্ধে সবচেয়ে বড় অভিযোগ ...</td>\n",
       "      <td>আমেরিকার সাবেক কজন প্রেসিডেন্ট কিন্তু তাদের সা...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>news-53166128</td>\n",
       "      <td>https://www.bbc.com/bengali/news-53166128</td>\n",
       "      <td>করোনা ভাইরাস: যে কারণে চাহিদা বৃদ্ধির সুযোগ কা...</td>\n",
       "      <td>মার্চ মাসের শেষদিক থেকে জুনের শুরু পর্যন্ত দুই...</td>\n",
       "      <td>বাংলাদেশে গত কয়েকবছর ধরে ফুডপান্ডা, হাঙ্গরি ন...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>news-51091547</td>\n",
       "      <td>https://www.bbc.com/bengali/news-51091547</td>\n",
       "      <td>সিরিয়ার কুর্দি নেতা হেভরিন খালাফের হত্যাকাণ্ড...</td>\n",
       "      <td>বিবিসির নিউজ অ্যারাবিকের তদন্তে জোরালো প্রমাণ ...</td>\n",
       "      <td>ইতালিতে বিক্ষোভকারীরা হেভরিন খালফের স্মরণে হাত...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                        url  \\\n",
       "0  news-54690291  https://www.bbc.com/bengali/news-54690291   \n",
       "1  news-50797621  https://www.bbc.com/bengali/news-50797621   \n",
       "2  news-46678346  https://www.bbc.com/bengali/news-46678346   \n",
       "3  news-51860832  https://www.bbc.com/bengali/news-51860832   \n",
       "4       56310420       https://www.bbc.com/bengali/56310420   \n",
       "5  news-52552857  https://www.bbc.com/bengali/news-52552857   \n",
       "6  news-55301521  https://www.bbc.com/bengali/news-55301521   \n",
       "7  news-56325380  https://www.bbc.com/bengali/news-56325380   \n",
       "8  news-53166128  https://www.bbc.com/bengali/news-53166128   \n",
       "9  news-51091547  https://www.bbc.com/bengali/news-51091547   \n",
       "\n",
       "                                               title  \\\n",
       "0  দুর্গাপূজার সময়ে যেভাবে শোক পালন করেন 'মহিষাস...   \n",
       "1  রাশিয়ায় ক্ষমতার ২০ বছর যেভাবে কেটেছে ভ্লাদিম...   \n",
       "2  সংসদ নির্বাচন: বরিশালে ছয়টি আসন কিন্তু সবার দ...   \n",
       "3  সর্বকালের সর্বশ্রেষ্ঠ বাঙালি: বিবিসি বাংলার জর...   \n",
       "4  ৭ই মার্চের ভাষণ: ৫০ বছর আগে রেসকোর্স ময়দানে উ...   \n",
       "5  করোনাভাইরাস: বাংলাদেশে মধ্যবিত্তের কি সঞ্চয়ে ...   \n",
       "6  সন্তান বিক্রির ব্যবসা: কেনিয়ার কালোবাজারে ব্য...   \n",
       "7  আমেরিকার কোন প্রেসিডেন্ট সবচেয়ে বেশি অসৎ মিথ্...   \n",
       "8  করোনা ভাইরাস: যে কারণে চাহিদা বৃদ্ধির সুযোগ কা...   \n",
       "9  সিরিয়ার কুর্দি নেতা হেভরিন খালাফের হত্যাকাণ্ড...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  হিন্দু বাঙালীরা যে সময়ে তাদের সবথেকে বড় উৎসব...   \n",
       "1  ভ্লাদিমির পুতিন তাঁর ক্ষমতায় থাকার ২০ বছর পূর...   \n",
       "2  বাংলাদেশের দক্ষিণাঞ্চলীয় জেলা বরিশাল এখন তুমু...   \n",
       "3  দু'হাজার চার সালে বিবিসি বাংলা একটি 'শ্রোতা জর...   \n",
       "4  'ভাষণ শুরু আগে মাথার উপর দিয়ে বিমান আর হেলিকপ...   \n",
       "5  করোনাভাইরাসের সংক্রমণ ঠেকাতে বাংলাদেশে গত ২৬শে...   \n",
       "6  বিবিসির আফ্রিকা আই অনুসন্ধান বিভাগ গত মাসে কেন...   \n",
       "7  ডোনাল্ড ট্রাম্পের বিরুদ্ধে সবচেয়ে বড় অভিযোগ ...   \n",
       "8  মার্চ মাসের শেষদিক থেকে জুনের শুরু পর্যন্ত দুই...   \n",
       "9  বিবিসির নিউজ অ্যারাবিকের তদন্তে জোরালো প্রমাণ ...   \n",
       "\n",
       "                                                text  \n",
       "0  দুর্গাপুজায় মহিষাসুর বধ্যে মধ্য দিয়ে অশুভর ও...  \n",
       "1  গত ২০ বছরে তিনি রাশিয়ার প্রেসিডেন্ট এবং প্রধা...  \n",
       "2  বরিশাল সদরে চলছে নির্বাচনী প্রচার প্রচারণা। যদ...  \n",
       "3  রবীন্দ্রনাথ ঠাকুর রবীন্দ্রনাথ ঠাকুর বাঙালির কা...  \n",
       "4  আর কুমিল্লা থেকে বাস ভাড়া করে অনেকের সাথে নিজ...  \n",
       "5  শুধু নিম্ন নয়, মধ্য আয়ের মানুষের রোজগারেও কর...  \n",
       "6  আডামা তার গ্রামে ফিরে গেছেন। তিনি বলছেন, ''জীব...  \n",
       "7  আমেরিকার সাবেক কজন প্রেসিডেন্ট কিন্তু তাদের সা...  \n",
       "8  বাংলাদেশে গত কয়েকবছর ধরে ফুডপান্ডা, হাঙ্গরি ন...  \n",
       "9  ইতালিতে বিক্ষোভকারীরা হেভরিন খালফের স্মরণে হাত...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = display(ds[\"train\"].select(range(10)).to_pandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d70fa34",
   "metadata": {},
   "source": [
    "## csebuetnlp/\n",
    "#### banglat5_small\n",
    "#### banglabert_small \n",
    "#### banglabert_large\n",
    "#### banglabert\n",
    "\n",
    "## Dataset\n",
    "#### https://huggingface.co/datasets/csebuetnlp/xlsum\n",
    "#### https://huggingface.co/csebuetnlp/mT5_multilingual_XLSum\n",
    "#### https://huggingface.co/datasets/csebuetnlp/CrossSum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f666c39f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd0ca057a47f4515b4e0d4dfe9ea3781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/82.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e800a86579be44f1b8d4a16205418e51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/553 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e788b3123b34facb08cd9ba3b4f3545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ve/main/spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "547ef32d2b154ed1aa72498d02e397ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "C:\\Users\\iftek\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "t5_tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "963c7afe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e842a04cf37e46f990908436a8af44cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8102 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99af54a9953f4c9db563c59eefe52c80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1012 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e901b74675644b6b7c94679fe5131b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1012 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_sample_data(data):\n",
    "  # Max token size is 14536 and 215 for inputs and labels, respectively.\n",
    "  # Here I restrict these token size.\n",
    "  input_feature = t5_tokenizer(data[\"text\"], truncation=True, max_length=1024)\n",
    "  label = t5_tokenizer(data[\"summary\"], truncation=True, max_length=128)\n",
    "  return {\n",
    "    \"input_ids\": input_feature[\"input_ids\"],\n",
    "    \"attention_mask\": input_feature[\"attention_mask\"],\n",
    "    \"labels\": label[\"input_ids\"],\n",
    "  }\n",
    "\n",
    "tokenized_ds = ds.map(\n",
    "  tokenize_sample_data,\n",
    "  remove_columns=[\"id\", \"url\", \"title\", \"summary\", \"text\"],\n",
    "  batched=True,\n",
    "  batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43f749f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 8102\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1012\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1012\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76699b7a",
   "metadata": {},
   "source": [
    "### Before fine-tuning, load pre-trained model and data collator.\n",
    "\n",
    "### In HuggingFace, several sizes of mT5 models are available, and here I’ll use small one (google/mt5-small) to fit to memory in my machine.\n",
    "### The name is “small”, but it’s still so large (over 1 GB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a218f20b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9591c562c204b68825573488388e11b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83445ae09f0f41bfb44c33afb20cdb9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoConfig, AutoModelForSeq2SeqLM\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# see https://huggingface.co/docs/transformers/main_classes/configuration\n",
    "mt5_config = AutoConfig.from_pretrained(\n",
    "  \"google/mt5-small\",\n",
    "  max_length=128,\n",
    "  length_penalty=0.6,\n",
    "  no_repeat_ngram_size=2,\n",
    "  num_beams=15,\n",
    ")\n",
    "model = (AutoModelForSeq2SeqLM\n",
    "         .from_pretrained(\"google/mt5-small\", config=mt5_config)\n",
    "         .to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fbe55e",
   "metadata": {},
   "source": [
    "### For the sequence-to-sequence (seq2seq) task, we need to not only stack the inputs for encoder, but also prepare for the decoder side. In seq2seq setup, a common technique called “teach forcing” will then be applied in decoder.\n",
    "### In Hugging Face, these tasks are not needed to be manually setup and the following DataCollatorForSeq2Seq will take care of all steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77b7c89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "  t5_tokenizer,\n",
    "  model=model,\n",
    "  return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7132f6ab",
   "metadata": {},
   "source": [
    "### The idea of ROUGE is similar to BLEU, but it also measures how many of n-gram tokens in the reference text appears in the generated (predicted) text. (This is why the name of ROUGE includes “RO”, which means “Recall-Oriented”.)\n",
    "### There also exist variations, ROUGE-L and ROUGE-Lsum, which also counts the longest common substrings (LCS) in ROUGE metrics computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c09b4709",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "# define function for custom tokenization\n",
    "def tokenize_sentence(arg):\n",
    "  encoded_arg = t5_tokenizer(arg)\n",
    "  return t5_tokenizer.convert_ids_to_tokens(encoded_arg.input_ids)\n",
    "\n",
    "# define function to get ROUGE scores with custom tokenization\n",
    "def metrics_func(eval_arg):\n",
    "  preds, labels = eval_arg\n",
    "  # Replace -100\n",
    "  labels = np.where(labels != -100, labels, t5_tokenizer.pad_token_id)\n",
    "  # Convert id tokens to text\n",
    "  text_preds = t5_tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "  text_labels = t5_tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "  # Insert a line break (\\n) in each sentence for ROUGE scoring\n",
    "  # (Note : Please change this code, when you perform on other languages except for Japanese)\n",
    "  text_preds = [(p if p.endswith((\"!\", \"！\", \"?\", \"？\", \"。\")) else p + \"。\") for p in text_preds]\n",
    "  text_labels = [(l if l.endswith((\"!\", \"！\", \"?\", \"？\", \"。\")) else l + \"。\") for l in text_labels]\n",
    "  sent_tokenizer_jp = RegexpTokenizer(u'[^!！?？。]*[!！?？。]')\n",
    "  text_preds = [\"\\n\".join(np.char.strip(sent_tokenizer_jp.tokenize(p))) for p in text_preds]\n",
    "  text_labels = [\"\\n\".join(np.char.strip(sent_tokenizer_jp.tokenize(l))) for l in text_labels]\n",
    "  # compute ROUGE score with custom tokenization\n",
    "  return rouge_metric.compute(\n",
    "    predictions=text_preds,\n",
    "    references=text_labels,\n",
    "    tokenizer=tokenize_sentence\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f010c7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\iftek\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\generation\\utils.py:1421: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.11761273733876473,\n",
       " 'rouge2': 0.06261264940510224,\n",
       " 'rougeL': 0.11826839826839827,\n",
       " 'rougeLsum': 0.11761273733876473}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "sample_dataloader = DataLoader(\n",
    "  tokenized_ds[\"test\"].with_format(\"torch\"),\n",
    "  collate_fn=data_collator,\n",
    "  batch_size=5)\n",
    "for batch in sample_dataloader:\n",
    "  with torch.no_grad():\n",
    "    preds = model.generate(\n",
    "      batch[\"input_ids\"].to(device),\n",
    "      num_beams=15,\n",
    "      num_return_sequences=1,\n",
    "      no_repeat_ngram_size=1,\n",
    "      remove_invalid_values=True,\n",
    "      max_length=128,\n",
    "    )\n",
    "  labels = batch[\"labels\"]\n",
    "  break\n",
    "\n",
    "metrics_func([preds, labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5931eb4",
   "metadata": {},
   "source": [
    "### In usual training evaluation, training loss and accuracy will be computed and evaluated, by comparing the generated logits with labels. However, as we saw above, we want to evaluate ROUGE score using the predicted tokens.\n",
    "### To simplify these sequence-to-sequence specific steps, here I use built-in Seq2SeqTrainingArguments and Seq2SeqTrainer classes, instead of usual TrainingArguments and Trainer.\n",
    "### By setting predict_with_generate=True as follows in this class, the predicted tokens will be generated by model.generate() and it will then be passed into evaluation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f034dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "  output_dir = \"mt5-summarize-Bengali\",\n",
    "  log_level = \"error\",\n",
    "  num_train_epochs = 10,\n",
    "  learning_rate = 5e-4,\n",
    "  lr_scheduler_type = \"linear\",\n",
    "  warmup_steps = 90,\n",
    "  optim = \"adafactor\",\n",
    "  weight_decay = 0.01,\n",
    "  per_device_train_batch_size = 2,\n",
    "  per_device_eval_batch_size = 1,\n",
    "  gradient_accumulation_steps = 16,\n",
    "  evaluation_strategy = \"steps\",\n",
    "  eval_steps = 100,\n",
    "  predict_with_generate=True,\n",
    "  generation_max_length = 128,\n",
    "  save_steps = 500,\n",
    "  logging_steps = 10,\n",
    "  push_to_hub = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03056f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "  model = model,\n",
    "  args = training_args,\n",
    "  data_collator = data_collator,\n",
    "  compute_metrics = metrics_func,\n",
    "  train_dataset = tokenized_ds[\"train\"],\n",
    "  eval_dataset = tokenized_ds[\"validation\"].select(range(20)),\n",
    "  tokenizer = t5_tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fef0593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "# save fine-tuned model in local\n",
    "os.makedirs(\"./trained_for_summarization_jp\", exist_ok=True)\n",
    "if hasattr(trainer.model, \"module\"):\n",
    "  trainer.model.module.save_pretrained(\"./trained_for_summarization_jp\")\n",
    "else:\n",
    "  trainer.model.save_pretrained(\"./trained_for_summarization_jp\")\n",
    "\n",
    "# load local model\n",
    "model = (AutoModelForSeq2SeqLM\n",
    "         .from_pretrained(\"./trained_for_summarization_jp\")\n",
    "         .to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c5f9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Predict with test data (first 5 rows)\n",
    "sample_dataloader = DataLoader(\n",
    "  tokenized_ds[\"test\"].with_format(\"torch\"),\n",
    "  collate_fn=data_collator,\n",
    "  batch_size=5)\n",
    "for batch in sample_dataloader:\n",
    "  with torch.no_grad():\n",
    "    preds = model.generate(\n",
    "      batch[\"input_ids\"].to(device),\n",
    "      num_beams=15,\n",
    "      num_return_sequences=1,\n",
    "      no_repeat_ngram_size=1,\n",
    "      remove_invalid_values=True,\n",
    "      max_length=128,\n",
    "    )\n",
    "  labels = batch[\"labels\"]\n",
    "  break\n",
    "\n",
    "# Replace -100 (see above)\n",
    "labels = np.where(labels != -100, labels, t5_tokenizer.pad_token_id)\n",
    "\n",
    "# Convert id tokens to text\n",
    "text_preds = t5_tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "text_labels = t5_tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "# Show result\n",
    "print(\"***** Input's Text *****\")\n",
    "print(ds[\"test\"][\"text\"][2])\n",
    "print(\"***** Summary Text (True Value) *****\")\n",
    "print(text_labels[2])\n",
    "print(\"***** Summary Text (Generated Text) *****\")\n",
    "print(text_preds[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c998828f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
