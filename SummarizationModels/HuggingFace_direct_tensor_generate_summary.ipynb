{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20535f8c",
   "metadata": {},
   "source": [
    "## https://blog.devgenius.io/transformers-for-text-summarization-a-step-by-step-tutorial-in-python-9d8e2c74233e\n",
    "### pip install transformers\n",
    "### pip install torch \n",
    "### pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17d663c",
   "metadata": {},
   "source": [
    "## Hugging Face provides a wide range of pre-trained models, including BERT, GPT-2, and T5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd381a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd0ff174ffa34c349449c9cef6c14f6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f5ce0c54af449c9b121c49338464f8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "059b8fb3336c41799b5af027250930a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c7643ae4e3f4fd3a85599c526bf7bee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bc5055f354c48a2b06fcd66e6ae5a28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b2ff63ba3a141d5b48ab078c2c30b5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0112ab75",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"\n",
    "Machine learning is a branch of artificial intelligence that allows computers to learn and improve from experience without being explicitly programmed. It is the process of using algorithms and statistical models to analyze and draw insights from large amounts of data, and then use those insights to make predictions or decisions. Machine learning has become increasingly popular in recent years, as the amount of available data has grown and computing power has increased. There are three main types of machine learning: supervised learning, unsupervised learning, and reinforcement learning. In supervised learning, the algorithm is given a labeled dataset and learns to make predictions based on that data. In unsupervised learning, the algorithm is given an unlabeled dataset and must find patterns and relationships within the data on its own. In reinforcement learning, the algorithm learns by trial and error, receiving feedback in the form of rewards or punishments for certain actions. Machine learning is used in a wide range of applications, including image recognition, natural language processing, autonomous vehicles, fraud detection, and recommendation systems. As the technology continues to improve, it is likely that machine learning will become even more prevalent in our daily lives.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79358aa5",
   "metadata": {},
   "source": [
    "### Tokenization \n",
    "##### We will be using the T5 tokenizer that we loaded earlier. encode() method:\n",
    "\n",
    "##### return_tensors='pt': This tells the method to return a PyTorch tensor instead of a list of integers.\n",
    "#####  max_length=512: This sets the maximum length of the input text to 512 tokens.\n",
    "#####  truncation=True: This tells the tokenizer to truncate the input text if it exceeds the maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2edbb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(input_text, return_tensors='pt', max_length=512, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1916246b",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_ids = model.generate(inputs,\n",
    "                              max_length=150,\n",
    "                              min_length=40,\n",
    "                              length_penalty=2.0,\n",
    "                              num_beams=4,\n",
    "                              early_stopping=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56d4ec7",
   "metadata": {},
   "source": [
    "#### The generate() method returns a tensor representation of the generated summary, which we can convert back to text using the decode() method of the tokenizer object.\n",
    "#### skip_special_tokens=True to remove any special tokens from the generated summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2fd3b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84168fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "', and reinforcement learning. In reinforcement learning, the algorithm learns by trial and error, receiving feedback in form of rewards or punishments for certain actions. Machine learning is used in a wide range of applications, including image recognition, natural language processing, autonomous vehicles, fraud detection, and recommendation systems.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69dcd045",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"\"\"Like so many problem-solving experiments conducted over the years, the answer depends not on \n",
    " intelligence or skill but perspective, feedback, and circumstance. Small manipulations, such as offering money to \n",
    " participants for generating more anagrams or revealing the average number of anagrams other participants completed, \n",
    " can drastically sway results. \n",
    " \n",
    " In recent years, social psychologist Heidi Grant Halvorson, the associate director of the Motivation Science Centre \n",
    " at Columbia Business School, has used the anagram puzzle to study how people focus. Across dozens of experiments and \n",
    " articles, Halvorson has revealed that we approach problems in one of two ways. “If you are promotion-focused, \n",
    " you want to advance and avoid missed opportunities. If you are prevention-focused, you want to minimize losses and \n",
    " keep things working.\" Which one is more effective? \n",
    " \n",
    " Halvorson teamed with Jens Forster and Lorraine Chen Idson to find out. In an experiment conduct several years ago, \n",
    " they gathered 109 participants and divided them into two groups. Those in the promotion condition received four \n",
    " dollars and the chance to earn an extra dollar if they scored above the 70th percentile on the anagram task. Their \n",
    " peers in the prevention condition received five dollars, but if their performance dropped below the 70th percentile, \n",
    " they risked losing a dollar. \n",
    " \n",
    " On paper, each condition was the same: every participant would receive at least four dollars. The difference is how \n",
    " Halvorson and her team framed the experiment. In the promotion condition, success meant gaining a dollar; in the \n",
    " prevention condition, it meant avoiding losing a dollar. \n",
    " \n",
    " This subtle manipulation had a big impact. Halfway through the experiment, Halvorson told every participant that they \n",
    " were performing either above or below the 70th percentile (the researchers randomly assigned the feedback). The \n",
    " participants in the promotion-focused group took positive feedback well--it boosted their expectations and \n",
    " motivation--while those in the prevention-focused group did not. Their motivation decreased. When the news \n",
    " was bad, the responses flipped. In promotion-focused group, expectations of success and motivation went down. \n",
    " Expectations in the prevention-focused groups dropped dramatically but motivation surged. \n",
    " \n",
    " This finding suggests that when we focus on gaining something, positive feedback helps us persist until we complete a \n",
    " problem. If, on the other hand, we dwell on the possibility of failure, negative feedback can also stimulate \n",
    " motivation and boost performance. We're more willing to stick with it when we think there's something to lose. \n",
    " \n",
    " \"Aren't we supposed to banish negative thoughts if we want to succeed?\" Halvorson asks in her book Focus: Use \n",
    " Different Ways of Seeing the World for Success and Influence (co-authored with E. Tory Higgins). \n",
    " \n",
    " \"Not if you are prevention-focused or are pursuing a prevention-focused goal. Because if you are, optimism not only \n",
    " feels wrong--it will disrupt and dampen your motivation. If you are sure that everything is going to work out \n",
    " for you, then why would you go out of your way to avoid mistakes or to plan your way around obstacles or two come up \n",
    " with plan B?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98f264d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5Config, T5ForConditionalGeneration,GPT2Tokenizer, GPT2LMHeadModel,BartForConditionalGeneration,BartTokenizer,BartConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a45c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sumT5(aritcle):\n",
    "    # Loading the model and tokenizer for t5-small\n",
    "    my_model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "    tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "    text = \"summarize:\" + article\n",
    "\n",
    "    # encoding the input text\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt', max_length=512, truncation=True)\n",
    "    summary_ids = my_model.generate(input_ids)\n",
    "\n",
    "    # Decoding and printing the summary\n",
    "    t5_summary = tokenizer.decode(summary_ids[0])\n",
    "    print(\"T-5 Summary: \"+t5_summary)\n",
    "\n",
    "def sumbart(aritcle):\n",
    "    # Loading the model and tokenizer for bart-large-cnn\n",
    "    tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "    model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "    # Encoding the inputs and passing them to model.generate()\n",
    "    inputs = tokenizer.batch_encode_plus([article], return_tensors='pt')\n",
    "    summary_ids = model.generate(inputs['input_ids'])\n",
    "\n",
    "    # Decoding and printing the summary\n",
    "    bart_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    print(\"BART Summary: \"+bart_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c4aecf",
   "metadata": {},
   "source": [
    "#### https://www.analyticsvidhya.com/blog/2022/01/youtube-summariser-mini-nlp-project/?utm_source=related_WP&utm_medium=https://www.analyticsvidhya.com/blog/2023/07/exploring-gpt-2-and-xlnet-transformers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ee75a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "input_tensor = tokenizer.encode( subtitle, return_tensors=\"pt\", max_length=512)\n",
    "\n",
    "outputs_tensor = model.generate(input_tensor, max_length=160, min_length=120, length_penalty=2.0, num_beams=4, early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88fe881",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(outputs_tensor[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
