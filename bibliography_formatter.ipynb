{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c9cc300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Sample input bibliography string\n",
    "bibliography = r\"\"\"\n",
    "\\bibitem{elkassas2021automatic}\n",
    "\\begin{thebibliography}{100}\n",
    "\n",
    "\\bibitem{elkassas2021automatic}\n",
    "El W S-Kassas, Salama C R, Rafea A A, and Mohamed H K, ``Automatic text\n",
    "  summarization: A comprehensive survey,'' {\\em Expert Systems with\n",
    "  Applications}, vol. 165, p 113679, 2021.\n",
    "\n",
    "\\bibitem{widyassari2022review}\n",
    "Widyassari A P, Rustad S, Shidik G F, Noersasongko E, A Syukur, and\n",
    "  A Affandy, ``Review of automatic text summarization techniques \\& methods,''\n",
    "  {\\em Journal of King Saud University-Computer and Information Sciences},\n",
    "  vol. 34, no. 4, pp. 1029--1046, 2022.\n",
    "\n",
    "\\bibitem{ferreira2013assessing}\n",
    "Ferreira R, L de Souza Cabral, Lins R D, e G P Silva, Freitas F, G D\n",
    "  Cavalcanti, ..., and Favaro L, ``Assessing sentence scoring techniques for\n",
    "  extractive text summarization,'' {\\em Expert Systems with Applications},\n",
    "  vol. 40, no. 14, pp. 5755--5764, 2013.\n",
    "\n",
    "\\bibitem{verma2020review}\n",
    "Verma P and Verma A, ``A review on text summarization techniques,'' {\\em\n",
    "  Journal of Scientific Research}, vol. 64, no. 1, pp. 251--257, 2020.\n",
    "\n",
    "\\bibitem{gambhir2017recent}\n",
    "Gambhir M and Gupta V, ``Recent automatic text summarization techniques: a\n",
    "  survey,'' {\\em Artificial Intelligence Review}, vol. 47, pp. 1--66, 2017.\n",
    "\n",
    "\\bibitem{mridha2021survey}\n",
    "Mridha M F, Lima A A, Nur K, Das S C, Hasan M, and Kabir M M, ``A\n",
    "  survey of automatic text summarization: Progress, process and challenges,''\n",
    "  {\\em \\textit{IEEE Access}}, vol. 9, pp. 156043--156070, 2021.\n",
    "\n",
    "\\bibitem{liu2021entity}\n",
    "Liu Q, Cheng G, K Gunaratna, and Qu Y, ``Entity summarization: State of the\n",
    "  art and future challenges,'' {\\em Journal of Web Semantics}, vol. 69,\n",
    "  p 100647, 2021.\n",
    "\n",
    "\\bibitem{muneera2022Abstractive}\n",
    "Muneera N M and Sriramya P, ``Abstractive text summarization employing\n",
    "  ontology-based knowledge-aware multi-focus conditional generative adversarial\n",
    "  network (okam-cgan) with hybrid pre-processing methodology,'' {\\em MULTIMEDIA\n",
    "  TOOLS AND APPLICATIONS}, 2022.\n",
    "\n",
    "\\bibitem{yadav2022automatic}\n",
    "Yadav D, Desai J, and Yadav A K, ``Automatic text summarization methods: A\n",
    "  comprehensive review,'' {\\em arXiv preprint arXiv:2204.01849}, 2022.\n",
    "\n",
    "\\bibitem{sharma2022automatic}\n",
    "Sharma G and Sharma D, ``Automatic text summarization methods: A\n",
    "  comprehensive review,'' {\\em SN Computer Science}, vol. 4, no. 1, p 33,\n",
    "  2022.\n",
    "\n",
    "\\bibitem{devlin2018bert}\n",
    "Devlin J, M-W Chang, Lee K, and Toutanova K, ``Bert: Pre-training of deep\n",
    "  bidirectional transformers for language understanding,'' {\\em arXiv preprint\n",
    "  arXiv:1810.04805}, 2018.\n",
    "\n",
    "\\bibitem{yenduri2023generative}\n",
    "Yenduri G, Srivastava G, R P K Maddikunta, Jhaveri R H, Wang W, A V\n",
    "  Vasilakos, Gadekallu T R, {\\em et al.}, ``Generative pre-trained\n",
    "  transformer: A comprehensive review on enabling technologies, potential\n",
    "  applications, emerging challenges, and future directions,'' {\\em arXiv\n",
    "  preprint arXiv:2305.10435}, 2023.\n",
    "\n",
    "\\bibitem{lin2022survey}\n",
    "Lin T, Wang Y, X Liu, and Qiu X, ``A survey of transformers,'' {\\em AI\n",
    "  Open}, 2022.\n",
    "\n",
    "\\bibitem{lu2023task}\n",
    "Lu L, Liu Y, W Xu, Li H, and Sun G, ``From task to evaluation: an\n",
    "  automatic text summarization review,'' {\\em Artificial Intelligence Review},\n",
    "  vol. 56, no. Suppl 2, pp. 2477--2507, 2023.\n",
    "\n",
    "\\bibitem{barbella2022rouge}\n",
    "Barbella M and Tortora G, ``Rouge metric evaluation for text summarization\n",
    "  techniques,'' {\\em Available at SSRN 4120317}, 2022.\n",
    "\n",
    "\\bibitem{khilji2021Abstractive}\n",
    "U A F R Khilji, Sinha U, P Singh, Ali A, and Pakray P, ``Abstractive\n",
    "  text summarization approaches with analysis of evaluation techniques,'' in\n",
    "  {\\em International Conference on Computational Intelligence in Communications\n",
    "  and Business Analytics}, (Cham), pp. 243--258, Springer International\n",
    "  Publishing, January 2021.\n",
    "\n",
    "\\bibitem{mukta2019phrase}\n",
    "Mukta A P, Mamun A A, Basak C, S Nahar, and Arif H M F, ``A\n",
    "  phrase-based machine translation from english to bangla using rule-based\n",
    "  approach,'' in {\\em 2019 International Conference on Electrical, Computer and\n",
    "  Communication Engineering (ECCE)}, pp. 1--5, IEEE, 2019.\n",
    "\n",
    "\\bibitem{hossain2021bert}\n",
    "Hossain N and Ahnaf A, ``Bert-based text simplification approach to reduce\n",
    "  linguistic complexity of bangla language,'' in {\\em 2021 International\n",
    "  Conference on Intelligent Technology, System and Service for Internet of\n",
    "  Everything (ITSS-IoE)}, pp. 1--5, IEEE, November 2021.\n",
    "\n",
    "\\bibitem{shetu2020identifying}\n",
    "Shetu S F, Saifuzzaman M, M Parvin, Moon N N, Yousuf R, and Sultana S,\n",
    "  ``Identifying the writing style of bangla language using natural language\n",
    "  processing,'' in {\\em 2020 11th International Conference on Computing,\n",
    "  Communication and Networking Technologies (ICCCNT)}, pp. 1--6, IEEE, July\n",
    "  2020.\n",
    "\n",
    "\\bibitem{sen2022bangla}\n",
    "Sen O, Fuad M, Islam M N, Rabbi J, M Masud, Hasan M K, ..., and\n",
    "  R M A Iftee, ``Bangla natural language processing: A comprehensive\n",
    "  analysis of classical, machine learning, and deep learning-based methods,''\n",
    "  {\\em IEEE Access}, vol. 10, pp. 38999--39044, 2022.\n",
    "\n",
    "\\bibitem{sikder2019automatic}\n",
    "Sikder R, Hossain M M, and Robi R F M H, ``Automatic text summarization\n",
    "  for bengali language including grammatical analysis,'' {\\em International\n",
    "  Journal of Scientific \\& Technology Research}, vol. 8, no. 6, 2019.\n",
    "\n",
    "\\bibitem{mridha2022study}\n",
    "Mridha M F, Ohi A Q, Hamid M A, and Monowar M M, ``A study on the\n",
    "  challenges and opportunities of speech recognition for bengali language,''\n",
    "  {\\em Artificial Intelligence Review}, pp. 1--25, 2022.\n",
    "\n",
    "\\bibitem{masum2019Abstractive}\n",
    "M A K Masum, Abujar S, I M A Talukder, A A S Rabby, and S A\n",
    "  Hossain, ``Abstractive method of text summarization with sequence to sequence\n",
    "  rnns,'' in {\\em 2019 10th international conference on computing,\n",
    "  communication and networking technologies (ICCCNT)}, pp. 1--5, IEEE, 2019.\n",
    "\n",
    "\\bibitem{chowdhury2021unsupervised}\n",
    "Chowdhury R R, Nayeem M T, Mim T T, R M S Chowdhury, and Jannat T,\n",
    "  ``Unsupervised abstractive summarization of bengali text documents,'' {\\em\n",
    "  arXiv preprint arXiv:2102.04490}, 2021.\n",
    "\n",
    "\\bibitem{mukherjee2022developing}\n",
    "Mukherjee A, {\\em Developing Bengali Text Summarization with Transformer Base\n",
    "  Model}.\n",
    "\\newblock Doctoral dissertation, National College of Ireland, Dublin, 2022.\n",
    "\n",
    "\\bibitem{king2015practical}\n",
    "King B P, {\\em Practical Natural Language Processing for Low-Resource\n",
    "  Languages}.\n",
    "\\newblock Doctoral dissertation, 2015.\n",
    "\n",
    "\\bibitem{bhattacharjee2022banglanlg}\n",
    "Bhattacharjee A, Hasan T, Ahmad W U, and Shahriyar R, ``Banglanlg:\n",
    "  Benchmarks and resources for evaluating low-resource natural language\n",
    "  generation in bangla,'' {\\em arXiv preprint arXiv:2205.11081}, 2022.\n",
    "\n",
    "\\bibitem{alam2021review}\n",
    "Alam F, Hasan A, T Alam, Khan A, J Tajrin, Khan N, and Chowdhury S A,\n",
    "  ``A review of bangla natural language processing tasks and the utility of\n",
    "  transformer models,'' {\\em arXiv preprint arXiv:2107.03844}, 2021.\n",
    "\n",
    "\\bibitem{karim2013technical}\n",
    "Karim M A, ed., {\\em Technical Challenges and Design Issues in Bangla\n",
    "  Language Processing}.\n",
    "\\newblock IGI Global, 2013.\n",
    "\n",
    "\\bibitem{gupta2021training}\n",
    "Gupta A, Vavre A, and Sarawagi S, ``Training data augmentation for\n",
    "  code-mixed translation,'' in {\\em Proceedings of the 2021 Conference of the\n",
    "  North American Chapter of the Association for Computational Linguistics:\n",
    "  Human Language Technologies}, pp. 5760--5766, June 2021.\n",
    "\n",
    "\\bibitem{hoque2020bdsl36}\n",
    "Hoque O B, Jubair M I, Akash A A, and Islam S, ``Bdsl36: A dataset for\n",
    "  bangladeshi sign letters recognition,'' in {\\em Proceedings of the Asian\n",
    "  Conference on Computer Vision}, 2020.\n",
    "\n",
    "\\bibitem{zhang2019hibert}\n",
    "Zhang X, Wei F, and Zhou M, ``Hibert: Document level pre-training of\n",
    "  hierarchical bidirectional transformers for document summarization,'' {\\em\n",
    "  arXiv preprint arXiv:1905.06566}, 2019.\n",
    "\n",
    "\\bibitem{hasan2020not}\n",
    "Hasan T, Bhattacharjee A, K Samin, Hasan M, M Basak, Rahman M S, and\n",
    "  R Shahriyar, ``Not low-resource anymore: Aligner ensembling, batch\n",
    "  filtering, and new datasets for bengali-english machine translation,'' {\\em\n",
    "  arXiv preprint arXiv:2009.09359}, 2020.\n",
    "\n",
    "\\bibitem{sen2021bangla}\n",
    "Sen O, Fuad M, Islam M N, Rabbi J, Hasan M K, BAZ M, and RAIHAN M A,\n",
    "  ``Bangla natural language processing: A comprehensive review of classical\n",
    "  machine learning and deep learning based methods,'' {\\em arXiv preprint\n",
    "  arXiv:2105.14875}, 2021.\n",
    "\n",
    "\\bibitem{uddin2007study}\n",
    "Uddin M N and Khan S A, ``A study on text summarization techniques and\n",
    "  implement few of them for bangla language,'' in {\\em 2007 10th International\n",
    "  Conference on Computer and Information Technology (ICCIT)}, (Dhaka,\n",
    "  Bangladesh), pp. 1--4, IEEE, December 2007.\n",
    "\n",
    "\\bibitem{efat2013automated}\n",
    "A M I Efat, Ibrahim M, and Kayesh H, ``Automated bangla text summarization\n",
    "  by sentence scoring and ranking,'' in {\\em 2013 International Conference on\n",
    "  Informatics, Electronics and Vision (ICIEV)}, pp. 1--5, IEEE, 2013.\n",
    "\n",
    "\\bibitem{haque2015automatic}\n",
    "Haque M M, Pervin S, and Begum Z, ``Automatic bengali news documents\n",
    "  summarization by introducing sentence frequency and clustering,'' in {\\em\n",
    "  2015 18th International Conference on Computer and Information Technology\n",
    "  (ICCIT)}, pp. 156--160, IEEE, 2015.\n",
    "\n",
    "\\bibitem{haque2016enhancement}\n",
    "Haque M M, Pervin S, and Begum Z, ``Enhancement of keyphrase-based approach\n",
    "  of automatic bangla text summarization,'' in {\\em 2016 IEEE Region 10\n",
    "  Conference (TENCON)}, pp. 42--46, IEEE, 2016.\n",
    "\n",
    "\\bibitem{paul2017bangla}\n",
    "Paul A, Imtiaz M T, Latif A H, Ahmed M, Adnan F A, Khan R, I Kadery,\n",
    "  and Rahman R M, ``Bangla news summarization,'' in {\\em Computational\n",
    "  Collective Intelligence: 9th International Conference, ICCCI 2017, Nicosia,\n",
    "  Cyprus, September 27-29, 2017, Proceedings, Part II 9}, pp. 479--488,\n",
    "  Springer, 2017.\n",
    "\n",
    "\\bibitem{abujar2017heuristic}\n",
    "Abujar S, Hasan M, M Shahin, and Hossain S A, ``A heuristic approach of\n",
    "  text summarization for bengali documentation,'' in {\\em 2017 8th\n",
    "  International Conference on Computing, Communication and Networking\n",
    "  Technologies (ICCCNT)}, pp. 1--8, IEEE, 2017.\n",
    "\n",
    "\\bibitem{akter2017Extractive}\n",
    "Akter S, Asa A S, Uddin M P, Hossain M D, Roy S K, and Afjal M I,\n",
    "  ``An extractive text summarization technique for bengali document (s) using\n",
    "  k-means clustering algorithm,'' in {\\em 2017 ieee international conference on\n",
    "  imaging, vision \\& pattern recognition (icivpr)}, pp. 1--6, IEEE, 2017.\n",
    "\n",
    "\\bibitem{chowdhury2017approach}\n",
    "Chowdhury S R, Sarkar K, and Dam S, ``An approach to generic bengali text\n",
    "  summarization using latent semantic analysis,'' in {\\em 2017 international\n",
    "  conference on information technology (ICIT)}, pp. 11--16, IEEE, 2017.\n",
    "\n",
    "\\bibitem{ghosh2018rule}\n",
    "Ghosh P P, Shahariar R, and  H M A Khan, ``A rule based extractive text\n",
    "  summarization technique for bangla news documents,'' {\\em International\n",
    "  Journal of Modern Education and Computer Science}, vol. 10, no. 12, p 44,\n",
    "  2018.\n",
    "\n",
    "\\bibitem{tumpa2018improved}\n",
    "Tumpa P, Yeasmin S, A Nitu, Uddin M, M Afjal, and Mamun M, ``An improved\n",
    "  extractive summarization technique for bengali text (s),'' in {\\em 2018\n",
    "  International Conference on Computer, Communication, Chemical, Material and\n",
    "  Electronic Engineering (IC4ME2)}, pp. 1--4, IEEE, 2018.\n",
    "\n",
    "\\bibitem{chandro2018automated}\n",
    "Chandro P, H M F Arif, Rahman M M, Siddik M S, Rahman M S, and M A\n",
    "  Rahman, ``Automated bengali document summarization by collaborating\n",
    "  individual word \\& sentence scoring,'' in {\\em 2018 21st International\n",
    "  Conference of Computer and Information Technology (ICCIT)}, pp. 1--6, IEEE,\n",
    "  2018.\n",
    "\n",
    "\\bibitem{sarkar2018automatic}\n",
    "Sarkar A and Hossen M S, ``Automatic bangla text summarization using term\n",
    "  frequency and semantic similarity approach,'' in {\\em 2018 21st International\n",
    "  Conference of Computer and Information Technology (ICCIT)}, pp. 1--6, IEEE,\n",
    "  2018.\n",
    "\n",
    "\\bibitem{talukder2019bengali}\n",
    "I M A Talukder, Abujar S, M A K Masum, Faisal F, and Hossain S A,\n",
    "  ``Bengali abstractive text summarization using sequence to sequence rnns,''\n",
    "  in {\\em 2019 10th International Conference on Computing, Communication and\n",
    "  Networking Technologies (ICCCNT)}, pp. 1--5, IEEE, 2019.\n",
    "\n",
    "\\bibitem{abujar2019approach}\n",
    "Abujar S, M A K Masum, Mohibullah M, Hossain S A, {\\em et al.}, ``An\n",
    "  approach for bengali text summarization using word2vector,'' in {\\em 2019\n",
    "  10th International Conference on Computing, Communication and Networking\n",
    "  Technologies (ICCCNT)}, pp. 1--5, IEEE, 2019.\n",
    "\n",
    "\\bibitem{masum2019sentence}\n",
    "M A K Masum, Abujar S, H R T Tusher, Faisal F, and Hossain S A,\n",
    "  ``Sentence similarity measurement for bengali abstractive text\n",
    "  summarization,'' in {\\em 2019 10th International Conference on Computing,\n",
    "  Communication and Networking Technologies (ICCCNT)}, pp. 1--5, IEEE, 2019.\n",
    "\n",
    "\\bibitem{rahman2019bengali}\n",
    "Rahman A, Rafiq F M, Saha R, R Rafian, and Arif H, ``Bengali text\n",
    "  summarization using textrank, fuzzy c-means and aggregate scoring methods,''\n",
    "  in {\\em 2019 IEEE Region 10 Symposium (TENSYMP)}, pp. 331--336, IEEE, 2019.\n",
    "\n",
    "\\bibitem{abujar2020bengali}\n",
    "Abujar S, M A K Masum, M Sanzidul Islam, Faisal F, and Hossain S A, ``A\n",
    "  bengali text generation approach in context of abstractive text summarization\n",
    "  using rnn,'' {\\em Innovations in Computer Science and Engineering:\n",
    "  Proceedings of 7th ICICSE}, pp. 509--518, 2020.\n",
    "\n",
    "\\bibitem{bhattacharjee2021bengali}\n",
    "Bhattacharjee P, Mallick A, and M Saiful Islam, ``Bengali abstractive news\n",
    "  summarization (bans): a neural attention approach,'' in {\\em Proceedings of\n",
    "  International Conference on Trends in Computational and Cognitive\n",
    "  Engineering: Proceedings of TCCE 2020}, pp. 41--51, Springer, 2021.\n",
    "\n",
    "\\bibitem{fouzia2021bengali}\n",
    "Fouzia F A, Rahat M A, Alie M T-Al-Mahdi, M A K Masum, Abujar S, and\n",
    "  Hossain S A, ``A bengali text summarization using encoder-decoder based on\n",
    "  social media dataset,'' in {\\em Emerging Technologies in Data Mining and\n",
    "  Information Security: Proceedings of IEMIS 2020, Volume 2}, pp. 539--549,\n",
    "  Springer, 2021.\n",
    "\n",
    "\\bibitem{islam2021abstraction}\n",
    "Islam M M, Islam M, M A K Masum, Abujar S, and Hossain S A,\n",
    "  ``Abstraction based bengali text summarization using bi-directional attentive\n",
    "  recurrent neural networks,'' in {\\em Emerging Technologies in Data Mining and\n",
    "  Information Security: Proceedings of IEMIS 2020, Volume 2}, pp. 317--327,\n",
    "  Springer, 2021.\n",
    "\n",
    "\\bibitem{gupta2022automated}\n",
    "Gupta A, Chugh D, Anjum, and Katarya R, ``Automated news summarization using\n",
    "  transformers,'' in {\\em Sustainable Advanced Computing: Select Proceedings of\n",
    "  ICSAC 2021}, (Singapore), pp. 249--259, Springer Singapore, 2022.\n",
    "\n",
    "\\bibitem{ranganathan2022text}\n",
    "Ranganathan J and Abuka G, ``Text summarization using transformer model,'' in\n",
    "  {\\em 2022 Ninth International Conference on Social Networks Analysis,\n",
    "  Management and Security (SNAMS)}, pp. 1--5, IEEE, November 2022.\n",
    "\n",
    "\\bibitem{kumar2023Abstractive}\n",
    "Kumar S and Solanki A, ``An abstractive text summarization technique using\n",
    "  transformer model with self-attention mechanism,'' {\\em Neural Computing and\n",
    "  Applications}, pp. 1--20, 2023.\n",
    "\n",
    "\\bibitem{sultana2022bengali}\n",
    "Sultana M, Chakraborty P, and Choudhury T, ``Bengali abstractive news\n",
    "  summarization using seq2seq learning with attention,'' in {\\em Cyber\n",
    "  Intelligence and Information Retrieval: Proceedings of CIIR 2021},\n",
    "  pp. 279--289, Springer, 2022.\n",
    "\n",
    "\\bibitem{Barsha2023}\n",
    "Hoque F A Barsha and Uddin M N, ``Comparative analysis of banglat5 and\n",
    "  pointer generator network for bengali abstractive story summarization,'' in\n",
    "  {\\em 2023 International Conference on Information and Communication\n",
    "  Technology for Sustainable Development (ICICT4SD)}, pp. 84--88, 2023.\n",
    "\n",
    "\\bibitem{bhattacharjee_banglanlg_2023}\n",
    "Bhattacharjee A, Hasan T, Ahmad W U, and Shahriyar R, ``{BanglaNLG} and\n",
    "  {BanglaT5}: {Benchmarks} and {Resources} for {Evaluating} {Low}-{Resource}\n",
    "  {Natural} {Language} {Generation} in {Bangla},'' in {\\em Findings of the\n",
    "  {Association} for {Computational} {Linguistics}: {EACL} 2023} (A Vlachos and\n",
    "  I Augenstein, eds.), (Dubrovnik, Croatia), pp. 726--735, Association for\n",
    "  Computational Linguistics, May 2023.\n",
    "\n",
    "\\bibitem{landro2022two}\n",
    "Landro N, Gallo I, R La Grassa, and Federici E, ``Two new datasets for\n",
    "  italian-language abstractive text summarization,'' {\\em Information},\n",
    "  vol. 13, no. 5, p 228, 2022.\n",
    "\n",
    "\\bibitem{phan2022vit5}\n",
    "Phan L, Tran H, H Nguyen, and Trinh T H, ``Vit5: Pretrained text-to-text\n",
    "  transformer for vietnamese language generation,'' {\\em arXiv preprint\n",
    "  arXiv:2205.06457}, 2022.\n",
    "\n",
    "\\bibitem{khandelwal2019sample}\n",
    "Khandelwal U, Clark K, D Jurafsky, and Kaiser L, ``Sample efficient text\n",
    "  summarization using a single pre-trained transformer,'' {\\em arXiv preprint\n",
    "  arXiv:1905.08836}, 2019.\n",
    "\n",
    "\\bibitem{guan2020survey}\n",
    "Guan W, Smetannikov I, and Tianxing M, ``Survey on automatic text\n",
    "  summarization and transformer models applicability,'' in {\\em Proceedings of\n",
    "  the 2020 1st International Conference on Control, Robotics and Intelligent\n",
    "  System}, pp. 176--184, 2020.\n",
    "\n",
    "\\bibitem{zhao2022calibrating}\n",
    "Zhao Y, Khalman M, R Joshi, Narayan S, M Saleh, and Liu P J,\n",
    "  ``Calibrating sequence likelihood improves conditional language generation,''\n",
    "  {\\em arXiv preprint arXiv:2210.00045}, 2022.\n",
    "\n",
    "\\bibitem{zhao2023stepbystep}\n",
    "Zhao S, Li Q, T He, and Wen J, ``A step-by-step gradient penalty with\n",
    "  similarity calculation for text summary generation,'' {\\em Neural Processing\n",
    "  Letters}, vol. 55, no. 4, pp. 4111--4126, 2023.\n",
    "\n",
    "\\bibitem{tang2022textbox2}\n",
    "Tang T, Li J, Z Chen, Hu Y, Z Yu, Dai W, and Wen J R, ``Textbox 2.0: A\n",
    "  text generation library with pre-trained language models,'' {\\em arXiv\n",
    "  preprint arXiv:2212.13005}, 2022.\n",
    "\n",
    "\\bibitem{laquatra2023bartit}\n",
    "M La Quatra and Cagliero L, ``Bart-it: An efficient sequence-to-sequence\n",
    "  model for italian text summarization,'' {\\em Future Internet}, vol. 15,\n",
    "  no. 1, p 15, 2023.\n",
    "\n",
    "\\bibitem{landro2022twonew}\n",
    "Landro N, Gallo I, R La Grassa, and Federici E, ``Two new datasets for\n",
    "  italian-language abstractive text summarization,'' {\\em Information},\n",
    "  vol. 13, no. 5, p 228, 2022.\n",
    "\n",
    "\\bibitem{vaswani2017attention}\n",
    "Vaswani A, Shazeer N, N Parmar, Uszkoreit J, L Jones, Gomez A N,\n",
    "  {\\L}. Kaiser, and Polosukhin I, ``Attention is all you need,'' {\\em Advances\n",
    "  in neural information processing systems}, vol. 30, 2017.\n",
    "\n",
    "\\bibitem{dhar2021pointer}\n",
    "Dhar N, Saha G, P Bhattacharjee, Mallick A, and Islam M S, ``Pointer over\n",
    "  attention: An improved bangla text summarization approach using hybrid\n",
    "  pointer generator network,'' in {\\em 2021 24th International Conference on\n",
    "  Computer and Information Technology (ICCIT)}, pp. 1--5, IEEE, 2021.\n",
    "\n",
    "\\bibitem{bani-almarjeh2023arabic}\n",
    "M Bani-Almarjeh and Kurdy M B, ``Arabic abstractive text summarization using\n",
    "  rnn-based and transformer-based architectures,'' {\\em Information Processing\n",
    "  \\& Management}, vol. 60, no. 2, p 103227, 2023.\n",
    "\n",
    "\\bibitem{nunez-robinson2022comparative}\n",
    "D N{\\'u}{\\ n}ez-Robinson, J Talavera-Montalto, and Ugarte W, ``A comparative\n",
    "  analysis on the summarization of legal texts using transformer models,'' in\n",
    "  {\\em International Conference on Advanced Research in Technologies,\n",
    "  Information, Innovation and Sustainability}, (Cham), pp. 372--386, Springer\n",
    "  Nature Switzerland, September 2022.\n",
    "\n",
    "\\bibitem{lewis-etal-2020-bart}\n",
    "Lewis M, Liu Y, N Goyal, Ghazvininejad M, A Mohamed, Levy O, V Stoyanov,\n",
    "  and Zettlemoyer L, ``Bart: Denoising sequence-to-sequence pre-training for\n",
    "  natural language generation, translation, and comprehension,'' in {\\em\n",
    "  Proceedings of the 58th Annual Meeting of the Association for Computational\n",
    "  Linguistics}, pp. 7871--7880, Association for Computational Linguistics,\n",
    "  2020.\n",
    "\n",
    "\\bibitem{cnn_dailymail}\n",
    "McDonald R, Brokos G, and Zettlemoyer L, ``{CNN/Daily Mail} dataset.''\n",
    "  \\url{https://huggingface.co/datasets/cnn_dailymail}, 2015.\n",
    "\n",
    "\\bibitem{nallapati2016Abstractive}\n",
    "Nallapati R, Zhou B, C Gulcehre, and Xiang B, ``Abstractive text\n",
    "  summarization using sequence-to-sequence rnns and beyond,'' {\\em arXiv\n",
    "  preprint arXiv:1602.06023}, 2016.\n",
    "\n",
    "\\bibitem{farahani2021leveraging}\n",
    "Farahani M, Gharachorloo M, and Manthouri M, ``Leveraging parsbert and\n",
    "  pretrained mt5 for persian abstractive text summarization,'' in {\\em 2021\n",
    "  26th International Computer Conference, Computer Society of Iran (CSICC)},\n",
    "  pp. 1--6, IEEE, March 2021.\n",
    "\n",
    "\\bibitem{reda2022hybrid}\n",
    "Reda A, Salah N, J Adel, Ehab M, I Ahmed, Magdy M, {\\em et al.}, ``A\n",
    "  hybrid arabic text summarization approach based on transformers,'' in {\\em\n",
    "  2022 2nd International Mobile, Intelligent, and Ubiquitous Computing\n",
    "  Conference (MIUCC)}, pp. 56--62, IEEE, May 2022.\n",
    "\n",
    "\\bibitem{borah2022comparative}\n",
    "Borah M P, Dadure P, and Pakray P, ``Comparative analysis of t5 model for\n",
    "  abstractive text summarization on different datasets,'' 2022.\n",
    "\\newblock Preprint or Conference Paper (Please add the specific journal or\n",
    "  conference information).\n",
    "\n",
    "\\bibitem{hossen2018bengali}\n",
    "Hossen M A, Govindaiah A, S Sultana, and A-A Bhuiyan, ``Bengali sign\n",
    "  language recognition using deep convolutional neural network,'' in {\\em 2018\n",
    "  Joint 7th International Conference on Informatics, Electronics \\& Vision\n",
    "  (ICIEV) and 2018 2nd International Conference on Imaging, Vision \\& Pattern\n",
    "  Recognition (icIVPR)}, pp. 369--373, IEEE, June 2018.\n",
    "\n",
    "\\bibitem{xue2020mt5}\n",
    "Xue L, Constant N, A Roberts, Kale M, R Al-Rfou, Siddhant A, ..., and\n",
    "  C Raffel, ``mt5: A massively multilingual pre-trained text-to-text\n",
    "  transformer,'' {\\em arXiv preprint arXiv:2010.11934}, 2020.\n",
    "\n",
    "\\bibitem{fuad2022crosslingual}\n",
    "Fuad A and M Al-Yahya, ``Cross-lingual transfer learning for arabic\n",
    "  task-oriented dialogue systems using multilingual transformer model mt5,''\n",
    "  {\\em Mathematics}, vol. 10, no. 5, p 746, 2022.\n",
    "\n",
    "\\bibitem{zhang2018improving}\n",
    "Zhang J, Luan H, M Sun, Zhai F, J Xu, Zhang M, and Liu Y, ``Improving\n",
    "  the transformer translation model with document-level context,'' {\\em arXiv\n",
    "  preprint arXiv:1810.03581}, 2018.\n",
    "\n",
    "\\bibitem{abadi2023enhancing}\n",
    "M V N Abadi and Ghasemian F, ``Enhancing persian text summarization using\n",
    "  the mt5 transformer model: A three-phased fine-tuning approach and\n",
    "  reinforcement learning,'' 2023.\n",
    "\n",
    "\\bibitem{goyal2021largerscale}\n",
    "Goyal N, Du J, M Ott, Anantharaman G, and Conneau A, ``Larger-scale\n",
    "  transformers for multilingual masked language modeling,'' {\\em arXiv preprint\n",
    "  arXiv:2105.00572}, 2021.\n",
    "\n",
    "\\bibitem{toledano-lopez2022finetuning}\n",
    "Toledano O G-López, Madera J, H González, A Simón-Cuevas, Demeester T,\n",
    "  and Mannens E, ``Finetuning mt5-based transformer via cma-es for sentiment\n",
    "  analysis,'' in {\\em Proceedings of the Third Workshop for Iberian Languages\n",
    "  Evaluation Forum (IberLEF 2022), CEUR WS Proceedings}, 2022.\n",
    "\n",
    "\\bibitem{ganesan2018rouge}\n",
    "Ganesan K, ``Rouge 2.0: Updated and improved measures for evaluation of\n",
    "  summarization tasks,'' {\\em arXiv preprint arXiv:1803.01937}, 2018.\n",
    "\n",
    "\\bibitem{westphaln2021arksey}\n",
    "Westphaln K K, Regoeczi W, M Masotya, B Vazquez-Westphaln, Lounsbury K,\n",
    "  L McDavid, Lee H, J Johnson, and Ronis S D, ``From arksey and o’malley\n",
    "  and beyond: Customizations to enhance a team-based, mixed approach to scoping\n",
    "  review methodology,'' {\\em MethodsX}, vol. 8, p 101375, 2021.\n",
    "\n",
    "\\bibitem{tricco2018prisma}\n",
    "Tricco A C, Lillie E, W Zarin, O K K'Brien, Colquhoun H, D Levac,\n",
    "  D Moher, Peters M D, Horsley T, L Weeks, {\\em et al.}, ``Prisma\n",
    "  extension for scoping reviews (prisma-scr): checklist and explanation,'' {\\em\n",
    "  Annals of internal medicine}, vol. 169, no. 7, pp. 467--473, 2018.\n",
    "\n",
    "\\bibitem{kumar2021study}\n",
    "Kumar Y, Kaur K, and Kaur S, ``Study of automatic text summarization\n",
    "  approaches in different languages,'' {\\em Artificial Intelligence Review},\n",
    "  vol. 54, no. 8, pp. 5897--5929, 2021.\n",
    "\n",
    "\\bibitem{csebuetnlp_mT5_multilingual_XLSum}\n",
    "Hasan T, Bhattacharjee A, Islam M S, Mubasshir K, Y-F Li, Y-B Kang,\n",
    "  Rahman M S, and Shahriyar R, ``{XL}-sum: Large-scale multilingual\n",
    "  abstractive summarization for 44 languages,'' in {\\em Findings of the\n",
    "  Association for Computational Linguistics: ACL-IJCNLP 2021}, (Online),\n",
    "  pp. 4693--4703, Association for Computational Linguistics, Aug. 2021.\n",
    "\n",
    "\\bibitem{grusky2018newsroom}\n",
    "Grusky M, Naaman M, and Artzi Y, ``Newsroom: A dataset of 1.3 million\n",
    "  summaries with diverse extractive strategies,'' {\\em arXiv preprint\n",
    "  arXiv:1804.11283}, 2018.\n",
    "\n",
    "\\bibitem{fabbri2019multinews}\n",
    "Fabbri A R, Li I, T She, Li S, and Radev D R, ``Multi-news: A\n",
    "  large-scale multi-document summarization dataset and abstractive hierarchical\n",
    "  model,'' {\\em arXiv preprint arXiv:1906.01749}, 2019.\n",
    "\n",
    "\\bibitem{liu2018generatingwikipedia}\n",
    "Liu P J, Saleh M, E Pot, Goodrich B, R Sepassi, Kaiser L, and\n",
    "  N Shazeer, ``Generating wikipedia by summarizing long sequences,'' {\\em\n",
    "  arXiv preprint arXiv:1801.10198}, 2018.\n",
    "\n",
    "\\bibitem{ladhak2020wikilingua}\n",
    "Ladhak F, Durmus E, C Cardie, and McKeown K, ``Wikilingua: A new benchmark\n",
    "  dataset for cross-lingual abstractive summarization,'' {\\em arXiv preprint\n",
    "  arXiv:2010.03093}, 2020.\n",
    "\n",
    "\\bibitem{ghalandari2020largescale}\n",
    "Ghalandari D G, Hokamp C, Pham N T, Glover J, and Ifrim G, ``A\n",
    "  large-scale multi-document summarization dataset from the wikipedia current\n",
    "  events portal,'' {\\em arXiv preprint arXiv:2005.10070}, 2020.\n",
    "\n",
    "\\bibitem{wojciech2021booksum}\n",
    "Wojciech K, Nazneen R, A Divyansh, Caiming X, and Dragomir R, ``Booksum: A\n",
    "  collection of datasets for long-form narrative summarization,'' {\\em CoRR},\n",
    "  2021.\n",
    "\n",
    "\\bibitem{zhang2019email}\n",
    "Zhang R and Tetreault J, ``This email could save your life: Introducing the\n",
    "  task of email subject line generation,'' {\\em arXiv preprint\n",
    "  arXiv:1906.03497}, 2019.\n",
    "\n",
    "\\bibitem{yasunaga2019scisummnet}\n",
    "Yasunaga M, Kasai J, R Zhang, Fabbri A R, Li I, D Friedman, and D R\n",
    "  Radev, ``Scisummnet: A large annotated corpus and content-impact models for\n",
    "  scientific paper summarization with citation networks,'' in {\\em Proceedings\n",
    "  of the AAAI Conference on Artificial Intelligence}, vol. 33, pp. 7386--7393,\n",
    "  2019.\n",
    "\n",
    "\\bibitem{cohan2018discourse}\n",
    "Cohan A, Dernoncourt F, Kim D S, Bui T, S Kim, Chang W, and Goharian N,\n",
    "  ``A discourse-aware attention model for abstractive summarization of long\n",
    "  documents,'' {\\em arXiv preprint arXiv:1804.05685}, 2018.\n",
    "\n",
    "\\bibitem{chowdhury2019cqasumm}\n",
    "Chowdhury T and Chakraborty T, ``Cqasumm: Building references for community\n",
    "  question answering summarization corpora,'' in {\\em Proceedings of the ACM\n",
    "  India Joint International Conference on Data Science and Management of Data},\n",
    "  pp. 18--26, 2019.\n",
    "\n",
    "\\bibitem{sayali2020aquamuse}\n",
    "Sayali K, Sheide C, Z Wan, Fei S, and Eugene I, ``Aquamuse: Automatically\n",
    "  generating datasets for query-based multi-document summarization,'' {\\em\n",
    "  arXiv preprint arXiv:2010.12694}, 2020.\n",
    "\n",
    "\\bibitem{hasan-etal-2020-low}\n",
    "Hasan T, Bhattacharjee A, K Samin, Hasan M, M Basak, Rahman M S, and\n",
    "  R Shahriyar, ``Not low-resource anymore: Aligner ensembling, batch\n",
    "  filtering, and new datasets for {B}engali-{E}nglish machine translation,'' in\n",
    "  {\\em Proceedings of the 2020 Conference on Empirical Methods in Natural\n",
    "  Language Processing (EMNLP)}, (Online), pp. 2612--2623, Association for\n",
    "  Computational Linguistics, Nov. 2020.\n",
    "\n",
    "\\bibitem{pegasus_xsum}\n",
    "Zhang J, Liu Y, P Wei, Zhao X, L Shou, Palangi H, J Gao, Shen Y, and\n",
    "  B Dolan, ``Pegasus: Pre-training with extracted gap-sentences for\n",
    "  abstractive summarization.''\n",
    "  \\url{https://huggingface.co/google/pegasus-xsum}, 2019.\n",
    "\n",
    "\\bibitem{lewis2020bart}\n",
    "Lewis M, Liu Y, N Goyal, Ghazvininejad M, A Mohamed, Levy O, and\n",
    "  V Stoyanov, ``Bart: Denoising sequence-to-sequence pre-training for natural\n",
    "  language generation, translation, and comprehension.''\n",
    "  \\url{https://huggingface.co/facebook/bart-large-xsum}, 2020.\n",
    "\n",
    "\\bibitem{morenolq_bart_xsum}\n",
    "Moreno L Q, ``Bart: Denoising sequence-to-sequence pre-training for natural\n",
    "  language generation, translation, and comprehension.''\n",
    "  \\url{https://huggingface.co/morenolq/bart-base-xsum}, year-of-access.\n",
    "\n",
    "\\bibitem{financial_summarization_pegasus}\n",
    "Passali T, Gidiotis A, E Chatzikyriakidis, and Tsoumakas G, ``Financial\n",
    "  summarization pegasus.''\n",
    "  \\url{https://huggingface.co/human-centered-summarization/financial-summarization-pegasus},\n",
    "  2023.\n",
    "\\newblock Accessed on Apr. 05, 2023.\n",
    "\n",
    "\\bibitem{sysresearch101_t5_large_finetuned_xsum}\n",
    "Kapre R, ``T5 large fine-tuned xsum.''\n",
    "  \\url{https://huggingface.co/sysresearch101/t5-large-finetuned-xsum}, 2024.\n",
    "\\newblock Accessed on May. 05, 2024.\n",
    "\n",
    "\\bibitem{pszemraj_long_t5_tglobal_base_16384_booksum_V11_big_patent_V2}\n",
    "Szemraj P, ``Long t5 tglobal base 16384 booksum v11 big patent v2.''\n",
    "  \\url{https://huggingface.co/pszemraj/long-t5-tglobal-base-16384-booksum-V11-big_patent-V2},\n",
    "  2024.\n",
    "\\newblock Accessed on Jan. 18, 2024.\n",
    "\n",
    "\\bibitem{nbroad_longt5_base_global_mediasum}\n",
    "Broad N, ``Longt5 base global mediasum.''\n",
    "  \\url{https://huggingface.co/nbroad/longt5-base-global-mediasum}, 2024.\n",
    "\\newblock Accessed on May. 05, 2024.\n",
    "\n",
    "\\bibitem{xysmalobia_pegasus_samsum}\n",
    "Nimmagadda S D and Paleti K, ``Pegasus samsum.''\n",
    "  \\url{https://huggingface.co/dhyutin/pegasus-samsum}, 2024.\n",
    "\\newblock Accessed on Jan. 18, 2024.\n",
    "\n",
    "\\bibitem{philschmid_distilbart_cnn_12_6_samsum}\n",
    "Schmid P, ``Distilbart cnn 12-6 samsum.''\n",
    "  \\url{https://huggingface.co/philschmid/distilbart-cnn-12-6-samsum}, 2024.\n",
    "\\newblock Accessed on Jan. 18, 2024.\n",
    "\n",
    "\\bibitem{pszemraj_long_t5_tglobal_base_16384_book_summary}\n",
    "Szemeraj P, ``Long t5 tglobal base 16384 book summary.''\n",
    "  \\url{https://huggingface.co/pszemraj/long-t5-tglobal-base-16384-book-summary},\n",
    "  2022.\n",
    "\\newblock Accessed on Nov. 30, 2022.\n",
    "\n",
    "\\bibitem{pszemraj_long_t5_tglobal_large_pubmed_3k_booksum_16384_WIP}\n",
    "Szemeraj P, ``Long t5 tglobal large pubmed 3k book sum 16384 wip.''\n",
    "  \\url{https://huggingface.co/pszemraj/long-t5-tglobal-large-pubmed-3k-booksum-16384-WIP},\n",
    "  2022.\n",
    "\\newblock Accessed on Nov. 30, 2022.\n",
    "\n",
    "\\bibitem{pszemraj_long_t5_tglobal_large_pubmed_3k_booksum_16384_WIP13}\n",
    "Szemraj P, ``Long t5 tglobal large pubmed 3k book sum 16384 wip13.''\n",
    "  \\url{https://huggingface.co/pszemraj/long-t5-tglobal-large-pubmed-3k-booksum-16384-WIP13},\n",
    "  2022.\n",
    "\\newblock Accessed on Nov. 30, 2022.\n",
    "\n",
    "\\bibitem{pszemraj_led_base_book_summary}\n",
    "Szemraj P, ``Led base book summary.''\n",
    "  \\url{https://huggingface.co/pszemraj/led-base-book-summary}, 2022.\n",
    "\\newblock Accessed on Nov. 30, 2022.\n",
    "\n",
    "\\bibitem{google_bigbird_pegasus_large_arxiv}\n",
    "Zaheer M, Guruganesh G, A Dubey, Ainslie J, C Alberti, Ontanon S,\n",
    "  P Pham, Ravula A, Q Wang, Yang L, and Ahmed A, ``Big bird: Transformers\n",
    "  for longer sequences.''\n",
    "  \\url{https://huggingface.co/google/bigbird-pegasus-large-arxiv}, 2021.\n",
    "\\newblock Accessed on Jan. 18, 2024.\n",
    "\n",
    "\\bibitem{nlp_hug}\n",
    "Wolf T, Debut L, V Sanh, Chaumond J, C Delangue, Moi A, P Cistac,\n",
    "  T Rault, Louf R, M Funtowicz, and Brew J, ``Nlp: datasets, metrics, and\n",
    "  evaluation methods for natural language processing in 100+ languages,'' 2020.\n",
    "\n",
    "\\bibitem{islam2020hybrid}\n",
    "Islam M, Majumdar F N, Galib A, and Hoque M M, ``Hybrid text summarizer\n",
    "  for bangla document,'' {\\em Int J Comput Vis Sig Process}, vol. 1, no. 1,\n",
    "  pp. 27--38, 2020.\n",
    "\n",
    "\\bibitem{islam2020summarizing}\n",
    "Islam M J, Ahammad K, and Chowdhury H M K, ``Summarizing online product\n",
    "  reviews in bengali based on similarity using sequence to sequence rnns,'' in\n",
    "  {\\em 2020 11th International Conference on Electrical and Computer\n",
    "  Engineering (ICECE)}, pp. 33--36, IEEE, 2020.\n",
    "\n",
    "\\bibitem{banik2022bangla}\n",
    "Banik N, Saha C, I Ahmed, and Shapna K A, ``Bangla text generation system\n",
    "  by incorporating attention in sequence-to-sequence model,'' {\\em World\n",
    "  Journal of Advanced Research and Reviews}, vol. 14, no. 1, pp. 080--094,\n",
    "  2022.\n",
    "\n",
    "\\end{thebibliography}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Step 1: Split the bibliography into individual entries\n",
    "entries = bibliography.strip().split(r\"\\bibitem\")\n",
    "entries = [entry.strip() for entry in entries if entry.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57888d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\bibitem{elkassas2021automatic}\n",
      "El W S-Kassas, Salama C R, Rafea A A, and Mohamed H K, 2021. ``Automatic text\n",
      "  summarization: A comprehensive survey,'' {\\em Expert Systems with\n",
      "  Applications}, vol. 165, p 113679.\n",
      "\n",
      "\\bibitem{widyassari2022review}\n",
      "Widyassari A P, Rustad S, Shidik G F, Noersasongko E, A Syukur, and\n",
      "  A Affandy, 2022. ``Review of automatic text summarization techniques \\& methods,''\n",
      "  {\\em Journal of King Saud University-Computer and Information Sciences},\n",
      "  vol. 34, no. 4, pp. 1029--1046.\n",
      "\n",
      "\\bibitem{ferreira2013assessing}\n",
      "Ferreira R, L de Souza Cabral, Lins R D, e G P Silva, Freitas F, G D\n",
      "  Cavalcanti, ..., and Favaro L, 2013. ``Assessing sentence scoring techniques for\n",
      "  extractive text summarization,'' {\\em Expert Systems with Applications},\n",
      "  vol. 40, no. 14, pp. 5755--5764.\n",
      "\n",
      "\\bibitem{verma2020review}\n",
      "Verma P and Verma A, 2020. ``A review on text summarization techniques,'' {\\em\n",
      "  Journal of Scientific Research}, vol. 64, no. 1, pp. 251--257.\n",
      "\n",
      "\\bibitem{gambhir2017recent}\n",
      "Gambhir M and Gupta V, 2017. ``Recent automatic text summarization techniques: a\n",
      "  survey,'' {\\em Artificial Intelligence Review}, vol. 47, pp. 1--66.\n",
      "\n",
      "\\bibitem{mridha2021survey}\n",
      "Mridha M F, Lima A A, Nur K, Das S C, Hasan M, and Kabir M M, 2021. ``A\n",
      "  survey of automatic text summarization: Progress, process and challenges,''\n",
      "  {\\em \\textit{IEEE Access}}, vol. 9, pp. 156043--156070.\n",
      "\n",
      "\\bibitem{liu2021entity}\n",
      "Liu Q, Cheng G, K Gunaratna, and Qu Y, 2021. ``Entity summarization: State of the\n",
      "  art and future challenges,'' {\\em Journal of Web Semantics}, vol. 69,\n",
      "  p 100647.\n",
      "\n",
      "\\bibitem{muneera2022Abstractive}\n",
      "Muneera N M and Sriramya P, 2022. ``Abstractive text summarization employing\n",
      "  ontology-based knowledge-aware multi-focus conditional generative adversarial\n",
      "  network (okam-cgan) with hybrid pre-processing methodology,'' {\\em MULTIMEDIA\n",
      "  TOOLS AND APPLICATIONS}.\n",
      "\n",
      "\\bibitem{yadav2022automatic}\n",
      "Yadav D, Desai J, and Yadav A K, 2022. ``Automatic text summarization methods: A\n",
      "  comprehensive review,'' {\\em arXiv preprint arXiv:2204.01849}.\n",
      "\n",
      "\\bibitem{sharma2022automatic}\n",
      "Sharma G and Sharma D, 2022. ``Automatic text summarization methods: A\n",
      "  comprehensive review,'' {\\em SN Computer Science}, vol. 4, no. 1, p 33.\n",
      "\n",
      "\\bibitem{devlin2018bert}\n",
      "Devlin J, M-W Chang, Lee K, and Toutanova K, 2018. ``Bert: Pre-training of deep\n",
      "  bidirectional transformers for language understanding,'' {\\em arXiv preprint\n",
      "  arXiv:1810.04805}.\n",
      "\n",
      "\\bibitem{yenduri2023generative}\n",
      "Yenduri G, Srivastava G, R P K Maddikunta, Jhaveri R H, Wang W, A V\n",
      "  Vasilakos, Gadekallu T R, {\\em et al.}, 2023. ``Generative pre-trained\n",
      "  transformer: A comprehensive review on enabling technologies, potential\n",
      "  applications, emerging challenges, and future directions,'' {\\em arXiv\n",
      "  preprint arXiv:2305.10435}.\n",
      "\n",
      "\\bibitem{lin2022survey}\n",
      "Lin T, Wang Y, X Liu, and Qiu X, 2022. ``A survey of transformers,'' {\\em AI\n",
      "  Open}.\n",
      "\n",
      "\\bibitem{lu2023task}\n",
      "Lu L, Liu Y, W Xu, Li H, and Sun G, 2023. ``From task to evaluation: an\n",
      "  automatic text summarization review,'' {\\em Artificial Intelligence Review},\n",
      "  vol. 56, no. Suppl 2, pp. 2477--2507.\n",
      "\n",
      "\\bibitem{barbella2022rouge}\n",
      "Barbella M and Tortora G, 2022. ``Rouge metric evaluation for text summarization\n",
      "  techniques,'' {\\em Available at SSRN 4120317}.\n",
      "\n",
      "\\bibitem{khilji2021Abstractive}\n",
      "U A F R Khilji, Sinha U, P Singh, Ali A, and Pakray P, January 2021. ``Abstractive\n",
      "  text summarization approaches with analysis of evaluation techniques,'' in\n",
      "  {\\em International Conference on Computational Intelligence in Communications\n",
      "  and Business Analytics}, (Cham), pp. 243--258, Springer International\n",
      "  Publishing.\n",
      "\n",
      "\\bibitem{mukta2019phrase}\n",
      "Mukta A P, Mamun A A, Basak C, S Nahar, and Arif H M F, 2019. ``A\n",
      "  phrase-based machine translation from english to bangla using rule-based\n",
      "  approach,'' in {\\em 2019 International Conference on Electrical, Computer and\n",
      "  Communication Engineering (ECCE)}, pp. 1--5, IEEE.\n",
      "\n",
      "\\bibitem{hossain2021bert}\n",
      "Hossain N and Ahnaf A, November 2021. ``Bert-based text simplification approach to reduce\n",
      "  linguistic complexity of bangla language,'' in {\\em 2021 International\n",
      "  Conference on Intelligent Technology, System and Service for Internet of\n",
      "  Everything (ITSS-IoE)}, pp. 1--5, IEEE.\n",
      "\n",
      "\\bibitem{shetu2020identifying}\n",
      "Shetu S F, Saifuzzaman M, M Parvin, Moon N N, Yousuf R, and Sultana S,\n",
      "  ``Identifying the writing style of bangla language using natural language\n",
      "  processing,'' in {\\em 2020 11th International Conference on Computing,\n",
      "  Communication and Networking Technologies (ICCCNT)}, pp. 1--6, IEEE.\n",
      "\n",
      "\\bibitem{sen2022bangla}\n",
      "Sen O, Fuad M, Islam M N, Rabbi J, M Masud, Hasan M K, ..., and\n",
      "  R M A Iftee, 2022. ``Bangla natural language processing: A comprehensive\n",
      "  analysis of classical, machine learning, and deep learning-based methods,''\n",
      "  {\\em IEEE Access}, vol. 10, pp. 38999--39044.\n",
      "\n",
      "\\bibitem{sikder2019automatic}\n",
      "Sikder R, Hossain M M, and Robi R F M H, 2019. ``Automatic text summarization\n",
      "  for bengali language including grammatical analysis,'' {\\em International\n",
      "  Journal of Scientific \\& Technology Research}, vol. 8, no. 6.\n",
      "\n",
      "\\bibitem{mridha2022study}\n",
      "Mridha M F, Ohi A Q, Hamid M A, and Monowar M M, 2022. ``A study on the\n",
      "  challenges and opportunities of speech recognition for bengali language,''\n",
      "  {\\em Artificial Intelligence Review}, pp. 1--25.\n",
      "\n",
      "\\bibitem{masum2019Abstractive}\n",
      "M A K Masum, Abujar S, I M A Talukder, A A S Rabby, and S A\n",
      "  Hossain, 2019. ``Abstractive method of text summarization with sequence to sequence\n",
      "  rnns,'' in {\\em 2019 10th international conference on computing,\n",
      "  communication and networking technologies (ICCCNT)}, pp. 1--5, IEEE.\n",
      "\n",
      "\\bibitem{chowdhury2021unsupervised}\n",
      "Chowdhury R R, Nayeem M T, Mim T T, R M S Chowdhury, and Jannat T,\n",
      "  ``Unsupervised abstractive summarization of bengali text documents,'' {\\em\n",
      "  arXiv preprint arXiv:2102.04490}.\n",
      "\n",
      "\\bibitem{mukherjee2022developing}\n",
      "Mukherjee A, {\\em Developing Bengali Text Summarization with Transformer Base\n",
      "  Model}.\n",
      "\\newblock Doctoral dissertation, National College of Ireland, Dublin.\n",
      "\n",
      "\\bibitem{king2015practical}\n",
      "King B P, {\\em Practical Natural Language Processing for Low-Resource\n",
      "  Languages}.\n",
      "\\newblock Doctoral dissertation.\n",
      "\n",
      "\\bibitem{bhattacharjee2022banglanlg}\n",
      "Bhattacharjee A, Hasan T, Ahmad W U, and Shahriyar R, 2022. ``Banglanlg:\n",
      "  Benchmarks and resources for evaluating low-resource natural language\n",
      "  generation in bangla,'' {\\em arXiv preprint arXiv:2205.11081}.\n",
      "\n",
      "\\bibitem{alam2021review}\n",
      "Alam F, Hasan A, T Alam, Khan A, J Tajrin, Khan N, and Chowdhury S A,\n",
      "  ``A review of bangla natural language processing tasks and the utility of\n",
      "  transformer models,'' {\\em arXiv preprint arXiv:2107.03844}.\n",
      "\n",
      "\\bibitem{karim2013technical}\n",
      "Karim M A, ed., {\\em Technical Challenges and Design Issues in Bangla\n",
      "  Language Processing}.\n",
      "\\newblock IGI Global.\n",
      "\n",
      "\\bibitem{gupta2021training}\n",
      "Gupta A, Vavre A, and Sarawagi S, June 2021. ``Training data augmentation for\n",
      "  code-mixed translation,'' in {\\em Proceedings of the 2021 Conference of the\n",
      "  North American Chapter of the Association for Computational Linguistics:\n",
      "  Human Language Technologies}, pp. 5760--5766.\n",
      "\n",
      "\\bibitem{hoque2020bdsl36}\n",
      "Hoque O B, Jubair M I, Akash A A, and Islam S, 2020. ``Bdsl36: A dataset for\n",
      "  bangladeshi sign letters recognition,'' in {\\em Proceedings of the Asian\n",
      "  Conference on Computer Vision}.\n",
      "\n",
      "\\bibitem{zhang2019hibert}\n",
      "Zhang X, Wei F, and Zhou M, 2019. ``Hibert: Document level pre-training of\n",
      "  hierarchical bidirectional transformers for document summarization,'' {\\em\n",
      "  arXiv preprint arXiv:1905.06566}.\n",
      "\n",
      "\\bibitem{hasan2020not}\n",
      "Hasan T, Bhattacharjee A, K Samin, Hasan M, M Basak, Rahman M S, and\n",
      "  R Shahriyar, 2020. ``Not low-resource anymore: Aligner ensembling, batch\n",
      "  filtering, and new datasets for bengali-english machine translation,'' {\\em\n",
      "  arXiv preprint arXiv:2009.09359}.\n",
      "\n",
      "\\bibitem{sen2021bangla}\n",
      "Sen O, Fuad M, Islam M N, Rabbi J, Hasan M K, BAZ M, and RAIHAN M A,\n",
      "  ``Bangla natural language processing: A comprehensive review of classical\n",
      "  machine learning and deep learning based methods,'' {\\em arXiv preprint\n",
      "  arXiv:2105.14875}.\n",
      "\n",
      "\\bibitem{uddin2007study}\n",
      "Uddin M N and Khan S A, December 2007. ``A study on text summarization techniques and\n",
      "  implement few of them for bangla language,'' in {\\em 2007 10th International\n",
      "  Conference on Computer and Information Technology (ICCIT)}, (Dhaka,\n",
      "  Bangladesh), pp. 1--4, IEEE.\n",
      "\n",
      "\\bibitem{efat2013automated}\n",
      "A M I Efat, Ibrahim M, and Kayesh H, 2013. ``Automated bangla text summarization\n",
      "  by sentence scoring and ranking,'' in {\\em 2013 International Conference on\n",
      "  Informatics, Electronics and Vision (ICIEV)}, pp. 1--5, IEEE.\n",
      "\n",
      "\\bibitem{haque2015automatic}\n",
      "Haque M M, Pervin S, and Begum Z, 2015. ``Automatic bengali news documents\n",
      "  summarization by introducing sentence frequency and clustering,'' in {\\em\n",
      "  2015 18th International Conference on Computer and Information Technology\n",
      "  (ICCIT)}, pp. 156--160, IEEE.\n",
      "\n",
      "\\bibitem{haque2016enhancement}\n",
      "Haque M M, Pervin S, and Begum Z, 2016. ``Enhancement of keyphrase-based approach\n",
      "  of automatic bangla text summarization,'' in {\\em 2016 IEEE Region 10\n",
      "  Conference (TENCON)}, pp. 42--46, IEEE.\n",
      "\n",
      "\\bibitem{paul2017bangla}\n",
      "Paul A, Imtiaz M T, Latif A H, Ahmed M, Adnan F A, Khan R, I Kadery,\n",
      "  and Rahman R M, 2017. ``Bangla news summarization,'' in {\\em Computational\n",
      "  Collective Intelligence: 9th International Conference, ICCCI 2017, Nicosia,\n",
      "  Cyprus, September 27-29, 2017, Proceedings, Part II 9}, pp. 479--488,\n",
      "  Springer.\n",
      "\n",
      "\\bibitem{abujar2017heuristic}\n",
      "Abujar S, Hasan M, M Shahin, and Hossain S A, 2017. ``A heuristic approach of\n",
      "  text summarization for bengali documentation,'' in {\\em 2017 8th\n",
      "  International Conference on Computing, Communication and Networking\n",
      "  Technologies (ICCCNT)}, pp. 1--8, IEEE.\n",
      "\n",
      "\\bibitem{akter2017Extractive}\n",
      "Akter S, Asa A S, Uddin M P, Hossain M D, Roy S K, and Afjal M I,\n",
      "  ``An extractive text summarization technique for bengali document (s) using\n",
      "  k-means clustering algorithm,'' in {\\em 2017 ieee international conference on\n",
      "  imaging, vision \\& pattern recognition (icivpr)}, pp. 1--6, IEEE.\n",
      "\n",
      "\\bibitem{chowdhury2017approach}\n",
      "Chowdhury S R, Sarkar K, and Dam S, 2017. ``An approach to generic bengali text\n",
      "  summarization using latent semantic analysis,'' in {\\em 2017 international\n",
      "  conference on information technology (ICIT)}, pp. 11--16, IEEE.\n",
      "\n",
      "\\bibitem{ghosh2018rule}\n",
      "Ghosh P P, Shahariar R, and  H M A Khan, 2018. ``A rule based extractive text\n",
      "  summarization technique for bangla news documents,'' {\\em International\n",
      "  Journal of Modern Education and Computer Science}, vol. 10, no. 12, p 44.\n",
      "\n",
      "\\bibitem{tumpa2018improved}\n",
      "Tumpa P, Yeasmin S, A Nitu, Uddin M, M Afjal, and Mamun M, 2018. ``An improved\n",
      "  extractive summarization technique for bengali text (s),'' in {\\em 2018\n",
      "  International Conference on Computer, Communication, Chemical, Material and\n",
      "  Electronic Engineering (IC4ME2)}, pp. 1--4, IEEE.\n",
      "\n",
      "\\bibitem{chandro2018automated}\n",
      "Chandro P, H M F Arif, Rahman M M, Siddik M S, Rahman M S, and M A\n",
      "  Rahman, 2018. ``Automated bengali document summarization by collaborating\n",
      "  individual word \\& sentence scoring,'' in {\\em 2018 21st International\n",
      "  Conference of Computer and Information Technology (ICCIT)}, pp. 1--6, IEEE.\n",
      "\n",
      "\\bibitem{sarkar2018automatic}\n",
      "Sarkar A and Hossen M S, 2018. ``Automatic bangla text summarization using term\n",
      "  frequency and semantic similarity approach,'' in {\\em 2018 21st International\n",
      "  Conference of Computer and Information Technology (ICCIT)}, pp. 1--6, IEEE.\n",
      "\n",
      "\\bibitem{talukder2019bengali}\n",
      "I M A Talukder, Abujar S, M A K Masum, Faisal F, and Hossain S A,\n",
      "  ``Bengali abstractive text summarization using sequence to sequence rnns,''\n",
      "  in {\\em 2019 10th International Conference on Computing, Communication and\n",
      "  Networking Technologies (ICCCNT)}, pp. 1--5, IEEE.\n",
      "\n",
      "\\bibitem{abujar2019approach}\n",
      "Abujar S, M A K Masum, Mohibullah M, Hossain S A, {\\em et al.}, 2019. ``An\n",
      "  approach for bengali text summarization using word2vector,'' in {\\em 2019\n",
      "  10th International Conference on Computing, Communication and Networking\n",
      "  Technologies (ICCCNT)}, pp. 1--5, IEEE.\n",
      "\n",
      "\\bibitem{masum2019sentence}\n",
      "M A K Masum, Abujar S, H R T Tusher, Faisal F, and Hossain S A,\n",
      "  ``Sentence similarity measurement for bengali abstractive text\n",
      "  summarization,'' in {\\em 2019 10th International Conference on Computing,\n",
      "  Communication and Networking Technologies (ICCCNT)}, pp. 1--5, IEEE.\n",
      "\n",
      "\\bibitem{rahman2019bengali}\n",
      "Rahman A, Rafiq F M, Saha R, R Rafian, and Arif H, 2019. ``Bengali text\n",
      "  summarization using textrank, fuzzy c-means and aggregate scoring methods,''\n",
      "  in {\\em 2019 IEEE Region 10 Symposium (TENSYMP)}, pp. 331--336, IEEE.\n",
      "\n",
      "\\bibitem{abujar2020bengali}\n",
      "Abujar S, M A K Masum, M Sanzidul Islam, Faisal F, and Hossain S A, 2020. ``A\n",
      "  bengali text generation approach in context of abstractive text summarization\n",
      "  using rnn,'' {\\em Innovations in Computer Science and Engineering:\n",
      "  Proceedings of 7th ICICSE}, pp. 509--518.\n",
      "\n",
      "\\bibitem{bhattacharjee2021bengali}\n",
      "Bhattacharjee P, Mallick A, and M Saiful Islam, 2021. ``Bengali abstractive news\n",
      "  summarization (bans): a neural attention approach,'' in {\\em Proceedings of\n",
      "  International Conference on Trends in Computational and Cognitive\n",
      "  Engineering: Proceedings of TCCE 2020}, pp. 41--51, Springer.\n",
      "\n",
      "\\bibitem{fouzia2021bengali}\n",
      "Fouzia F A, Rahat M A, Alie M T-Al-Mahdi, M A K Masum, Abujar S, and\n",
      "  Hossain S A, 2021. ``A bengali text summarization using encoder-decoder based on\n",
      "  social media dataset,'' in {\\em Emerging Technologies in Data Mining and\n",
      "  Information Security: Proceedings of IEMIS 2020, Volume 2}, pp. 539--549,\n",
      "  Springer.\n",
      "\n",
      "\\bibitem{islam2021abstraction}\n",
      "Islam M M, Islam M, M A K Masum, Abujar S, and Hossain S A,\n",
      "  ``Abstraction based bengali text summarization using bi-directional attentive\n",
      "  recurrent neural networks,'' in {\\em Emerging Technologies in Data Mining and\n",
      "  Information Security: Proceedings of IEMIS 2020, Volume 2}, pp. 317--327,\n",
      "  Springer.\n",
      "\n",
      "\\bibitem{gupta2022automated}\n",
      "Gupta A, Chugh D, Anjum, and Katarya R, 2022. ``Automated news summarization using\n",
      "  transformers,'' in {\\em Sustainable Advanced Computing: Select Proceedings of\n",
      "  ICSAC 2021}, (Singapore), pp. 249--259, Springer Singapore.\n",
      "\n",
      "\\bibitem{ranganathan2022text}\n",
      "Ranganathan J and Abuka G, November 2022. ``Text summarization using transformer model,'' in\n",
      "  {\\em 2022 Ninth International Conference on Social Networks Analysis,\n",
      "  Management and Security (SNAMS)}, pp. 1--5, IEEE.\n",
      "\n",
      "\\bibitem{kumar2023Abstractive}\n",
      "Kumar S and Solanki A, 2023. ``An abstractive text summarization technique using\n",
      "  transformer model with self-attention mechanism,'' {\\em Neural Computing and\n",
      "  Applications}, pp. 1--20.\n",
      "\n",
      "\\bibitem{sultana2022bengali}\n",
      "Sultana M, Chakraborty P, and Choudhury T, 2022. ``Bengali abstractive news\n",
      "  summarization using seq2seq learning with attention,'' in {\\em Cyber\n",
      "  Intelligence and Information Retrieval: Proceedings of CIIR 2021},\n",
      "  pp. 279--289, Springer.\n",
      "\n",
      "\\bibitem{Barsha2023}\n",
      "Hoque F A Barsha and Uddin M N, 2023. ``Comparative analysis of banglat5 and\n",
      "  pointer generator network for bengali abstractive story summarization,'' in\n",
      "  {\\em 2023 International Conference on Information and Communication\n",
      "  Technology for Sustainable Development (ICICT4SD)}, pp. 84--88.\n",
      "\n",
      "\\bibitem{bhattacharjee_banglanlg_2023}\n",
      "Bhattacharjee A, Hasan T, Ahmad W U, and Shahriyar R, May 2023. ``{BanglaNLG} and\n",
      "  {BanglaT5}: {Benchmarks} and {Resources} for {Evaluating} {Low}-{Resource}\n",
      "  {Natural} {Language} {Generation} in {Bangla},'' in {\\em Findings of the\n",
      "  {Association} for {Computational} {Linguistics}: {EACL} 2023} (A Vlachos and\n",
      "  I Augenstein, eds.), (Dubrovnik, Croatia), pp. 726--735, Association for\n",
      "  Computational Linguistics.\n",
      "\n",
      "\\bibitem{landro2022two}\n",
      "Landro N, Gallo I, R La Grassa, and Federici E, 2022. ``Two new datasets for\n",
      "  italian-language abstractive text summarization,'' {\\em Information},\n",
      "  vol. 13, no. 5, p 228.\n",
      "\n",
      "\\bibitem{phan2022vit5}\n",
      "Phan L, Tran H, H Nguyen, and Trinh T H, 2022. ``Vit5: Pretrained text-to-text\n",
      "  transformer for vietnamese language generation,'' {\\em arXiv preprint\n",
      "  arXiv:2205.06457}.\n",
      "\n",
      "\\bibitem{khandelwal2019sample}\n",
      "Khandelwal U, Clark K, D Jurafsky, and Kaiser L, 2019. ``Sample efficient text\n",
      "  summarization using a single pre-trained transformer,'' {\\em arXiv preprint\n",
      "  arXiv:1905.08836}.\n",
      "\n",
      "\\bibitem{guan2020survey}\n",
      "Guan W, Smetannikov I, and Tianxing M, 2020. ``Survey on automatic text\n",
      "  summarization and transformer models applicability,'' in {\\em Proceedings of\n",
      "  the 2020 1st International Conference on Control, Robotics and Intelligent\n",
      "  System}, pp. 176--184.\n",
      "\n",
      "\\bibitem{zhao2022calibrating}\n",
      "Zhao Y, Khalman M, R Joshi, Narayan S, M Saleh, and Liu P J,\n",
      "  ``Calibrating sequence likelihood improves conditional language generation,''\n",
      "  {\\em arXiv preprint arXiv:2210.00045}.\n",
      "\n",
      "\\bibitem{zhao2023stepbystep}\n",
      "Zhao S, Li Q, T He, and Wen J, 2023. ``A step-by-step gradient penalty with\n",
      "  similarity calculation for text summary generation,'' {\\em Neural Processing\n",
      "  Letters}, vol. 55, no. 4, pp. 4111--4126.\n",
      "\n",
      "\\bibitem{tang2022textbox2}\n",
      "Tang T, Li J, Z Chen, Hu Y, Z Yu, Dai W, and Wen J R, 2022. ``Textbox 2.0: A\n",
      "  text generation library with pre-trained language models,'' {\\em arXiv\n",
      "  preprint arXiv:2212.13005}.\n",
      "\n",
      "\\bibitem{laquatra2023bartit}\n",
      "M La Quatra and Cagliero L, 2023. ``Bart-it: An efficient sequence-to-sequence\n",
      "  model for italian text summarization,'' {\\em Future Internet}, vol. 15,\n",
      "  no. 1, p 15.\n",
      "\n",
      "\\bibitem{landro2022twonew}\n",
      "Landro N, Gallo I, R La Grassa, and Federici E, 2022. ``Two new datasets for\n",
      "  italian-language abstractive text summarization,'' {\\em Information},\n",
      "  vol. 13, no. 5, p 228.\n",
      "\n",
      "\\bibitem{vaswani2017attention}\n",
      "Vaswani A, Shazeer N, N Parmar, Uszkoreit J, L Jones, Gomez A N,\n",
      "  {\\L}. Kaiser, and Polosukhin I, 2017. ``Attention is all you need,'' {\\em Advances\n",
      "  in neural information processing systems}, vol. 30.\n",
      "\n",
      "\\bibitem{dhar2021pointer}\n",
      "Dhar N, Saha G, P Bhattacharjee, Mallick A, and Islam M S, 2021. ``Pointer over\n",
      "  attention: An improved bangla text summarization approach using hybrid\n",
      "  pointer generator network,'' in {\\em 2021 24th International Conference on\n",
      "  Computer and Information Technology (ICCIT)}, pp. 1--5, IEEE.\n",
      "\n",
      "\\bibitem{bani-almarjeh2023arabic}\n",
      "M Bani-Almarjeh and Kurdy M B, 2023. ``Arabic abstractive text summarization using\n",
      "  rnn-based and transformer-based architectures,'' {\\em Information Processing\n",
      "  \\& Management}, vol. 60, no. 2, p 103227.\n",
      "\n",
      "\\bibitem{nunez-robinson2022comparative}\n",
      "D N{\\'u}{\\ n}ez-Robinson, J Talavera-Montalto, and Ugarte W, September 2022. ``A comparative\n",
      "  analysis on the summarization of legal texts using transformer models,'' in\n",
      "  {\\em International Conference on Advanced Research in Technologies,\n",
      "  Information, Innovation and Sustainability}, (Cham), pp. 372--386, Springer\n",
      "  Nature Switzerland.\n",
      "\n",
      "\\bibitem{lewis-etal-2020-bart}\n",
      "Lewis M, Liu Y, N Goyal, Ghazvininejad M, A Mohamed, Levy O, V Stoyanov,\n",
      "  and Zettlemoyer L, 2020. ``Bart: Denoising sequence-to-sequence pre-training for\n",
      "  natural language generation, translation, and comprehension,'' in {\\em\n",
      "  Proceedings of the 58th Annual Meeting of the Association for Computational\n",
      "  Linguistics}, pp. 7871--7880, Association for Computational Linguistics.\n",
      "\n",
      "\\bibitem{cnn_dailymail}\n",
      "McDonald R, Brokos G, and Zettlemoyer L, 2015. ``{CNN/Daily Mail} dataset.''\n",
      "  \\url{https://huggingface.co/datasets/cnn_dailymail}.\n",
      "\n",
      "\\bibitem{nallapati2016Abstractive}\n",
      "Nallapati R, Zhou B, C Gulcehre, and Xiang B, 2016. ``Abstractive text\n",
      "  summarization using sequence-to-sequence rnns and beyond,'' {\\em arXiv\n",
      "  preprint arXiv:1602.06023}.\n",
      "\n",
      "\\bibitem{farahani2021leveraging}\n",
      "Farahani M, Gharachorloo M, and Manthouri M, March 2021. ``Leveraging parsbert and\n",
      "  pretrained mt5 for persian abstractive text summarization,'' in {\\em 2021\n",
      "  26th International Computer Conference, Computer Society of Iran (CSICC)},\n",
      "  pp. 1--6, IEEE.\n",
      "\n",
      "\\bibitem{reda2022hybrid}\n",
      "Reda A, Salah N, J Adel, Ehab M, I Ahmed, Magdy M, {\\em et al.}, May 2022. ``A\n",
      "  hybrid arabic text summarization approach based on transformers,'' in {\\em\n",
      "  2022 2nd International Mobile, Intelligent, and Ubiquitous Computing\n",
      "  Conference (MIUCC)}, pp. 56--62, IEEE.\n",
      "\n",
      "\\bibitem{borah2022comparative}\n",
      "Borah M P, Dadure P, and Pakray P, 2022. ``Comparative analysis of t5 model for\n",
      "  abstractive text summarization on different datasets.\n",
      "\n",
      "\\bibitem{hossen2018bengali}\n",
      "Hossen M A, Govindaiah A, S Sultana, and A-A Bhuiyan, June 2018. ``Bengali sign\n",
      "  language recognition using deep convolutional neural network,'' in {\\em 2018\n",
      "  Joint 7th International Conference on Informatics, Electronics \\& Vision\n",
      "  (ICIEV) and 2018 2nd International Conference on Imaging, Vision \\& Pattern\n",
      "  Recognition (icIVPR)}, pp. 369--373, IEEE.\n",
      "\n",
      "\\bibitem{xue2020mt5}\n",
      "Xue L, Constant N, A Roberts, Kale M, R Al-Rfou, Siddhant A, ..., and\n",
      "  C Raffel, 2020. ``mt5: A massively multilingual pre-trained text-to-text\n",
      "  transformer,'' {\\em arXiv preprint arXiv:2010.11934}.\n",
      "\n",
      "\\bibitem{fuad2022crosslingual}\n",
      "Fuad A and M Al-Yahya, 2022. ``Cross-lingual transfer learning for arabic\n",
      "  task-oriented dialogue systems using multilingual transformer model mt5,''\n",
      "  {\\em Mathematics}, vol. 10, no. 5, p 746.\n",
      "\n",
      "\\bibitem{zhang2018improving}\n",
      "Zhang J, Luan H, M Sun, Zhai F, J Xu, Zhang M, and Liu Y, 2018. ``Improving\n",
      "  the transformer translation model with document-level context,'' {\\em arXiv\n",
      "  preprint arXiv:1810.03581}.\n",
      "\n",
      "\\bibitem{abadi2023enhancing}\n",
      "M V N Abadi and Ghasemian F, 2023. ``Enhancing persian text summarization using\n",
      "  the mt5 transformer model: A three-phased fine-tuning approach and\n",
      "  reinforcement learning.\n",
      "\n",
      "\\bibitem{goyal2021largerscale}\n",
      "Goyal N, Du J, M Ott, Anantharaman G, and Conneau A, 2021. ``Larger-scale\n",
      "  transformers for multilingual masked language modeling,'' {\\em arXiv preprint\n",
      "  arXiv:2105.00572}.\n",
      "\n",
      "\\bibitem{toledano-lopez2022finetuning}\n",
      "Toledano O G-López, Madera J, H González, A Simón-Cuevas, Demeester T,\n",
      "  and Mannens E, 2022. ``Finetuning mt5-based transformer via cma-es for sentiment\n",
      "  analysis,'' in {\\em Proceedings of the Third Workshop for Iberian Languages\n",
      "  Evaluation Forum (IberLEF 2022), CEUR WS Proceedings}.\n",
      "\n",
      "\\bibitem{ganesan2018rouge}\n",
      "Ganesan K, 2018. ``Rouge 2.0: Updated and improved measures for evaluation of\n",
      "  summarization tasks,'' {\\em arXiv preprint arXiv:1803.01937}.\n",
      "\n",
      "\\bibitem{westphaln2021arksey}\n",
      "Westphaln K K, Regoeczi W, M Masotya, B Vazquez-Westphaln, Lounsbury K,\n",
      "  L McDavid, Lee H, J Johnson, and Ronis S D, 2021. ``From arksey and o’malley\n",
      "  and beyond: Customizations to enhance a team-based, mixed approach to scoping\n",
      "  review methodology,'' {\\em MethodsX}, vol. 8, p 101375.\n",
      "\n",
      "\\bibitem{tricco2018prisma}\n",
      "Tricco A C, Lillie E, W Zarin, O K K'Brien, Colquhoun H, D Levac,\n",
      "  D Moher, Peters M D, Horsley T, L Weeks, {\\em et al.}, 2018. ``Prisma\n",
      "  extension for scoping reviews (prisma-scr): checklist and explanation,'' {\\em\n",
      "  Annals of internal medicine}, vol. 169, no. 7, pp. 467--473.\n",
      "\n",
      "\\bibitem{kumar2021study}\n",
      "Kumar Y, Kaur K, and Kaur S, 2021. ``Study of automatic text summarization\n",
      "  approaches in different languages,'' {\\em Artificial Intelligence Review},\n",
      "  vol. 54, no. 8, pp. 5897--5929.\n",
      "\n",
      "\\bibitem{csebuetnlp_mT5_multilingual_XLSum}\n",
      "Hasan T, Bhattacharjee A, Islam M S, Mubasshir K, Y-F Li, Y-B Kang,\n",
      "  Rahman M S, and Shahriyar R, 2021. ``{XL}-sum: Large-scale multilingual\n",
      "  abstractive summarization for 44 languages,'' in {\\em Findings of the\n",
      "  Association for Computational Linguistics: ACL-IJCNLP 2021}, (Online),\n",
      "  pp. 4693--4703, Association for Computational Linguistics.\n",
      "\n",
      "\\bibitem{grusky2018newsroom}\n",
      "Grusky M, Naaman M, and Artzi Y, 2018. ``Newsroom: A dataset of 1.3 million\n",
      "  summaries with diverse extractive strategies,'' {\\em arXiv preprint\n",
      "  arXiv:1804.11283}.\n",
      "\n",
      "\\bibitem{fabbri2019multinews}\n",
      "Fabbri A R, Li I, T She, Li S, and Radev D R, 2019. ``Multi-news: A\n",
      "  large-scale multi-document summarization dataset and abstractive hierarchical\n",
      "  model,'' {\\em arXiv preprint arXiv:1906.01749}.\n",
      "\n",
      "\\bibitem{liu2018generatingwikipedia}\n",
      "Liu P J, Saleh M, E Pot, Goodrich B, R Sepassi, Kaiser L, and\n",
      "  N Shazeer, 2018. ``Generating wikipedia by summarizing long sequences,'' {\\em\n",
      "  arXiv preprint arXiv:1801.10198}.\n",
      "\n",
      "\\bibitem{ladhak2020wikilingua}\n",
      "Ladhak F, Durmus E, C Cardie, and McKeown K, 2020. ``Wikilingua: A new benchmark\n",
      "  dataset for cross-lingual abstractive summarization,'' {\\em arXiv preprint\n",
      "  arXiv:2010.03093}.\n",
      "\n",
      "\\bibitem{ghalandari2020largescale}\n",
      "Ghalandari D G, Hokamp C, Pham N T, Glover J, and Ifrim G, 2020. ``A\n",
      "  large-scale multi-document summarization dataset from the wikipedia current\n",
      "  events portal,'' {\\em arXiv preprint arXiv:2005.10070}.\n",
      "\n",
      "\\bibitem{wojciech2021booksum}\n",
      "Wojciech K, Nazneen R, A Divyansh, Caiming X, and Dragomir R, 2021. ``Booksum: A\n",
      "  collection of datasets for long-form narrative summarization,'' {\\em CoRR}.\n",
      "\n",
      "\\bibitem{zhang2019email}\n",
      "Zhang R and Tetreault J, 2019. ``This email could save your life: Introducing the\n",
      "  task of email subject line generation,'' {\\em arXiv preprint\n",
      "  arXiv:1906.03497}.\n",
      "\n",
      "\\bibitem{yasunaga2019scisummnet}\n",
      "Yasunaga M, Kasai J, R Zhang, Fabbri A R, Li I, D Friedman, and D R\n",
      "  Radev, 2019. ``Scisummnet: A large annotated corpus and content-impact models for\n",
      "  scientific paper summarization with citation networks,'' in {\\em Proceedings\n",
      "  of the AAAI Conference on Artificial Intelligence}, vol. 33, pp. 7386--7393.\n",
      "\n",
      "\\bibitem{cohan2018discourse}\n",
      "Cohan A, Dernoncourt F, Kim D S, Bui T, S Kim, Chang W, and Goharian N,\n",
      "  ``A discourse-aware attention model for abstractive summarization of long\n",
      "  documents,'' {\\em arXiv preprint arXiv:1804.05685}.\n",
      "\n",
      "\\bibitem{chowdhury2019cqasumm}\n",
      "Chowdhury T and Chakraborty T, 2019. ``Cqasumm: Building references for community\n",
      "  question answering summarization corpora,'' in {\\em Proceedings of the ACM\n",
      "  India Joint International Conference on Data Science and Management of Data},\n",
      "  pp. 18--26.\n",
      "\n",
      "\\bibitem{sayali2020aquamuse}\n",
      "Sayali K, Sheide C, Z Wan, Fei S, and Eugene I, 2020. ``Aquamuse: Automatically\n",
      "  generating datasets for query-based multi-document summarization,'' {\\em\n",
      "  arXiv preprint arXiv:2010.12694}.\n",
      "\n",
      "\\bibitem{hasan-etal-2020-low}\n",
      "Hasan T, Bhattacharjee A, K Samin, Hasan M, M Basak, Rahman M S, and\n",
      "  R Shahriyar, 2020. ``Not low-resource anymore: Aligner ensembling, batch\n",
      "  filtering, and new datasets for {B}engali-{E}nglish machine translation,'' in\n",
      "  {\\em Proceedings of the 2020 Conference on Empirical Methods in Natural\n",
      "  Language Processing (EMNLP)}, (Online), pp. 2612--2623, Association for\n",
      "  Computational Linguistics.\n",
      "\n",
      "\\bibitem{pegasus_xsum}\n",
      "Zhang J, Liu Y, P Wei, Zhao X, L Shou, Palangi H, J Gao, Shen Y, and\n",
      "  B Dolan, 2019. ``Pegasus: Pre-training with extracted gap-sentences for\n",
      "  abstractive summarization.''\n",
      "  \\url{https://huggingface.co/google/pegasus-xsum}.\n",
      "\n",
      "\\bibitem{lewis2020bart}\n",
      "Lewis M, Liu Y, N Goyal, Ghazvininejad M, A Mohamed, Levy O, and\n",
      "  V Stoyanov, 2020. ``Bart: Denoising sequence-to-sequence pre-training for natural\n",
      "  language generation, translation, and comprehension.''\n",
      "  \\url{https://huggingface.co/facebook/bart-large-xsum}.\n",
      "\n",
      "\\bibitem{financial_summarization_pegasus}\n",
      "Passali T, Gidiotis A, E Chatzikyriakidis, and Tsoumakas G, 2023. ``Financial\n",
      "  summarization pegasus.''\n",
      "  \\url{https://huggingface.co/human-centered-summarization/financial-summarization-pegasus},\n",
      "  2023.\n",
      "\\newblock Accessed on Apr. 05.\n",
      "\n",
      "\\bibitem{sysresearch101_t5_large_finetuned_xsum}\n",
      "Kapre R, 2024. ``T5 large fine-tuned xsum.''\n",
      "  \\url{https://huggingface.co/sysresearch101/t5-large-finetuned-xsum}, 2024.\n",
      "\\newblock Accessed on May. 05.\n",
      "\n",
      "\\bibitem{pszemraj_long_t5_tglobal_base_16384_booksum_V11_big_patent_V2}\n",
      "Szemraj P, 2024. ``Long t5 tglobal base 16384 booksum v11 big patent v2.''\n",
      "  \\url{https://huggingface.co/pszemraj/long-t5-tglobal-base-16384-booksum-V11-big_patent-V2},\n",
      "  2024.\n",
      "\\newblock Accessed on Jan. 18.\n",
      "\n",
      "\\bibitem{nbroad_longt5_base_global_mediasum}\n",
      "Broad N, 2024. ``Longt5 base global mediasum.''\n",
      "  \\url{https://huggingface.co/nbroad/longt5-base-global-mediasum}, 2024.\n",
      "\\newblock Accessed on May. 05.\n",
      "\n",
      "\\bibitem{xysmalobia_pegasus_samsum}\n",
      "Nimmagadda S D and Paleti K, 2024. ``Pegasus samsum.''\n",
      "  \\url{https://huggingface.co/dhyutin/pegasus-samsum}, 2024.\n",
      "\\newblock Accessed on Jan. 18.\n",
      "\n",
      "\\bibitem{philschmid_distilbart_cnn_12_6_samsum}\n",
      "Schmid P, 2024. ``Distilbart cnn 12-6 samsum.''\n",
      "  \\url{https://huggingface.co/philschmid/distilbart-cnn-12-6-samsum}, 2024.\n",
      "\\newblock Accessed on Jan. 18.\n",
      "\n",
      "\\bibitem{pszemraj_long_t5_tglobal_base_16384_book_summary}\n",
      "Szemeraj P, 2022. ``Long t5 tglobal base 16384 book summary.''\n",
      "  \\url{https://huggingface.co/pszemraj/long-t5-tglobal-base-16384-book-summary},\n",
      "  2022.\n",
      "\\newblock Accessed on Nov. 30.\n",
      "\n",
      "\\bibitem{pszemraj_long_t5_tglobal_large_pubmed_3k_booksum_16384_WIP}\n",
      "Szemeraj P, 2022. ``Long t5 tglobal large pubmed 3k book sum 16384 wip.''\n",
      "  \\url{https://huggingface.co/pszemraj/long-t5-tglobal-large-pubmed-3k-booksum-16384-WIP},\n",
      "  2022.\n",
      "\\newblock Accessed on Nov. 30.\n",
      "\n",
      "\\bibitem{pszemraj_long_t5_tglobal_large_pubmed_3k_booksum_16384_WIP13}\n",
      "Szemraj P, 2022. ``Long t5 tglobal large pubmed 3k book sum 16384 wip13.''\n",
      "  \\url{https://huggingface.co/pszemraj/long-t5-tglobal-large-pubmed-3k-booksum-16384-WIP13},\n",
      "  2022.\n",
      "\\newblock Accessed on Nov. 30.\n",
      "\n",
      "\\bibitem{pszemraj_led_base_book_summary}\n",
      "Szemraj P, 2022. ``Led base book summary.''\n",
      "  \\url{https://huggingface.co/pszemraj/led-base-book-summary}, 2022.\n",
      "\\newblock Accessed on Nov. 30.\n",
      "\n",
      "\\bibitem{google_bigbird_pegasus_large_arxiv}\n",
      "Zaheer M, Guruganesh G, A Dubey, Ainslie J, C Alberti, Ontanon S,\n",
      "  P Pham, Ravula A, Q Wang, Yang L, and Ahmed A, 2024. ``Big bird: Transformers\n",
      "  for longer sequences.''\n",
      "  \\url{https://huggingface.co/google/bigbird-pegasus-large-arxiv}, 2021.\n",
      "\\newblock Accessed on Jan. 18.\n",
      "\n",
      "\\bibitem{nlp_hug}\n",
      "Wolf T, Debut L, V Sanh, Chaumond J, C Delangue, Moi A, P Cistac,\n",
      "  T Rault, Louf R, M Funtowicz, and Brew J, 2020. ``Nlp: datasets, metrics, and\n",
      "  evaluation methods for natural language processing in 100+ languages.\n",
      "\n",
      "\\bibitem{islam2020hybrid}\n",
      "Islam M, Majumdar F N, Galib A, and Hoque M M, 2020. ``Hybrid text summarizer\n",
      "  for bangla document,'' {\\em Int J Comput Vis Sig Process}, vol. 1, no. 1,\n",
      "  pp. 27--38.\n",
      "\n",
      "\\bibitem{islam2020summarizing}\n",
      "Islam M J, Ahammad K, and Chowdhury H M K, 2020. ``Summarizing online product\n",
      "  reviews in bengali based on similarity using sequence to sequence rnns,'' in\n",
      "  {\\em 2020 11th International Conference on Electrical and Computer\n",
      "  Engineering (ICECE)}, pp. 33--36, IEEE.\n",
      "\n",
      "\\bibitem{banik2022bangla}\n",
      "Banik N, Saha C, I Ahmed, and Shapna K A, 2022. ``Bangla text generation system\n",
      "  by incorporating attention in sequence-to-sequence model,'' {\\em World\n",
      "  Journal of Advanced Research and Reviews}, vol. 14, no. 1, pp. 080--094.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Process each entry\n",
    "formatted_entries = []\n",
    "\n",
    "# Define regex for recognizing month names\n",
    "month_names = r'January|February|March|April|May|June|July|August|September|October|November|December'\n",
    "\n",
    "for entry in entries:\n",
    "    # Step 2a: Find the publication date (either a year or a month-year combination)\n",
    "    last_comma_split = entry.rsplit(\",\", 1)\n",
    "    if len(last_comma_split) < 2:\n",
    "        continue  # Skip if the format is not as expected\n",
    "    \n",
    "    main_content, date_part = last_comma_split\n",
    "\n",
    "    # Match either a year or month-year format\n",
    "    year_match = re.search(r'\\b(?:(' + month_names + r')\\s+)?(20\\d{2})\\b', date_part)\n",
    "    if not year_match:\n",
    "        continue  # Skip if no valid date is found\n",
    "\n",
    "    # Extract the month (if present) and year\n",
    "    month = year_match.group(1) or ''  # Use month if present, otherwise empty\n",
    "    year = year_match.group(2)\n",
    "\n",
    "    # Construct the full publication date\n",
    "    publication_date = f\"{month} {year}\".strip()\n",
    "\n",
    "    # Step 2b: Replace `, `` ` with `, <publication date>. ```\n",
    "    formatted_entry = re.sub(r', ``', f', {publication_date}. ``', main_content)\n",
    "\n",
    "    # Reconstruct the entry with the new format\n",
    "    formatted_entries.append(f\"\\\\bibitem{formatted_entry}.\")\n",
    "\n",
    "# Join the entries back together with \\bibitem prefix\n",
    "formatted_bibliography = \"\\n\\n\".join(formatted_entries)\n",
    "\n",
    "print(formatted_bibliography)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
