{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "77475d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TFAutoModelForCausalLM, AutoTokenizer, AdamWeightDecay, pipeline, create_optimizer\n",
    "from transformers import DefaultDataCollator\n",
    "import tensorflow as tf\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "pio.renderers.default = 'notebook_connected'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81dccfe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 20:23:00.119630: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-06 20:23:00.119685: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-06 20:23:00.119705: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ifte-HomePC): /proc/driver/nvidia/version does not exist\n",
      "2023-02-06 20:23:00.120132: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at distilgpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = TFAutoModelForCausalLM.from_pretrained(\"distilgpt2\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "760b4d7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7526252782e5496691bdc53708b35190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0c7d3d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lyrics = pd.read_csv('/home/ifte-home/Downloads/lyrics-data.csv')\n",
    "df = pd.read_csv('/home/ifte-home/Downloads/imdb_sample/texts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1df2fabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = lyrics.astype('str')\n",
    "df = df.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a8045c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the songs with lyrics too long (after more than 1024 tokens, does not work)\n",
    "# df = df[df['Lyric'].apply(lambda x: len(x.split(' ')) < 350)]\n",
    "df = df[df['text'].apply(lambda x: len(x.split(' ')) < 350)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "34691f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Every once in a long while a movie will come along that will be so awful that I feel compelled to warn people. If I labor all my days and I can save but one soul from watching this movie, how great will be my joy.<br /><br />Where to begin my discussion of pain. For starters, there was a musical montage every five minutes. There was no character development. Every character was a stereotype. We had swearing guy, fat guy who eats donuts, goofy foreign guy, etc. The script felt as if it were being written as the movie was being shot. The production value was so incredibly low that it felt like I was watching a junior high video presentation. Have the directors, producers, etc. ever even seen a movie before? Halestorm is getting worse and worse with every new entry. The concept for this movie sounded so funny. How could you go wrong with Gary Coleman and a handful of somewhat legitimate actors. But trust me when I say this, things went wrong, VERY WRONG.'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0aee2314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Every once in a long while'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(df['text'].values[1].split(' ')[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "85052845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_text=df['Lyric'][0]\n",
    "prompt_text=' '.join(df['text'].values[1].split(' ')[:6])\n",
    "n_seqs=1\n",
    "max_length=374"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7906a438",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "# We are encoding the text using the gpt tokenizer. The return tensors are of type \"pt\"\n",
    "# since we are using PyTorch, not tensorflow\n",
    "output_sequences = model.generate(\n",
    "  input_ids=encoded_prompt,\n",
    "  max_length=max_length+len(encoded_prompt), # The model has to generate something, \n",
    "  # so we add the length of the original sequence to max_length\n",
    "  temperature=1.0,\n",
    "  top_k=0,\n",
    "  top_p=0.9,\n",
    "  repetition_penalty=1.2, # To ensure that we dont get repeated phrases\n",
    "  do_sample=True,\n",
    "  num_return_sequences=n_seqs\n",
    ") # We feed the encoded input into the model.\n",
    "## Getting the output ##\n",
    "if len(output_sequences.shape) > 2:\n",
    "    output_sequences.squeeze_() # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f1476d1",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 375), dtype=int32, numpy=\n",
       "array([[   51, 12003,   267,  8358,   304,    84,   627,  3529,   299,\n",
       "        21411,   410,  3755,    11,   198,    51, 11329,   410,  3755,\n",
       "           11, 38251,   198, 38351,   716,   283, 12776, 25792,   198,\n",
       "           32,  3876, 12776, 25792,   198,   198,    46,   384,    84,\n",
       "          716,   273, 38251,   401,    78,   334,  2611,   442,  1689,\n",
       "          257,   728,    64,   198, 15681,  8083,   390,   279,  3247,\n",
       "          263,   198,  5005,   279,  3247,   263,   198,   198,    36,\n",
       "           84,   474,  6557,   277,  1000,    72,   401, 37365,  8358,\n",
       "          299, 28749, 25211,   573,   390,   844,   283,   198,    53,\n",
       "          280,   573,   443,  7785,  7201,   319,  2934,   329,   198,\n",
       "        46181, 10819,   300, 35652,   198,    41,  6557,   277,   528,\n",
       "          390,   256, 12003,  7201,   299, 28749,   573,   583,  1082,\n",
       "          198,   198,  8491,    81, 25792,    11,   198, 37280, 10866,\n",
       "           11, 23781, 20005,    11, 23781,  1842,   401, 12776, 25792,\n",
       "          198,  8491,    81, 25792,    11,   198, 37280, 10866,    11,\n",
       "        23781, 20005,    11, 23781,  1842,   401, 12776, 25792,   198,\n",
       "          198,    34,  1872,    11,  1275,    72,    11,  1275,    72,\n",
       "           11,  1275,    72,    11,  1275,    72,  7201,   269,  6557,\n",
       "          198, 10814,    11, 17207,    11, 17207,   198, 47247,    12,\n",
       "         4598,    11, 28047,    12,  4598,    11,   410,  1872,   686,\n",
       "        21681,   308,   446,   361,  9093,   373,   952,  5875,   344,\n",
       "         1931,   292,   795,  6839,  3681,    13,   327, 40197, 26846,\n",
       "          316,   285, 15152, 15867,  2493, 15498,   275,  2271,   387,\n",
       "        35317,   305,   730,   385, 31110,   556,   265, 22238,  8836,\n",
       "        37427,   551,  1796,   268,   288, 44645, 12379,  4515,   324,\n",
       "          388,  8169,  4161,  1462,   466,   469,   260,  7813,  3263,\n",
       "           68, 18912, 25440,   698,  4533,  3671,   298,   368, 33721,\n",
       "          369,  3919,  1024,  1534,  1072, 28141,  8591,  9285, 14078,\n",
       "          366, 11498,  1819,  2743, 16964,  1288,  3795, 24611,  3400,\n",
       "        13370,   418,     1,   357,  1399, 46328,  4186,   569,  1734,\n",
       "           73,  5733,   290, 36300,   371,  3301,  2810,   262,  1708,\n",
       "         2420,   351,  3210,  2569,  1366,   546,   703,   356,  1625,\n",
       "          284,   651,   656,  3660,  3037,    25,   352,     8,   968,\n",
       "         7261,   318,  3562,   523,   326,   674,  9017,   389, 25921,\n",
       "          287,   883,   749, 20294,  2842,    26,   362,   828,  1103,\n",
       "          661, 12553,   511,  4931,   355,  1593,  3925,   508,   423,\n",
       "         5071,  3748, 15421,   393,  6712,  2861,  8914,   422,   611,\n",
       "          484,  2051,   281,  3663,  1626,  2405,  1231,  1719,  1682,\n",
       "         3177,   606,   878,   530, 14358,  1243, 13318,   706,   852,\n",
       "         7981,   286,  1223,  2073,   986,   513,   737,  4874,   345,\n",
       "          892, 19475,   618,  3375,   546,   649,  8514,  1487,  2130,\n",
       "          338,  1204,   835,   517,   621,  1752]], dtype=int32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fb0401d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_sequences = []\n",
    "for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
    "    #print(generated_sequence)\n",
    "    text = tokenizer.decode(generated_sequence)\n",
    "    total_sequence = (\n",
    "    prompt_text + text[len(tokenizer.decode(encoded_prompt[0], \n",
    "                                            clean_up_tokenization_spaces=True, )) :]\n",
    "    )\n",
    "    generated_sequences.append(total_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3da0a2d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Every once in a long while we start to see issues that grow out of it, the way racism and misogyny are portrayed. If there\\'s one thing I think people often lose when confronted with criticism about their experience does is saying something just because they don\\'t really mean what you\\'re doing or would like otherwise.\" In 2012, she began her readjustment project through working on \"the reasons why non-whites should avoid being women,\" which was reviewed by Shurman Hemline for MLive/Mazda eWeek from 2013 until January 2016. She spent six months here before committing herself fully back home next month after having led numerous daytime appearances as host at standup paddle wetsuit weeklies—at least three were omitted altogether this year due solely [to] design difficulty…For those unfamiliar already heeningly gay folks living behind bars say Stacey must have been waiting 30 minutes if African Americans committed so many misdemeanors every day during his July five appearance recording …We\\'ve seen despite Frank Ramsey its Over 2 times more than 3 IQ per mouse queergypie girl striptease,\" says feminists previously focused only upon Kim Campbell Tofkin, who has told our LATEST online partner Not Guess Woman posts explicitly antiabortion stances against all leftist celebrities over points within problem territories (but doesn: Posted again separately following remarks )\\nThe difference between these two camps conveniently goes beyond basic journalism animating heterosexuals exactly right now but an ideology equaling prejudice instead? For feminists taking some liberties making jokes dedicated specifically towards LGBTI men then perfectly sensible questions needn\\' no further ado whenever Dawkins appears atop CBS AND NBC op Edd Clark \\'02 listed most interviews where Joseph Moore had indeed heard somodendall comments regarding same sex marriage included birth control …\"What do immy sherrie lie consider sire?\"–Published video interviewer >Yet Powell explains']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1944f69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using BLEU score to compare the real sentences with the generated ones\n",
    "import statistics\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "scores=[]\n",
    "\n",
    "for i in range(len(test_set)):\n",
    "  reference = test_set['True_end_lyrics'][i]\n",
    "  candidate = test_set['Generated_lyrics'][i]\n",
    "  scores.append(sentence_bleu(reference, candidate))\n",
    "\n",
    "statistics.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ea63ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rouge score\n",
    "from rouge import Rouge\n",
    "rouge=Rouge()\n",
    "\n",
    "rouge.get_scores(test_set['Generated_lyrics'], test_set['True_end_lyrics'], avg=True, ignore_empty=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374a30a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "753640be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SongLyrics(Dataset):\n",
    "    \n",
    "#     def __init__(self, control_code, truncate=False, gpt2_type=\"gpt2\", max_length=1024):\n",
    "    def __init__(self, control_code, truncate=False, gpt2_type=\"distilgpt2\", max_length=1024):\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(gpt2_type)\n",
    "        self.lyrics = []\n",
    "\n",
    "        for row in df['text']:\n",
    "          self.lyrics.append(torch.tensor(\n",
    "                self.tokenizer.encode(f\"<|{control_code}|>{row[:max_length]}<|endoftext|>\")\n",
    "            ))\n",
    "                \n",
    "        if truncate:\n",
    "            self.lyrics = self.lyrics[:20000]\n",
    "        self.lyrics_count = len(self.lyrics)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.lyrics_count\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.lyrics[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "aedd4db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SongLyrics(df['text'], truncate=True, gpt2_type=\"gpt2\")\n",
    "# dataset = SongLyrics(df['text'], truncate=True, gpt2_type=\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "15bd5b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_tensor(new_tensor, packed_tensor, max_seq_len):\n",
    "    if packed_tensor is None:\n",
    "        return new_tensor, True, None\n",
    "    if new_tensor.size()[1] + packed_tensor.size()[1] > max_seq_len:\n",
    "        return packed_tensor, False, new_tensor\n",
    "    else:\n",
    "        packed_tensor = torch.cat([new_tensor, packed_tensor[:, 1:]], dim=1)\n",
    "        return packed_tensor, True, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bab31227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    dataset, model, tokenizer,\n",
    "    batch_size=16, epochs=20, lr=2e-5,\n",
    "    max_seq_len=400, warmup_steps=200,\n",
    "    gpt2_type=\"gpt2\", output_dir=\".\", output_prefix=\"wreckgar\",\n",
    "    test_mode=False,save_model_on_epoch=False,\n",
    "):\n",
    "\n",
    "    acc_steps = 100\n",
    "    device=torch.device(\"cuda\")\n",
    "#     device=torch.device(\"cpu\")\n",
    "    model = model.cuda()\n",
    "#     model = model.cpu()\n",
    "    model.train()\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=-1\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "    loss=0\n",
    "    accumulating_batch_count = 0\n",
    "    input_tensor = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        print(f\"Training epoch {epoch}\")\n",
    "        print(loss)\n",
    "        for idx, entry in tqdm(enumerate(train_dataloader)):\n",
    "            (input_tensor, carry_on, remainder) = pack_tensor(entry, input_tensor, 768)\n",
    "\n",
    "            if carry_on and idx != len(train_dataloader) - 1:\n",
    "                continue\n",
    "\n",
    "            input_tensor = input_tensor.to(device)\n",
    "            outputs = model(input_tensor, labels=input_tensor)\n",
    "            loss = outputs[0]\n",
    "            loss.backward()\n",
    "\n",
    "            if (accumulating_batch_count % batch_size) == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                model.zero_grad()\n",
    "\n",
    "            accumulating_batch_count += 1\n",
    "            input_tensor = None\n",
    "        if save_model_on_epoch:\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                os.path.join(output_dir, f\"{output_prefix}-{epoch}.pt\"),\n",
    "            )\n",
    "    return model\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "244f5276",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23219/1989976749.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Train the model on the specific data we have\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_23219/3400556425.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, model, tokenizer, batch_size, epochs, lr, max_seq_len, warmup_steps, gpt2_type, output_dir, output_prefix, test_mode, save_model_on_epoch)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#     device=torch.device(\"cpu\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m#     model = model.cpu()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/exp-Env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    747\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m         \"\"\"\n\u001b[0;32m--> 749\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/exp-Env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/exp-Env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/exp-Env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    662\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/exp-Env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    747\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m         \"\"\"\n\u001b[0;32m--> 749\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/exp-Env/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'CUDA_MODULE_LOADING'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CUDA_MODULE_LOADING'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'LAZY'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "#Train the model on the specific data we have\n",
    "model = train(dataset, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d1ed57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "806c94b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration CShorten--ML-ArXiv-Papers-d4365de626c01fa2\n",
      "Found cached dataset csv (/home/ifte-home/.cache/huggingface/datasets/CShorten___csv/CShorten--ML-ArXiv-Papers-d4365de626c01fa2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Unnamed: 0', 'Unnamed: 0.1', 'title', 'abstract'],\n",
       "    num_rows: 117592\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_dataset(\"CShorten/ML-ArXiv-Papers\", split='train')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60ea42b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /home/ifte-home/.cache/huggingface/datasets/CShorten___csv/CShorten--ML-ArXiv-Papers-d4365de626c01fa2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-5d0c4eea58019ac3.arrow and /home/ifte-home/.cache/huggingface/datasets/CShorten___csv/CShorten--ML-ArXiv-Papers-d4365de626c01fa2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-615abb6e0a05153b.arrow\n"
     ]
    }
   ],
   "source": [
    "data = data.train_test_split(shuffle = True, seed = 200, test_size=0.2)\n",
    "\n",
    "train = data[\"train\"]\n",
    "val = data[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5bc069c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(dtype='string', id=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdd398d7",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ifte-home/.cache/huggingface/datasets/CShorten___csv/CShorten--ML-ArXiv-Papers-d4365de626c01fa2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a0c7394a54c399b7.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ifte-home/.cache/huggingface/datasets/CShorten___csv/CShorten--ML-ArXiv-Papers-d4365de626c01fa2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-6b48318383812c82.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ifte-home/.cache/huggingface/datasets/CShorten___csv/CShorten--ML-ArXiv-Papers-d4365de626c01fa2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-ff4a9e8e71103fa3.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ifte-home/.cache/huggingface/datasets/CShorten___csv/CShorten--ML-ArXiv-Papers-d4365de626c01fa2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-241e78e969e7d0af.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ifte-home/.cache/huggingface/datasets/CShorten___csv/CShorten--ML-ArXiv-Papers-d4365de626c01fa2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a4fa3f570227c730.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ifte-home/.cache/huggingface/datasets/CShorten___csv/CShorten--ML-ArXiv-Papers-d4365de626c01fa2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-615c2d9ba6032a21.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ifte-home/.cache/huggingface/datasets/CShorten___csv/CShorten--ML-ArXiv-Papers-d4365de626c01fa2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-e9555c6809eec782.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ifte-home/.cache/huggingface/datasets/CShorten___csv/CShorten--ML-ArXiv-Papers-d4365de626c01fa2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-1b43519573906a74.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ifte-home/.cache/huggingface/datasets/CShorten___csv/CShorten--ML-ArXiv-Papers-d4365de626c01fa2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-c099b3ed6c2bf7b2.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ifte-home/.cache/huggingface/datasets/CShorten___csv/CShorten--ML-ArXiv-Papers-d4365de626c01fa2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-e839b03c9e4d8804.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ifte-home/.cache/huggingface/datasets/CShorten___csv/CShorten--ML-ArXiv-Papers-d4365de626c01fa2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-5dfb917cb527d515.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ifte-home/.cache/huggingface/datasets/CShorten___csv/CShorten--ML-ArXiv-Papers-d4365de626c01fa2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-4d13fcc064dde7cb.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ifte-home/.cache/huggingface/datasets/CShorten___csv/CShorten--ML-ArXiv-Papers-d4365de626c01fa2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-d5f29330fd33d07d.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ifte-home/.cache/huggingface/datasets/CShorten___csv/CShorten--ML-ArXiv-Papers-d4365de626c01fa2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-cbe6725c0bdbeb6d.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ifte-home/.cache/huggingface/datasets/CShorten___csv/CShorten--ML-ArXiv-Papers-d4365de626c01fa2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-3e22f07d4f1da0ad.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ifte-home/.cache/huggingface/datasets/CShorten___csv/CShorten--ML-ArXiv-Papers-d4365de626c01fa2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-304e75f0f3eb6bae.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ifte-home/.cache/huggingface/datasets/CShorten___csv/CShorten--ML-ArXiv-Papers-d4365de626c01fa2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-83c5339d282e4b23.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ifte-home/.cache/huggingface/datasets/CShorten___csv/CShorten--ML-ArXiv-Papers-d4365de626c01fa2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-4bd5baf7c7d79f58.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ifte-home/.cache/huggingface/datasets/CShorten___csv/CShorten--ML-ArXiv-Papers-d4365de626c01fa2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-b8af896ce41549dd.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ifte-home/.cache/huggingface/datasets/CShorten___csv/CShorten--ML-ArXiv-Papers-d4365de626c01fa2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-6fae1b889eb53276.arrow\n"
     ]
    }
   ],
   "source": [
    "# The tokenization function\n",
    "def tokenization(data):\n",
    "    tokens = tokenizer(data[\"abstract\"], padding=\"max_length\", truncation=True, max_length=300)\n",
    "    return tokens\n",
    "\n",
    "# Apply the tokenizer in batch mode and drop all the columns except the tokenization result\n",
    "train_token = train.map(tokenization, batched = True, remove_columns=[\"title\", \"abstract\", \"Unnamed: 0\", \"Unnamed: 0.1\"], num_proc=10)\n",
    "val_token = val.map(tokenization, batched = True, remove_columns=[\"title\", \"abstract\", \"Unnamed: 0\", \"Unnamed: 0.1\"], num_proc=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f599058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [220,\n",
       "  23413,\n",
       "  11,\n",
       "  867,\n",
       "  40522,\n",
       "  423,\n",
       "  587,\n",
       "  5150,\n",
       "  284,\n",
       "  1332,\n",
       "  262,\n",
       "  17895,\n",
       "  198,\n",
       "  24622,\n",
       "  1634,\n",
       "  2694,\n",
       "  286,\n",
       "  17019,\n",
       "  7686,\n",
       "  13,\n",
       "  383,\n",
       "  15185,\n",
       "  14805,\n",
       "  39185,\n",
       "  11,\n",
       "  198,\n",
       "  48126,\n",
       "  8776,\n",
       "  351,\n",
       "  4277,\n",
       "  8718,\n",
       "  12,\n",
       "  17143,\n",
       "  7307,\n",
       "  422,\n",
       "  3210,\n",
       "  8861,\n",
       "  11,\n",
       "  389,\n",
       "  3402,\n",
       "  198,\n",
       "  1462,\n",
       "  2038,\n",
       "  12034,\n",
       "  13,\n",
       "  3423,\n",
       "  356,\n",
       "  10176,\n",
       "  326,\n",
       "  416,\n",
       "  22124,\n",
       "  1780,\n",
       "  2746,\n",
       "  198,\n",
       "  11250,\n",
       "  20074,\n",
       "  355,\n",
       "  4096,\n",
       "  355,\n",
       "  20796,\n",
       "  286,\n",
       "  11525,\n",
       "  67,\n",
       "  654,\n",
       "  11,\n",
       "  1903,\n",
       "  12225,\n",
       "  11,\n",
       "  3585,\n",
       "  198,\n",
       "  1930,\n",
       "  1859,\n",
       "  11525,\n",
       "  12083,\n",
       "  11,\n",
       "  290,\n",
       "  14499,\n",
       "  3602,\n",
       "  16354,\n",
       "  17670,\n",
       "  11,\n",
       "  356,\n",
       "  460,\n",
       "  22188,\n",
       "  198,\n",
       "  49453,\n",
       "  262,\n",
       "  2854,\n",
       "  286,\n",
       "  39185,\n",
       "  319,\n",
       "  17895,\n",
       "  2276,\n",
       "  1634,\n",
       "  13,\n",
       "  775,\n",
       "  989,\n",
       "  198,\n",
       "  49453,\n",
       "  902,\n",
       "  319,\n",
       "  1936,\n",
       "  2968,\n",
       "  40522,\n",
       "  25,\n",
       "  6374,\n",
       "  1565,\n",
       "  11,\n",
       "  18551,\n",
       "  48,\n",
       "  11,\n",
       "  4217,\n",
       "  30386,\n",
       "  11,\n",
       "  7375,\n",
       "  14313,\n",
       "  11,\n",
       "  290,\n",
       "  39448,\n",
       "  198,\n",
       "  19608,\n",
       "  292,\n",
       "  316,\n",
       "  13,\n",
       "  3954,\n",
       "  4981,\n",
       "  2987,\n",
       "  9922,\n",
       "  422,\n",
       "  2026,\n",
       "  4,\n",
       "  284,\n",
       "  7600,\n",
       "  4,\n",
       "  319,\n",
       "  262,\n",
       "  4217,\n",
       "  30386,\n",
       "  13714,\n",
       "  198,\n",
       "  35312,\n",
       "  11,\n",
       "  290,\n",
       "  422,\n",
       "  3439,\n",
       "  4,\n",
       "  284,\n",
       "  9773,\n",
       "  4,\n",
       "  319,\n",
       "  7375,\n",
       "  14313,\n",
       "  13,\n",
       "  1550,\n",
       "  6374,\n",
       "  1565,\n",
       "  11,\n",
       "  3585,\n",
       "  45203,\n",
       "  11525,\n",
       "  12083,\n",
       "  198,\n",
       "  11664,\n",
       "  306,\n",
       "  10255,\n",
       "  328,\n",
       "  689,\n",
       "  262,\n",
       "  412,\n",
       "  2640,\n",
       "  2551,\n",
       "  1917,\n",
       "  357,\n",
       "  3791,\n",
       "  805,\n",
       "  2123,\n",
       "  435,\n",
       "  1539,\n",
       "  12131,\n",
       "  828,\n",
       "  39127,\n",
       "  1802,\n",
       "  4,\n",
       "  198,\n",
       "  4134,\n",
       "  23843,\n",
       "  319,\n",
       "  262,\n",
       "  4129,\n",
       "  6626,\n",
       "  351,\n",
       "  257,\n",
       "  45616,\n",
       "  379,\n",
       "  2608,\n",
       "  13,\n",
       "  17267,\n",
       "  3875,\n",
       "  11,\n",
       "  2854,\n",
       "  198,\n",
       "  26069,\n",
       "  4972,\n",
       "  1022,\n",
       "  777,\n",
       "  4981,\n",
       "  389,\n",
       "  6032,\n",
       "  14836,\n",
       "  319,\n",
       "  262,\n",
       "  314,\n",
       "  2389,\n",
       "  1366,\n",
       "  6626,\n",
       "  13,\n",
       "  198,\n",
       "  1212,\n",
       "  3848,\n",
       "  329,\n",
       "  1774,\n",
       "  2276,\n",
       "  1634,\n",
       "  21201,\n",
       "  5621,\n",
       "  329,\n",
       "  5922,\n",
       "  17019,\n",
       "  198,\n",
       "  3262,\n",
       "  5225,\n",
       "  326,\n",
       "  2276,\n",
       "  1096,\n",
       "  25735,\n",
       "  13,\n",
       "  775,\n",
       "  7271,\n",
       "  2650,\n",
       "  262,\n",
       "  2438,\n",
       "  284,\n",
       "  198,\n",
       "  260,\n",
       "  18230,\n",
       "  344,\n",
       "  674,\n",
       "  2482,\n",
       "  13,\n",
       "  198,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_token[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c1908a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ifte-home/.cache/huggingface/datasets/CShorten___csv/CShorten--ML-ArXiv-Papers-d4365de626c01fa2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a153e9964835a045.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ifte-home/.cache/huggingface/datasets/CShorten___csv/CShorten--ML-ArXiv-Papers-d4365de626c01fa2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-6c60992628b6fe49.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ifte-home/.cache/huggingface/datasets/CShorten___csv/CShorten--ML-ArXiv-Papers-d4365de626c01fa2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-8ac88c0e73db5e9b.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ifte-home/.cache/huggingface/datasets/CShorten___csv/CShorten--ML-ArXiv-Papers-d4365de626c01fa2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-5ecd3ee8c8bc1f4d.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ifte-home/.cache/huggingface/datasets/CShorten___csv/CShorten--ML-ArXiv-Papers-d4365de626c01fa2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-1f3cd47cfb0ae737.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ifte-home/.cache/huggingface/datasets/CShorten___csv/CShorten--ML-ArXiv-Papers-d4365de626c01fa2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-9b0feb3be1847a05.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ifte-home/.cache/huggingface/datasets/CShorten___csv/CShorten--ML-ArXiv-Papers-d4365de626c01fa2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-d33211e4ac4d0435.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ifte-home/.cache/huggingface/datasets/CShorten___csv/CShorten--ML-ArXiv-Papers-d4365de626c01fa2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-2e0b5f3645a4d41e.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ifte-home/.cache/huggingface/datasets/CShorten___csv/CShorten--ML-ArXiv-Papers-d4365de626c01fa2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-d43067be67dfd425.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ifte-home/.cache/huggingface/datasets/CShorten___csv/CShorten--ML-ArXiv-Papers-d4365de626c01fa2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-229c4568da5af7ef.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ifte-home/.cache/huggingface/datasets/CShorten___csv/CShorten--ML-ArXiv-Papers-d4365de626c01fa2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-8f05faddf3e06652.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ifte-home/.cache/huggingface/datasets/CShorten___csv/CShorten--ML-ArXiv-Papers-d4365de626c01fa2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-866f0fc4882dac7f.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ifte-home/.cache/huggingface/datasets/CShorten___csv/CShorten--ML-ArXiv-Papers-d4365de626c01fa2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-fce71329f80f178f.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ifte-home/.cache/huggingface/datasets/CShorten___csv/CShorten--ML-ArXiv-Papers-d4365de626c01fa2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-682003cc072ec2e8.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ifte-home/.cache/huggingface/datasets/CShorten___csv/CShorten--ML-ArXiv-Papers-d4365de626c01fa2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-7cce5dc458a7cbe0.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ifte-home/.cache/huggingface/datasets/CShorten___csv/CShorten--ML-ArXiv-Papers-d4365de626c01fa2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-e0c9bbcc3ecdb5a5.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ifte-home/.cache/huggingface/datasets/CShorten___csv/CShorten--ML-ArXiv-Papers-d4365de626c01fa2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-5eb4731faff05739.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ifte-home/.cache/huggingface/datasets/CShorten___csv/CShorten--ML-ArXiv-Papers-d4365de626c01fa2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-956f0feafe115c90.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ifte-home/.cache/huggingface/datasets/CShorten___csv/CShorten--ML-ArXiv-Papers-d4365de626c01fa2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-19ab7ad01e17c859.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ifte-home/.cache/huggingface/datasets/CShorten___csv/CShorten--ML-ArXiv-Papers-d4365de626c01fa2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-565b4f5b28cdcd28.arrow\n"
     ]
    }
   ],
   "source": [
    "# Create labels as a copy of input_ids\n",
    "def create_labels(text):\n",
    "    text[\"labels\"] = text[\"input_ids\"].copy()\n",
    "    return text\n",
    "\n",
    "# Add the labels column using map()\n",
    "lm_train = train_token.map(create_labels, batched=True, num_proc=10)\n",
    "lm_val = val_token.map(create_labels, batched=True, num_proc=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7d10cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = model.prepare_tf_dataset(\n",
    "    lm_train,\n",
    "    shuffle=True,\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "validation_set = model.prepare_tf_dataset(\n",
    "    lm_val,\n",
    "    shuffle=False,\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efbb6fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the learning rate scheduler\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.0005,\n",
    "    decay_steps=500,\n",
    "    decay_rate=0.95,\n",
    "    staircase=False)\n",
    "    \n",
    "# Exponential decay learning rate\n",
    "optimizer = AdamWeightDecay(learning_rate=lr_schedule, weight_decay_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a5718ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tfgpt2lm_head_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " transformer (TFGPT2MainLaye  multiple                 81912576  \n",
      " r)                                                              \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 81,912,576\n",
      "Trainable params: 81,912,576\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab39f34d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "Invalid user token. If you didn't pass a user token, make sure you are properly logged in by executing `huggingface-cli login`, and if you did pass a user token, double-check it's correct.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/venvs/exp-Env/lib/python3.7/site-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/exp-Env/lib/python3.7/site-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/whoami-v2",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m~/venvs/exp-Env/lib/python3.7/site-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mwhoami\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m             \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/exp-Env/lib/python3.7/site-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    267\u001b[0m             )\n\u001b[0;32m--> 268\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-63e103f5-33dea8a63fa270032dda9ad6)\n\nRepository Not Found for url: https://huggingface.co/api/whoami-v2.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf the repo is private, make sure you are authenticated.\nInvalid username or password.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16253/3353335241.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mhub_model_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpush_to_hub_model_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mhub_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"your HF token\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m )\n",
      "\u001b[0;32m~/venvs/exp-Env/lib/python3.7/site-packages/transformers/keras_callbacks.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, save_strategy, save_steps, tokenizer, hub_model_id, hub_token, checkpoint, **model_card_args)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mhub_model_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabsolute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhub_model_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m             \u001b[0mhub_model_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_full_repo_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhub_model_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhub_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/exp-Env/lib/python3.7/site-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mget_full_repo_name\u001b[0;34m(model_id, organization, token)\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHfFolder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0morganization\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m         \u001b[0musername\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhoami\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    802\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34mf\"{username}/{model_id}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/exp-Env/lib/python3.7/site-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mwhoami\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m    600\u001b[0m                 \u001b[0;34m\"are properly logged in by executing `huggingface-cli login`, and \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m                 \u001b[0;34m\"if you did pass a user token, double-check it's correct.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m             ) from e\n\u001b[0m\u001b[1;32m    603\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: Invalid user token. If you didn't pass a user token, make sure you are properly logged in by executing `huggingface-cli login`, and if you did pass a user token, double-check it's correct."
     ]
    }
   ],
   "source": [
    "# This cell is optional\n",
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "\n",
    "model_name = \"GPT-2\"\n",
    "push_to_hub_model_id = f\"{model_name}-finetuned-papers\"\n",
    "\n",
    "push_to_hub_callback = PushToHubCallback(\n",
    "    output_dir=\"/home/ifte-home/work/clm_model_save\",\n",
    "    tokenizer=tokenizer,\n",
    "    hub_model_id=push_to_hub_model_id,\n",
    "    hub_token=\"your HF token\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2526a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"/home/ifte-home/venvs/exp-Env/lib/python3.7/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/ifte-home/venvs/exp-Env/lib/python3.7/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/ifte-home/venvs/exp-Env/lib/python3.7/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/ifte-home/venvs/exp-Env/lib/python3.7/site-packages/transformers/modeling_tf_utils.py\", line 1557, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/home/ifte-home/venvs/exp-Env/lib/python3.7/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 527, in minimize\n        self.apply_gradients(grads_and_vars)\n    File \"/home/ifte-home/venvs/exp-Env/lib/python3.7/site-packages/transformers/optimization_tf.py\", line 246, in apply_gradients\n        return super(AdamWeightDecay, self).apply_gradients(zip(grads, tvars), name=name, **kwargs)\n    File \"/home/ifte-home/venvs/exp-Env/lib/python3.7/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1140, in apply_gradients\n        return super().apply_gradients(grads_and_vars, name=name)\n    File \"/home/ifte-home/venvs/exp-Env/lib/python3.7/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 632, in apply_gradients\n        self._apply_weight_decay(trainable_variables)\n    File \"/home/ifte-home/venvs/exp-Env/lib/python3.7/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1162, in _apply_weight_decay\n        variables,\n    File \"/home/ifte-home/venvs/exp-Env/lib/python3.7/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1156, in distributed_apply_weight_decay\n        variable, weight_decay_fn, group=False\n    File \"/home/ifte-home/venvs/exp-Env/lib/python3.7/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1149, in weight_decay_fn  **\n        if self._use_weight_decay(variable):\n    File \"/home/ifte-home/venvs/exp-Env/lib/python3.7/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 587, in _use_weight_decay\n        for exclude_id in exclude_from_weight_decay:\n\n    TypeError: 'NoneType' object is not iterable\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16253/1166212667.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Fit with callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/venvs/exp-Env/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/exp-Env/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/exp-Env/lib/python3.7/site-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1556\u001b[0m         \u001b[0;31m# Run backwards pass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1557\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1559\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiled_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/exp-Env/lib/python3.7/site-packages/transformers/optimization_tf.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, name, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtvars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAdamWeightDecay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtvars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/home/ifte-home/venvs/exp-Env/lib/python3.7/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/ifte-home/venvs/exp-Env/lib/python3.7/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/ifte-home/venvs/exp-Env/lib/python3.7/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/ifte-home/venvs/exp-Env/lib/python3.7/site-packages/transformers/modeling_tf_utils.py\", line 1557, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/home/ifte-home/venvs/exp-Env/lib/python3.7/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 527, in minimize\n        self.apply_gradients(grads_and_vars)\n    File \"/home/ifte-home/venvs/exp-Env/lib/python3.7/site-packages/transformers/optimization_tf.py\", line 246, in apply_gradients\n        return super(AdamWeightDecay, self).apply_gradients(zip(grads, tvars), name=name, **kwargs)\n    File \"/home/ifte-home/venvs/exp-Env/lib/python3.7/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1140, in apply_gradients\n        return super().apply_gradients(grads_and_vars, name=name)\n    File \"/home/ifte-home/venvs/exp-Env/lib/python3.7/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 632, in apply_gradients\n        self._apply_weight_decay(trainable_variables)\n    File \"/home/ifte-home/venvs/exp-Env/lib/python3.7/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1162, in _apply_weight_decay\n        variables,\n    File \"/home/ifte-home/venvs/exp-Env/lib/python3.7/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1156, in distributed_apply_weight_decay\n        variable, weight_decay_fn, group=False\n    File \"/home/ifte-home/venvs/exp-Env/lib/python3.7/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1149, in weight_decay_fn  **\n        if self._use_weight_decay(variable):\n    File \"/home/ifte-home/venvs/exp-Env/lib/python3.7/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 587, in _use_weight_decay\n        for exclude_id in exclude_from_weight_decay:\n\n    TypeError: 'NoneType' object is not iterable\n"
     ]
    }
   ],
   "source": [
    "# Fit with callbacks\n",
    "model.fit(train_set, validation_data=validation_set, epochs=2, workers=9, use_multiprocessing=True,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1791fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ca54b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9f81e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
