Year,Paper title,Abstract summary,,Abstract,Takeaway suggests yes/no,Study type,Main findings,Outcomes measured,What was the dataset?,What was the main purpose?,What was the models?
,When One Textbook Is Not Enough: Linking Multiple Textbooks Using Probabilistic Topic Models,"The Web-revolution in publishing and reading is rapidly increasing the volume of online textbooks. Nowadays, for most of the subjects, a selection of online textbooks is available. Such an abundance leads to an interesting opportunity: if a student does not like how a primary textbook presents a particular topic s/he can always access its alternative (e.g. more detailed or advanced) presentation elsewhere. Modern e-learning environments could better support access to different versions of instructional material by generating intelligent links between the textbooks sections that present similar topics and concepts. This paper reports an attempt to investigate the problem of fine-grained intelligent linking of online textbooks based on the probabilistic topic modeling technology. Using collections of textbooks in two domains (Elementary Algebra and Information Retrieval), we have demonstrated that intelligent linking based on probabilistic topic models produces a much better modeling quality than traditional term-based approaches.",,,,,,,,,
2021,Neural labeled LDA: a topic model for semi-supervised  document classification,The proposed neural-label LDA performs significantly well on semi-supervised classification.,,"Recently, some statistical topic modeling approaches based on LDA have been applied in the field of supervised document classification, where the model generation procedure incorporates prior knowledge to improve the classification performance. However, these customizations of topic modeling are limited by the cumbersome derivation of a specific inference algorithm for each modification. In this paper, we propose a new supervised topic modeling approach for document classification problems, Neural Labeled LDA (NL-LDA), which builds on the VAE framework, and designs a special generative network to incorporate prior information. The proposed model can support semi-supervised learning based on the manifold assumption and low-density assumption. Meanwhile, NL-LDA has a consistent and concise inference method while semi-supervised learning and predicting. Quantitative experimental results demonstrate our model has outstanding performance on supervised document classification relative to the compared approaches, including traditional statistical and neural topic models. Specially, the proposed model can support both single-label and multi-label document classification. The proposed NL-LDA performs significantly well on semi-supervised classification, especially under a small amount of labeled data. Further comparisons with related works also indicate our model is competitive with state-of-the-art topic modeling approaches on semi-supervised classification.",,,"• Proposed a new supervised topic modeling approach for document classification problems, Neural Labeled LDA (NL-LDA), which builds on the VAE framework and designs a special generative network to incorporate prior information.

• NL-LDA has a consistent and concise inference method while semi-supervised learning and predicting.

• Quantitative experimental results demonstrate NL-LDA has outstanding performance on supervised document classification relative to the compared approaches, including traditional statistical and neural topic models.",•Classification Performance,,"to propose a new supervised topic modeling approach for document classification problems, Neural Labeled LDA (NL LDA), which builds on the VAE framework, and designs a special generative network to incorporate prior information",Neural Labeled LDA (NL LDA)
2021,Feature Augmentation for Improved Topic Modeling of Youtube Lecture Videos using Latent Dirichlet Allocation,The overall model interpretability is retained by introducing explainable features before modeling and using deep learning based text representation only at the post-modeling evaluation stage.,,"Application of Topic Models in text mining of educational data and more specifically, the text data obtained from lecture videos, is an area of research which is largely unexplored yet holds great potential. This work seeks to find empirical evidence for an improvement in Topic Modeling by pre−extracting bigram tokens and adding them as additional features in the Latent Dirichlet Allocation (LDA) algorithm, a widely−recognized topic modeling technique. The dataset considered for analysis is a collection of transcripts of video lectures on Machine Learning scraped from YouTube. Using the cosine similarity distance measure as a metric, the experiment showed a statistically significant improvement in topic model performance against the baseline topic model which did not use extra features, thus confirming the hypothesis. By introducing explainable features before modeling and using deep learning based text representation only at the post−modeling evaluation stage, the overall model interpretability is retained. This empowers educators and researchers alike to not only benefit from the LDA model in their own fields but also to play a substantial role in efforts to improve model performance. It also sets the direction for future work which could use the feature augmented topic model as the input to other more common text mining tasks like document categorization and information retrieval.",,,"• The experiment showed a statistically significant improvement in topic model performance when bigram tokens were pre-extracted and added as additional features in the Latent Dirichlet Allocation (LDA) algorithm.

• The overall model interpretability is retained by introducing explainable features before modeling and using deep learning based text representation only at the post-modeling evaluation stage.

• This sets the direction for future work which could use the feature augmented topic model as the input to other more common text mining tasks.",•Topic Model Performance,YouTube,"to find empirical evidence for an improvement in Topic Modeling by pre extracting bigram tokens and adding them as additional features in the Latent Dirichlet Allocation (LDA) algorithm, a widely recognized topic modeling technique",Latent Dirichlet Allocation (LDA)
2020,Twin labeled LDA: a supervised topic model for document classification,The proposed model is competitive with state-of-the-art approaches.,,"Recently, some statistic topic modeling approaches, e.g., Latent Dirichlet allocation (LDA), have been widely applied in the field of document classification. However, standard LDA is a completely unsupervised algorithm, and then there is growing interest in incorporating prior information into the topic modeling procedure. Some effective approaches have been developed to model different kinds of prior information, for example, observed labels, hidden labels, the correlation among labels, label frequencies; however, these methods often need heavy computing because of model complexity. In this paper, we propose a new supervised topic model for document classification problems, Twin Labeled LDA (TL-LDA), which has two sets of parallel topic modeling processes, one incorporates the prior label information by hierarchical Dirichlet distributions, the other models the grouping tags, which have prior knowledge about the label correlation; the two processes are independent from each other, so the TL-LDA can be trained efficiently by multi-thread parallel computing. Quantitative experimental results compared with state-of-the-art approaches demonstrate our model gets the best scores on both rank-based and binary prediction metrics in solving single-label classification, and gets the best scores on three metrics, i.e., One Error, Micro-F1, and Macro-F1 while multi-label classification, including non power-law and power-law datasets. The results show benefit from modeling fully prior knowledge, our model has outstanding performance and generalizability on document classification. Further comparisons with recent works also indicate the proposed model is competitive with state-of-the-art approaches.",,,"• Twin Labeled LDA (TL-LDA) is a new supervised topic model for document classification problems, which incorporates prior label information and the correlation among labels.

• TL-LDA can be trained efficiently by multi-thread parallel computing.

• Quantitative experimental results demonstrate that TL-LDA has outstanding performance and generalizability on document classification, and is competitive with state-of-the-art approaches.","•Rank Based Prediction Metrics
 •Binary Prediction Metrics
 •1 Error
 •Micro F1
 •Macro F1",,,Twin Labeled LDA (TL LDA)
2020,NET-LDA: a novel topic modeling method based on semantic document similarity,The semantic space of documents must be elaborated for correct identification of documents in a given corpus.,,"Topic models, such as latent Dirichlet allocation (LDA), allow us to categorize each document based on the topics. It builds a document as a mixture of topics and a topic is modeled as a probability distribution over words. However, the key drawback of the traditional topic model is that it cannot handle the semantic knowledge hidden in the documents. Therefore, semantically related, coherent and meaningful topics cannot be obtained. However, semantic inference plays a significant role in topic modeling as well as in other text mining tasks. In this paper, in order to tackle this problem, a novel NET-LDA model is proposed. In NET-LDA, semantically similar documents are merged to bring all semantically related words together and the obtained semantic similarity knowledge is incorporated into the model with a new adaptive semantic parameter. The motivation of the study is to reveal the impact of semantic knowledge in the topic model researches. Therefore, in a given corpus, different documents may contain different words but may speak about the same topic. For such documents to be correctly identified, the feature space of the documents must be elaborated with more powerful features. In order to accomplish this goal, the semantic space of documents is constructed with concepts and named entities. Two datasets in the English and Turkish languages and 12 different domains have been evaluated to show the independence of the model from both language and domain. The proposed NET-LDA, compared to the baselines, outperforms in terms of topic coherence, F-measure, and qualitative evaluation.",,,"• NET-LDA is a novel topic modeling method that incorporates semantic knowledge into the model with an adaptive semantic parameter.

• NET-LDA outperforms traditional topic models in terms of topic coherence, F-measure, and qualitative evaluation.

• NET-LDA is independent of language and domain, as demonstrated by experiments on two datasets in English and Turkish and 12 different domains.","•Topic Coherence
 •F Measure
 •Qualitative Evaluation",2 datasets in the English and Turkish languages and 12 different domains,"to propose a novel topic modeling method called NET LDA that incorporates semantic similarity knowledge to obtain semantically related, coherent and meaningful topics",NET LDA
2019,Improved Topic Modeling with Parallel-Supervised LDA,The supervised Latent Dirichlet Algorithm is applied together to obtain high accurate results with quicker response time.,,"In the modern era of digitalization, our day-to-day life is entirely dependent on digital platform-from raising our voice in social media to online shopping. Our collective knowledge is continued to be accumulated in the form of electronic texts, blogs, news, images, audios, videos and in many more ways and on account of this there is a greater need of analyzing these huge contents to get rid of the difficulties in searching the object, we aim for. Topic modelling is an efficient machine learning techniques for discovering the hidden semantic structure of contents. “Latent Dirichlet Allocation” (LDA) is a generative probabilistic topic modelling, which is the basis of other generative topic modelling techniques. New models are coming up with advanced algorithms in order to improve the topic modelling. Existing models have their own limitation. In case of obtaining more accuracy, the processing time of topic modelling goes high while in consideration of achieving more speed, accuracy gets low. Most of the algorithms implemented earlier cannot perform well in above-mentioned area. In this paper, we would like to introduce parallel-supervised LDA model where supervised Latent Dirichlet Algorithm (sLDA) and parallel Latent Dirichlet Algorithm (pLDA) are applied together to obtain high accurate results with quicker response time.",,,"• The proposed parallel-supervised LDA model combines supervised Latent Dirichlet Algorithm (sLDA) and parallel Latent Dirichlet Algorithm (pLDA) to obtain high accurate results with quicker response time.

• The model is able to improve the accuracy of topic modelling while reducing the processing time.

• The model is suitable for large-scale datasets and can be used for various applications.",,,to introduce a new topic modeling algorithm called parallel supervised LDA that combines supervised Latent Dirichlet Algorithm (sLDA) and parallel Latent Dirichlet Algorithm (pLDA) to obtain high accurate results with quicker response time,"supervised Latent Dirichlet Algorithm (sLDA), parallel Latent Dirichlet Algorithm (pLDA)"
2018,Topic Modelling in Bangla Language: An LDA Approach to Optimize Topics and News Classification,The document models are built using LDA with bigram.,,"Topic modeling is a powerful technique for unsupervised analysis of large document collections. Topic models have a wide range of applications including tag recommendation, text categorization, keyword extraction and similarity search in the text mining, information retrieval and statistical language modeling. The research on topic modeling is gaining popularity day by day. There are various efficient topic modeling techniques available for the English language as it is one of the most spoken languages in the whole world but not for the other spoken languages. Bangla being the seventh most spoken native language in the world by population, it needs automation in different aspects. This paper deals with finding the core topics of Bangla news corpus and classifying news with similarity measures. The document models are built using LDA (Latent Dirichlet Allocation) with bigram.",,,"• Topic modeling using LDA with bigram is an effective technique for unsupervised analysis of large document collections in Bangla language.

• The topics extracted from the Bangla news corpus are optimized and classified using similarity measures.

• The results of the study show that the LDA model is able to accurately identify the topics and classify the news.","•Core Topics Of Bangla News Corpus
 •Classification Of News With Similarity Measures",Bangla news corpus,to find the core topics of Bangla news corpus and classify news with similarity measures using LDA (Latent Dirichlet Allocation) with bigram,LDA (Latent Dirichlet Allocation) with bigram
2018,"Latent Dirichlet allocation (LDA) and topic modeling: models, applications, a survey",LDA approaches in topic modeling are popular in various fields.,,"Topic modeling is one of the most powerful techniques in text mining for data mining, latent data discovery, and finding relationships among data and text documents. Researchers have published many articles in the field of topic modeling and applied in various fields such as software engineering, political science, medical and linguistic science, etc. There are various methods for topic modelling; Latent Dirichlet Allocation (LDA) is one of the most popular in this field. Researchers have proposed various models based on the LDA in topic modeling. According to previous work, this paper will be very useful and valuable for introducing LDA approaches in topic modeling. In this paper, we investigated highly scholarly articles (between 2003 to 2016) related to topic modeling based on LDA to discover the research development, current trends and intellectual structure of topic modeling. In addition, we summarize challenges and introduce famous tools and datasets in topic modeling based on LDA.",,,"• Latent Dirichlet Allocation (LDA) is one of the most popular methods for topic modeling.

• Various models have been proposed based on the LDA in topic modeling.

• Challenges, tools, and datasets related to topic modeling based on LDA have been identified.",,,"to investigate highly scholarly articles related to topic modeling based on LDA to discover the research development, current trends and intellectual structure of topic modeling, summarize challenges and introduce famous tools and datasets in topic modeling based on LDA",Latent Dirichlet Allocation (LDA)
2017,A local context‐aware LDA model for topic modeling in a document network,The proposed model can differentiate the respective influence of each document in the local context on the target document according to both structural and temporal relationships between the two documents.,,"With the rapid development of the Internet and its applications, growing volumes of documents increasingly become interconnected to form large‐scale document networks. Accordingly, topic modeling in a network of documents has been attracting continuous research attention. Most of the existing network‐based topic models assume that topics in a document are influenced by its directly linked neighbouring documents in a document network and overlook the potential influence from indirectly linked ones. The existing work also has not carefully modeled variations of such influence among neighboring documents. Recognizing these modeling limitations, this paper introduces a novel Local Context‐Aware LDA Model (LC‐LDA), which is capable of observing a local context comprising a rich collection of documents that may directly or indirectly influence the topic distributions of a target document. The proposed model can also differentiate the respective influence of each document in the local context on the target document according to both structural and temporal relationships between the two documents. The proposed model is extensively evaluated through multiple document clustering and classification tasks conducted over several large‐scale document sets. Evaluation results clearly and consistently demonstrate the effectiveness and superiority of the new model with respect to several state‐of‐the‐art peer models.",,,"• The proposed Local Context-Aware LDA Model (LC-LDA) is capable of observing a local context comprising a rich collection of documents that may directly or indirectly influence the topic distributions of a target document.

• The proposed model can differentiate the respective influence of each document in the local context on the target document according to both structural and temporal relationships between the two documents.

• Evaluation results demonstrate the effectiveness and superiority of the new model with respect to several state-of-the-art peer models.",•Effectiveness Of The Proposed Local Context Aware Lda Model (Lc Lda) In Comparison To Several State Of The Art Peer Models,,to introduce a novel Local Context Aware LDA Model (LC LDA) for topic modeling in a network of documents that can differentiate the respective influence of each document in the local context on the target document according to both structural and temporal relationships between the 2 documents,Local Context Aware LDA Model (LC LDA)
2016,LDA-based Topic Modelling in Text Sentiment Classification: An Empirical Analysis,Latent Dirichlet allocation is a popular generative probabilistic model to represent collections of discrete data.,,"Sentiment analysis is the process of identifying the subjective information in the source materials towards an entity. It is a subfield of text and web mining. Web is a rich and progressively expanding source of information. Sentiment analysis can be modelled as a text classification problem. Text classification suffers from the high dimensional feature space and feature sparsity problems. The use of conventional representation schemes to represent text documents can be extremely costly especially for the large text collections. In this regard, data reduction techniques are viable tools in representing document collections. Latent Dirichlet allocation (LDA) is a popular generative probabilistic model to represent collections of discrete data. In this regard, this paper examines the performance of LDA in text sentiment classification. In the empirical analysis, five classification algorithms (Naïve Bayes, support vector machines, logistic regression, radial basis function network and K-nearest neighbor algorithms) and five ensemble methods (Bagging, AdaBoost, Random Subspace, voting and stacking) are evaluated on four sentiment datasets.",,,"• Latent Dirichlet Allocation (LDA) is a viable tool for representing document collections in text sentiment classification.

• Five classification algorithms (Naïve Bayes, support vector machines, logistic regression, radial basis function network and K-nearest neighbor algorithms) and five ensemble methods (Bagging, AdaBoost, Random Subspace, voting and stacking) were evaluated on four sentiment datasets.

• The results showed that LDA-based topic modelling improved the performance of the classification algorithms and ensemble methods in text sentiment classification.",•Performance Of Lda In Text Sentiment Classification,4 sentiment datasets,to examine the performance of Latent Dirichlet allocation (LDA) in text sentiment classification,Latent Dirichlet allocation (LDA)
2015,LDA based topic modeling of journal abstracts,Gibbs sampling outperforms CV B0 sampling.,,"Topic modeling is a powerful technique for unsupervised analysis of large document collections. Topic models conceive latent topics in text using hidden random variables, and discover that structure with posterior inference. Topic models have a wide range of applications like tag recommendation, text categorization, keyword extraction and similarity search in the broad fields of text mining, information retrieval, statistical language modeling. In this work, a dataset with 200 abstracts fall under four topics are collected from two different domain journals for tagging journal abstracts. The document models are built using LDA (Latent Dirichlet Allocation) with Collapsed Variational Bayes and Gibbs sampling. Then the built model is used to extract appropriate tags for abstracts. The performance of the built models are analyzed by the evaluation measure perplexity and observed that Gibbs sampling outperforms CV B0 sampling. Tags extracted by two algorithms remains almost the same.",,,"• LDA (Latent Dirichlet Allocation) with Collapsed Variational Bayes and Gibbs sampling can be used to build document models for tagging journal abstracts.

• Gibbs sampling outperforms CV B0 sampling in terms of perplexity.

• Tags extracted by two algorithms remain almost the same.",•Perplexity (As An Evaluation Measure),200 abstracts fall under 4 topics,,LDA (Latent Dirichlet Allocation)
2012,TweetLDA: supervised topic classification and link prediction in Twitter,L-LDA is an ideal classification technique for infrequent topics and for (short) profiles of moderately active users.,,"L-LDA is a new supervised topic model for assigning ""topics"" to a collection of documents (e.g., Twitter profiles). User studies have shown that L-LDA effectively performs a variety of tasks in Twitter that include not only assigning topics to profiles, but also re-ranking feeds, and suggesting new users to follow. Building upon these promising qualitative results, we here run an extensive quantitative evaluation of L-LDA. We test the extent to which, compared to the competitive baseline of Support Vector Machines (SVM), L-LDA is effective at two tasks: 1) assigning the correct topics to profiles; and 2) measuring the similarity of a profile pair. We find that L-LDA generally performs as well as SVM, and it clearly outperforms SVM when training data is limited, making it an ideal classification technique for infrequent topics and for (short) profiles of moderately active users. We have also built a web application that uses L-LDA to classify any given profile and graphically map predominant topics in specific geographic regions.",,,"• L-LDA is an effective supervised topic model for assigning topics to Twitter profiles.

• L-LDA performs as well as Support Vector Machines (SVM) for two tasks: assigning topics to profiles and measuring the similarity of a profile pair.

• L-LDA outperforms SVM when training data is limited, making it an ideal classification technique for infrequent topics and for (short) profiles of moderately active users.","•Correct Assignment Of Topics To Profiles
 •Similarity Measurement Of a Profile Pair","a collection of documents (e.g., Twitter profiles)",,"L LDA, Support Vector Machines (SVM)"
2012,Mr. LDA: a flexible large scale topic modeling package using variational inference in MapReduce,Variational implementations are easily extensible.,,"Latent Dirichlet Allocation (LDA) is a popular topic modeling technique for exploring document collections. Because of the increasing prevalence of large datasets, there is a need to improve the scalability of inference for LDA. In this paper, we introduce a novel and flexible large scale topic modeling package in MapReduce (Mr. LDA). As opposed to other techniques which use Gibbs sampling, our proposed framework uses variational inference, which easily fits into a distributed environment. More importantly, this variational implementation, unlike highly tuned and specialized implementations based on Gibbs sampling, is easily extensible. We demonstrate two extensions of the models possible with this scalable framework: informed priors to guide topic discovery and extracting topics from a multilingual corpus. We compare the scalability of Mr. LDA against Mahout, an existing large scale topic modeling package. Mr. LDA out-performs Mahout both in execution speed and held-out likelihood.",,,"• Mr. LDA is a novel and flexible large scale topic modeling package in MapReduce that uses variational inference.

• Mr. LDA is easily extensible and can be used to guide topic discovery and extract topics from a multilingual corpus.

• Mr. LDA outperforms Mahout in execution speed and held-out likelihood.","•Scalability Of Inference For Lda
 •Execution Speed
 •Held Out Likelihood",,,Latent Dirichlet Allocation (LDA)
2011,User interest modeling by labeled LDA with topic features,The LLDA-TF model can name user interests by category words as well as a keyword list for each category.,,"As well known, the user interest is carried in the user's web browsing history that can be mined out. This paper presents an innovative method to extract user's interests from his/her web browsing history. We first apply an efficient algorithm to extract useful texts from the web pages in user's browsed URL sequence. We then proposed a Labeled Latent Dirichlet Allocation with Topic Feature (LLDA-TF) to mine user's interests from the texts. Unlike other works that need a lot of training data to train a model to adopt supervised information, we directly introduce the raw supervised information to the procedure of LLDA-TF. As shown in the experimental results, results given by LLDA-TF fit predefined categories well. Furthermore, LLDA-TF model can name the user interests by category words as well as a keyword list for each category.",,,"• An efficient algorithm was proposed to extract useful texts from web pages in user's browsed URL sequence.

• Labeled Latent Dirichlet Allocation with Topic Feature (LLDA-TF) was proposed to mine user's interests from the texts.

• LLDA-TF model can name the user interests by category words as well as a keyword list for each category.",•User'S Interests,user's web browsing history,present an innovative method to extract user's interests from his/her web browsing history,Labeled Latent Dirichlet Allocation with Topic Feature (LLDA TF)
2010,D-LDA: A Topic Modeling Approach without Constraint Generation for Semi-defined Classification,The semi-supervised learning in D-LDA does not need the generation of pairwise constraints.,,"We study what we call semi-defined classification, which deals with the categorization tasks where the taxonomy of the data is not well defined in advance. It is motivated by the real-world applications, where the unlabeled data may also come from some other unknown classes besides the known classes for the labeled data. Given the unlabeled data, our goal is to not only identify the instances belonging to the known classes, but also cluster the remaining data into other meaningful groups. It differs from traditional semi-supervised clustering in the sense that in semi-supervised clustering the supervision knowledge is too far from being representative of a target classification, while in semi-defined classification the labeled data may be enough to supervise the learning on the known classes. In this paper we propose the model of Double-latent-layered LDA (D-LDA for short) for this problem. Compared with LDA with only one latent variable y for word topics, D-LDA contains another latent variable z for (known and unknown) document classes. With this double latent layers consisting of y and z and the dependency between them, D-LDA directly injects the class labels into z to supervise the exploiting of word topics in y. Thus, the semi-supervised learning in D-LDA does not need the generation of pair wise constraints, which is required in most of the previous semi-supervised clustering approaches. We present the experimental results on ten different data sets for semi-defined classification. Our results are either comparable to (on one data sets), or significantly better (on the other nine data set) than the six compared methods, including the state-of-the-art semi-supervised clustering methods.",,,"• We propose the Double-latent-layered LDA (D-LDA) model for semi-defined classification, which does not require the generation of pair wise constraints.

• Experimental results show that D-LDA is either comparable to or significantly better than the six compared methods, including the state-of-the-art semi-supervised clustering methods.

• D-LDA directly injects the class labels into the latent variable z to supervise the exploiting of word topics in y.",,,,Double latent layered LDA (D LDA)
2009,Topic Discovery Based on LDA Model with Fast Gibbs Sampling,The approach performs far better than other methods.,,"Topic discovery described here is used to determine the topic that a document or a segment discusses. It is very important for some applications of Natural Language Processing (NLP), such as information retrieval/extraction, summarization and topic analysis etc. The paper extracts topic words based on Shannon information, in which Latent Dirichlet Allocation (LDA) is employed to represent word distribution. The estimation of the parameters is speeded up by fast Gibbs sampling. Words which do not appear in the analyzed document can be inferred as topic with the help of word clustering of background. Topics are represented by means of word groups. The experiment results show that our approach performs far better than other methods.",,,"• Latent Dirichlet Allocation (LDA) is used to represent word distribution in order to extract topic words based on Shannon information.

• Fast Gibbs sampling is used to speed up the estimation of the parameters.

• Word clustering of background is used to infer words which do not appear in the analyzed document.","•Topic Words
 •Word Distribution
 •Word Clustering
 •Word Groups",,,Latent Dirichlet Allocation (LDA)
2009,Topic and keyword re-ranking for LDA-based topic modeling,LDA-based results may not be ideal for human understanding and consumption.,,"Topic-based text summaries promise to help average users quickly understand a text collection and derive insights. Recent research has shown that the Latent Dirichlet Allocation (LDA) model is one of the most effective approaches to topic analysis. However, the LDA-based results may not be ideal for human understanding and consumption. In this paper, we present several topic and keyword re-ranking approaches that can help users better understand and consume the LDA-derived topics in their text analysis. Our methods process the LDA output based on a set of criteria that model a user's information needs. Our evaluation demonstrates the usefulness of the methods in summarizing several large-scale, real world data sets.",,,"• Latent Dirichlet Allocation (LDA) is an effective approach to topic analysis.

• Topic and keyword re-ranking approaches can help users better understand and consume the LDA-derived topics in their text analysis.

• Evaluation demonstrates the usefulness of the methods in summarizing several large-scale, real world data sets.","•Effectiveness Of Latent Dirichlet Allocation (Lda) Model For Topic Analysis
 •Criteria For Modeling a User'S Information Needs
 •Usefulness Of Topic And Keyword Re Ranking Approaches In Summarizing Large Scale, Real World Data Sets","several large scale, real world data sets",to present several topic and keyword re ranking approaches that can help users better understand and consume the LDA derived topics in their text analysis,Latent Dirichlet Allocation (LDA)
