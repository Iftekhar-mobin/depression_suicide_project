{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import get_close_matches, SequenceMatcher\n",
    "import re\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from enchant import Dict, DictWithPWL\n",
    "\n",
    "load_clean_dataset()\n",
    "dataset = 'deep_clean_mobicontrol_data.txt'\n",
    "word_count_dict, all_letters = load_words_letters(dataset)\n",
    "vocab = load_vocabulary()\n",
    "dictionary = DictWithPWL(\"en_US\", 'vocab.txt')\n",
    "\n",
    "\n",
    "def load_words_letters(dataset):\n",
    "    hira_kata = 'かめふアうチパズヅさモぴグゆごヒサもシマりはゲひべヘイヤづペユへぽのほけエこツぺぢだをデどヨギぜミキリるろヌばむょラゴにウずしてすぬつスコネせムロたちゾぎゃおピぶンねガヲぱらカダメュョぷそナみノなんクホハニぞトでげワいャぐとザビやソプれぼきバベブジゅじゼまレセルびポがくわタドオケヂフボテえざあよ'\n",
    "    eng_letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    all_letters = hira_kata + eng_letters\n",
    "    word_count_dict = Counter(words(open(dataset).read()))\n",
    "    return word_count_dict, all_letters\n",
    "\n",
    "def load_vocabulary():\n",
    "    corpus_vocab_file = '/home/iftekhar/amiebot/exp_amiecore/amieCore/amie_core/core/retriever/Page_Ranking_Experiment/pipelines/vocabulary.txt'\n",
    "    with open(corpus_vocab_file) as f:\n",
    "        vocabulary = f.read().splitlines()\n",
    "    lines = [line.lower() for line in corpus_vocab_file]\n",
    "    with open('vocab.txt', 'w') as out:\n",
    "         out.writelines(lines)\n",
    "    return [vocab.lower() for vocab in vocabulary]\n",
    "\n",
    "def load_clean_dataset():\n",
    "    data_file = Path(\"/home/iftekhar/AI-system/Helpers/Mixed/POL_workshop/processed_texts.txt\")\n",
    "    with open(data_file, encoding='utf-8') as f:\n",
    "        data_list = f.read().splitlines()\n",
    "\n",
    "    f = open('deep_clean_mobicontrol_data.txt', 'w')\n",
    "    f.write(single_character_remover(\" \".join(data_list)))\n",
    "    f.close()\n",
    "\n",
    "def single_character_remover(text):\n",
    "    collector = []\n",
    "    for items in text.split():\n",
    "        if len(items) < 2:\n",
    "            replaced = re.sub(r'[ぁ-んァ-ン]', '', items)\n",
    "            replaced = re.sub(r'[A-Za-z]', '', replaced)\n",
    "            replaced = re.sub(r'[0-9]', '', replaced)\n",
    "            collector.append(replaced)\n",
    "        else:\n",
    "            collector.append(items)\n",
    "    return ' '.join([temp.strip(' ') for temp in collector])\n",
    "\n",
    "def all_substrings(string):\n",
    "    n = len(string)\n",
    "    return {string[i:j+1] for i in range(n) for j in range(i,n)}\n",
    "\n",
    "def longest_match(best_matches, items):\n",
    "    longest_content = []\n",
    "    for content in best_matches: \n",
    "        longest_content.append(max(all_substrings(content) & all_substrings(items), key=len))\n",
    "    return max(longest_content, key=len)\n",
    "\n",
    "def handling_spelling_mistakes(misspelled_word, vocabulary):\n",
    "    max_term = []\n",
    "    best_matches = get_close_matches(misspelled_word, vocabulary, n = 5, cutoff = 0.6)\n",
    "    if best_matches:\n",
    "        max_term = longest_match(best_matches, misspelled_word)\n",
    "    return max_term\n",
    "\n",
    "def probability(word): \n",
    "    return word_count_dict[word]/sum(word_count_dict.values())\n",
    "\n",
    "def words(text): \n",
    "    return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "def correction(word, letters): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word, letters), key=probability)\n",
    "\n",
    "def candidates(word, letters): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word, letters)) or known(edits2(word, letters)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    return set(w for w in words if w in word_count_dict)\n",
    "\n",
    "def edits1(word, letters):\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word, letters): \n",
    "    return (e2 for e1 in edits1(word, letters) for e2 in edits1(e1, letters))\n",
    "\n",
    "def spelling_checker_suggester(sent, vocab, dictionary, all_letters):\n",
    "    sent_list = sent.split()\n",
    "    correct_sentence = {}\n",
    "    for word in sent_list:\n",
    "        word = word.lower()\n",
    "        if word in vocab:\n",
    "            correct_sentence[word] = word\n",
    "        else:\n",
    "            # First find words suggestion from library\n",
    "            suggested_words = dictionary.suggest(word)\n",
    "            if suggested_words:\n",
    "                correct_sentence[word] = suggested_words\n",
    "            # find longest chunk match using difflib library\n",
    "            else:            \n",
    "                matches = handling_spelling_mistakes(word, vocab)\n",
    "                match_len = len(matches)/len(word)\n",
    "                # Macthed chunk length is <25% distance away\n",
    "                if match_len > 0.75 and SequenceMatcher(None, word, matches).ratio() > 0.75:\n",
    "                    suggested_words = dictionary.suggest(matches)\n",
    "                    correct_sentence[word] = suggested_words\n",
    "                # Macthed chunk length is <35% distance away\n",
    "                elif SequenceMatcher(None, word, matches).ratio() > 0.65 and not suggested_words:\n",
    "                    suggested_words = correction(word, all_letters).split() + list(known(edits2(word, all_letters)))\n",
    "                    correct_sentence[word] = list(set(suggested_words))\n",
    "                else:\n",
    "                    word = '*' + word + '*'\n",
    "                    correct_sentence[word] = word\n",
    "    return correct_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provide a input: 管理 機能 構成 プロフイル 規定 wifi ssid wifie 接続 続禁止 bluetoothe\n",
      "{'管理': '管理', '機能': '機能', '構成': '構成', 'プロフイル': ['プロパイダ', 'プロトコル', 'プロバイダ'], '規定': '規定', 'wifi': 'wifi', 'ssid': 'ssid', 'wifie': ['wife', 'WiFi'], '接続': '接続', '続禁止': ['禁止'], 'bluetoothe': ['Bluetooth']}\n",
      "Provide a input: \n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "sentence = 1\n",
    "while sentence:\n",
    "    sentence = input(\"Provide a input: \")\n",
    "    print(spelling_checker_suggester(sentence, vocab, dictionary, all_letters))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
