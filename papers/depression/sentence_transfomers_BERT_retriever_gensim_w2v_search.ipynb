{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a89a4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-06 17:03:22.889100: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-06 17:03:24.568211: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-06 17:03:24.568405: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-06 17:03:24.568428: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "# util.pytorch_cos_sim(embeddings[i], embeddings[j]).item())\n",
    "# sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "# t5_bert_model = SentenceTransformer('sentence-transformers/sentence-t5-base')\n",
    "# t5_bert_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "# t5_bert_model = SentenceTransformer('stsb-roberta-large')\n",
    "t5_bert_model = SentenceTransformer('sentence-transformers/msmarco-bert-base-dot-v5')\n",
    "\n",
    "\n",
    "# embeddings = model.encode(sentences)\n",
    "# print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ca5e98d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3904/4008397581.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_attempt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtwitter_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtwitter_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'Attempt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt5_bert_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msimilarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/exp-Env/lib/python3.7/site-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstart_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Batches\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0msentences_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentences_sorted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/exp-Env/lib/python3.7/site-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0mTokenizes\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \"\"\"\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_sentence_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/exp-Env/lib/python3.7/site-packages/sentence_transformers/models/Transformer.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mbatch1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtext_tuple\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 \u001b[0mbatch1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_tuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m                 \u001b[0mbatch2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_tuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mto_tokenize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "sentences = class_attempt[0].extend(twitter_df[twitter_df['label']=='Attempt'].values.tolist())\n",
    "embeddings = t5_bert_model.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "similarity = []\n",
    "for i in range(len(sentences)):\n",
    "    row = []\n",
    "    for j in range(len(sentences)):\n",
    "        row.append(util.pytorch_cos_sim(embeddings[i], embeddings[j]).item())\n",
    "        time.sleep(2)\n",
    "    tile.sleep(2)\n",
    "    similarity.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b218c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(model, text_data):\n",
    "    return model.encode([text_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65aff6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e100895",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSSR_CAT = ['Indicator', 'Attempt','Behavior','Ideation', 'Supportive']\n",
    "output = dict.fromkeys(CSSR_CAT)\n",
    "\n",
    "all_score = []\n",
    "for i in CSSR_CAT:\n",
    "    reddit_df = class_dataframe[class_dataframe['label']==i]\n",
    "    reddit_df.reset_index(inplace=True, drop=True)\n",
    "    reddit_data = ' '.join(reddit_df['text'].values.tolist())\n",
    "    reddit_embedding = get_embeddings(t5_bert_model, reddit_data)\n",
    "    \n",
    "    compare_score = []\n",
    "    i=1\n",
    "    for _, j in twitter_df.iterrows():\n",
    "        twitter_embedding = get_embeddings(t5_bert_model, j['text'])\n",
    "        compare_score.append(linear_kernel(reddit_embedding, twitter_embedding))\n",
    "        if i > 5:\n",
    "            break\n",
    "        i+=1\n",
    "    time.sleep(3)\n",
    "    all_score.append(sum(compare_score)/i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c861980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[200.5308]], dtype=float32),\n",
       " array([[203.47261]], dtype=float32),\n",
       " array([[201.56844]], dtype=float32),\n",
       " array([[199.58858]], dtype=float32),\n",
       " array([[197.9092]], dtype=float32)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7f6ccf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.58524317]], dtype=float32),\n",
       " array([[0.58334]], dtype=float32),\n",
       " array([[0.6497788]], dtype=float32),\n",
       " array([[0.64655393]], dtype=float32),\n",
       " array([[0.48477268]], dtype=float32)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bb2976",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dade7e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5899377346038819"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.array(all_score))/len(all_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f624554",
   "metadata": {},
   "source": [
    "### Gensim w2v model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7bfe8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import load_word2vec_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "165039e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86054eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.preprocess_and_generate_dataset import generate_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a6f6811",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dataframe = generate_dataframe(10)\n",
    "class_dataframe = class_dataframe.rename(columns={'Post': 'text', 'Label':'label'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e44c48c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>viable option youll leaving wife behind youd p...</td>\n",
       "      <td>Supportive</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hard appreciate notion could meet someone else...</td>\n",
       "      <td>Ideation</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hi last night wa sitting ledge window contempl...</td>\n",
       "      <td>Behavior</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tried kill self failed badly cause moment want...</td>\n",
       "      <td>Attempt</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hi nem3030 sort thing enjoy personally always ...</td>\n",
       "      <td>Ideation</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Organic mood affective disorders Hypomania Re...</td>\n",
       "      <td>Ideation</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Gender identity disorder of adolescence.3 pre...</td>\n",
       "      <td>Indicator</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Intertrigo of abdominal skin fold Early yaws ...</td>\n",
       "      <td>Indicator</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Parasitic skin infestation Foreign body granu...</td>\n",
       "      <td>Indicator</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Changing normal routine Loss of appetite Poo...</td>\n",
       "      <td>Behavior</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>730 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text       label  code\n",
       "0   viable option youll leaving wife behind youd p...  Supportive     4\n",
       "1   hard appreciate notion could meet someone else...    Ideation     2\n",
       "2   hi last night wa sitting ledge window contempl...    Behavior     1\n",
       "3   tried kill self failed badly cause moment want...     Attempt     0\n",
       "4   hi nem3030 sort thing enjoy personally always ...    Ideation     2\n",
       "..                                                ...         ...   ...\n",
       "30   Organic mood affective disorders Hypomania Re...    Ideation     2\n",
       "47   Gender identity disorder of adolescence.3 pre...   Indicator     3\n",
       "76   Intertrigo of abdominal skin fold Early yaws ...   Indicator     3\n",
       "82   Parasitic skin infestation Foreign body granu...   Indicator     3\n",
       "13    Changing normal routine Loss of appetite Poo...    Behavior     1\n",
       "\n",
       "[730 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9767fe09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_attempt = class_dataframe[class_dataframe['label']=='Attempt'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e7126cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = '/home/ifte-home/work/resources/w2v_300/model.bin'\n",
    "dimention = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d3284306",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_word2vec_format(model_file, binary=True, unicode_errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0be36991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>69567</th>\n",
       "      <td>time read format wanted make book could done j...</td>\n",
       "      <td>Indicator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>depressed year story hope help raise awareness...</td>\n",
       "      <td>Indicator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34978</th>\n",
       "      <td>long text friend doubt anyone read tldr highly...</td>\n",
       "      <td>Indicator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28886</th>\n",
       "      <td>fuck idek name call badger year canadian dont ...</td>\n",
       "      <td>Indicator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87420</th>\n",
       "      <td>pickle pickle existence girl matter born write...</td>\n",
       "      <td>Indicator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41618</th>\n",
       "      <td>hentai helped become simp warning loooong text...</td>\n",
       "      <td>Supportive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31865</th>\n",
       "      <td>anyone need reason live would miss worth alive...</td>\n",
       "      <td>Supportive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14780</th>\n",
       "      <td>stressing life frankly sick tired world someti...</td>\n",
       "      <td>Supportive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40249</th>\n",
       "      <td>last advice offer make mistake often severe ot...</td>\n",
       "      <td>Supportive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69718</th>\n",
       "      <td>faking someone else front people guess would w...</td>\n",
       "      <td>Supportive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text       label\n",
       "69567  time read format wanted make book could done j...   Indicator\n",
       "5571   depressed year story hope help raise awareness...   Indicator\n",
       "34978  long text friend doubt anyone read tldr highly...   Indicator\n",
       "28886  fuck idek name call badger year canadian dont ...   Indicator\n",
       "87420  pickle pickle existence girl matter born write...   Indicator\n",
       "...                                                  ...         ...\n",
       "41618  hentai helped become simp warning loooong text...  Supportive\n",
       "31865  anyone need reason live would miss worth alive...  Supportive\n",
       "14780  stressing life frankly sick tired world someti...  Supportive\n",
       "40249  last advice offer make mistake often severe ot...  Supportive\n",
       "69718  faking someone else front people guess would w...  Supportive\n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/home/ifte-home/Documents/mental_health/suicide/CSSRS/TFIDF_result_suicide_intensity.csv', index_col=0)\n",
    "twitter_df = df\n",
    "twitter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94090b55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>69567</th>\n",
       "      <td>time read format wanted make book could done j...</td>\n",
       "      <td>Indicator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>depressed year story hope help raise awareness...</td>\n",
       "      <td>Indicator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34978</th>\n",
       "      <td>long text friend doubt anyone read tldr highly...</td>\n",
       "      <td>Indicator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28886</th>\n",
       "      <td>fuck idek name call badger year canadian dont ...</td>\n",
       "      <td>Indicator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87420</th>\n",
       "      <td>pickle pickle existence girl matter born write...</td>\n",
       "      <td>Indicator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41618</th>\n",
       "      <td>hentai helped become simp warning loooong text...</td>\n",
       "      <td>Supportive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31865</th>\n",
       "      <td>anyone need reason live would miss worth alive...</td>\n",
       "      <td>Supportive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14780</th>\n",
       "      <td>stressing life frankly sick tired world someti...</td>\n",
       "      <td>Supportive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40249</th>\n",
       "      <td>last advice offer make mistake often severe ot...</td>\n",
       "      <td>Supportive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69718</th>\n",
       "      <td>faking someone else front people guess would w...</td>\n",
       "      <td>Supportive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text       label\n",
       "69567  time read format wanted make book could done j...   Indicator\n",
       "5571   depressed year story hope help raise awareness...   Indicator\n",
       "34978  long text friend doubt anyone read tldr highly...   Indicator\n",
       "28886  fuck idek name call badger year canadian dont ...   Indicator\n",
       "87420  pickle pickle existence girl matter born write...   Indicator\n",
       "...                                                  ...         ...\n",
       "41618  hentai helped become simp warning loooong text...  Supportive\n",
       "31865  anyone need reason live would miss worth alive...  Supportive\n",
       "14780  stressing life frankly sick tired world someti...  Supportive\n",
       "40249  last advice offer make mistake often severe ot...  Supportive\n",
       "69718  faking someone else front people guess would w...  Supportive\n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c21b3c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Indicator     100\n",
       "Attempt       100\n",
       "Behavior      100\n",
       "Ideation      100\n",
       "Supportive    100\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "44737671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(text, dimention):\n",
    "    sum_vec = np.zeros(dimention)\n",
    "    word_count = 0\n",
    "    for items in text.split():\n",
    "        try: \n",
    "            temp = model[items]\n",
    "        except KeyError:\n",
    "            temp = 0\n",
    "        sum_vec += temp\n",
    "        word_count += 1\n",
    "    return sum_vec / word_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fab0941b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dataset(dataset, dim):\n",
    "    vector_collection = []\n",
    "    for i, col in dataset.iterrows():\n",
    "        vector_collection.append(get_vector(col['text'], dim))\n",
    "    return np.array(vector_collection)\n",
    "\n",
    "vector_collection_twitter = train_dataset(twitter_df, dimention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2e5d22c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_simiarity(reddit_string, dimention, twitter_vector):\n",
    "    reddit_vector = get_vector(reddit_string, dimention).reshape(1,-1)\n",
    "    rank = linear_kernel(reddit_vector, twitter_vector)\n",
    "    cosine_similarities = rank.flatten()\n",
    "    ranked_index = cosine_similarities.argsort()[:-100:-1]    \n",
    "#     print(related_docs)\n",
    "    output = cosine_similarities[ranked_index]\n",
    "    avg_result = np.sum(output)/ranked_index.size\n",
    "    return avg_result, ranked_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d8acfac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSSR_CAT = ['Indicator', 'Attempt','Behavior','Ideation', 'Supportive']\n",
    "output = dict.fromkeys(CSSR_CAT)\n",
    "\n",
    "best_df = pd.DataFrame()\n",
    "for i in CSSR_CAT:\n",
    "    reddit_df = class_dataframe[class_dataframe['label']==i]\n",
    "    reddit_df.reset_index(inplace=True, drop=True)\n",
    "    reddit_compare = ' '.join(reddit_df['text'].values.tolist())\n",
    "    \n",
    "    temp, best_indices = get_avg_simiarity(reddit_compare, dimention, vector_collection_twitter)\n",
    "    output[i] = temp\n",
    "    \n",
    "    new_df = df.iloc[best_indices[:100]]\n",
    "    new_df['label_w2v'] = i\n",
    "    best_df = pd.concat([best_df, new_df])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "195a66ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Indicator': 0.22288251827977282,\n",
       " 'Attempt': 0.22585436680061988,\n",
       " 'Behavior': 0.23246457221659597,\n",
       " 'Ideation': 0.2323778402409954,\n",
       " 'Supportive': 0.24041617908844734}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "af37a147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>69567</th>\n",
       "      <td>time read format wanted make book could done j...</td>\n",
       "      <td>Indicator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>depressed year story hope help raise awareness...</td>\n",
       "      <td>Indicator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34978</th>\n",
       "      <td>long text friend doubt anyone read tldr highly...</td>\n",
       "      <td>Indicator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28886</th>\n",
       "      <td>fuck idek name call badger year canadian dont ...</td>\n",
       "      <td>Indicator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87420</th>\n",
       "      <td>pickle pickle existence girl matter born write...</td>\n",
       "      <td>Indicator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41618</th>\n",
       "      <td>hentai helped become simp warning loooong text...</td>\n",
       "      <td>Supportive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31865</th>\n",
       "      <td>anyone need reason live would miss worth alive...</td>\n",
       "      <td>Supportive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14780</th>\n",
       "      <td>stressing life frankly sick tired world someti...</td>\n",
       "      <td>Supportive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40249</th>\n",
       "      <td>last advice offer make mistake often severe ot...</td>\n",
       "      <td>Supportive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69718</th>\n",
       "      <td>faking someone else front people guess would w...</td>\n",
       "      <td>Supportive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text       label\n",
       "69567  time read format wanted make book could done j...   Indicator\n",
       "5571   depressed year story hope help raise awareness...   Indicator\n",
       "34978  long text friend doubt anyone read tldr highly...   Indicator\n",
       "28886  fuck idek name call badger year canadian dont ...   Indicator\n",
       "87420  pickle pickle existence girl matter born write...   Indicator\n",
       "...                                                  ...         ...\n",
       "41618  hentai helped become simp warning loooong text...  Supportive\n",
       "31865  anyone need reason live would miss worth alive...  Supportive\n",
       "14780  stressing life frankly sick tired world someti...  Supportive\n",
       "40249  last advice offer make mistake often severe ot...  Supportive\n",
       "69718  faking someone else front people guess would w...  Supportive\n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb57e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import io\n",
    "\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = map(float, tokens[1:])\n",
    "    return data\n",
    "\n",
    "vectors = load_vectors('/home/ifte-home/work/resources/crawl-300d-2M.vec')\n",
    "model = fasttext.load_model(vectors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
